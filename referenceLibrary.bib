@book{Fuster2008,
abstract = {This is the fourth edition of the undisputed classic on the prefrontal cortex, the principal "executive" structure of the brain. Because of its role in such cognitive functions as working memory, planning, and decision-making, the prefrontal cortex is critically involved in the organization of behavior, language, and reasoning. Prefrontal dysfunction lies at the foundation of several psychotic and neurodegenerative disorders, including schizophrenia and dementia. Written by an award-winning author who discovered "memory cells"-the physiological substrate of working memory Provides an in-depth examination of the contributions of every relevant methodology, from comparative anatomy to modern imaging Well-referenced with more than 2000 references},
author = {Fuster, J M},
booktitle = {Creativity},
doi = {10.1016/S0896-6273(00)80673-X},
isbn = {9780123736444},
issn = {08966273},
pages = {410},
title = {{The Prefrontal Cortex}},
url = {http://books.google.com/books?id=zuZlvNICdhUC{\&}pgis=1},
volume = {1},
year = {2008}
}
@article{Koechlin2003,
abstract = {The prefrontal cortex (PFC) subserves cognitive control: the ability to coordinate thoughts or actions in relation with internal goals. Its functional architecture, however, remains poorly understood. Using brain imaging in humans, we showed that the lateral PFC is organized as a cascade of executive processes from premotor to anterior PFC regions that control behavior according to stimuli, the present perceptual context, and the temporal episode in which stimuli occur, respectively. The results support an unified modular model of cognitive control that describes the overall functional organization of the human lateral PFC and has basic methodological and theoretical implications.},
author = {Koechlin, Etienne and Ody, Chryst{\`{e}}le and Kouneiher, Fr{\'{e}}d{\'{e}}rique},
doi = {10.1126/science.1088545},
isbn = {0036-8075},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
number = {5648},
pages = {1181--1185},
pmid = {14615530},
title = {{The architecture of cognitive control in the human prefrontal cortex.}},
volume = {302},
year = {2003}
}
@article{Miyake2000,
abstract = {This individual differences study examined the separability of three often postulated executive functions-mental set shifting ("Shifting"), information updating and monitoring ("Updating"), and inhibition of prepotent responses ("Inhibition")-and their roles in complex "frontal lobe" or "executive" tasks. One hundred thirty-seven college students performed a set of relatively simple experimental tasks that are considered to predominantly tap each target executive function as well as a set of frequently used executive tasks: the Wisconsin Card Sorting Test (WCST), Tower of Hanoi (TOH), random number generation (RNG), operation span, and dual tasking. Confirmatory factor analysis indicated that the three target executive functions are moderately correlated with one another, but are clearly separable. Moreover, structural equation modeling suggested that the three functions contribute differentially to performance on complex executive tasks. Specifically, WCST performance was related most strongly to Shifting, TOH to Inhibition, RNG to Inhibition and Updating, and operation span to Updating. Dual task performance was not related to any of the three target functions. These results suggest that it is important to recognize both the unity and diversity of executive functions and that latent variable analysis is a useful approach to studying the organization and roles of executive functions.},
author = {Miyake, A and Friedman, N P and Emerson, M J and Witzki, a H and Howerter, A and Wager, T D},
doi = {10.1006/cogp.1999.0734},
isbn = {9780415454940},
issn = {0010-0285},
journal = {Cognitive psychology},
number = {1},
pages = {49--100},
pmid = {10945922},
title = {{The unity and diversity of executive functions and their contributions to complex "Frontal Lobe" tasks: a latent variable analysis.}},
volume = {41},
year = {2000}
}
@article{Langton1990,
abstract = {In order for computation to emerge spontaneously and become an important factor in the dynamics of a system, the material substrate must support the primitive functions required for computation: the transmission, storage, and modification of information. Under what conditions might we expect physical systems to support such computational primitives?$\backslash$n$\backslash$nThis paper presents research on cellular automata which suggests that the optimal conditions for the support of information transmission, storage, and modification, are achieved in the vicinity of a phase transition. We observe surprising similarities between the behaviors of computations and systems near phase transitions, finding analogs of computational complexity classes and the halting problem within the phenomenology of phase transitions.$\backslash$n$\backslash$nWe conclude that there is a fundamental connection between computation and phase transitions, especially second-order or “critical” transitions, and discuss some of the implications for our understanding of nature if such a connection is borne out.},
archivePrefix = {arXiv},
arxivId = {adap-org/9306003},
author = {Langton, Chris G.},
doi = {doi: DOI: 10.1016/0167-2789(90)90064-V},
eprint = {9306003},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/TDT22 - Complex and Bio-Inspired Systems/Papers/3{\_}Langton.pdf:pdf},
isbn = {0-7382-0232-0},
issn = {0167-2789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {12--37},
pmid = {3751941},
primaryClass = {adap-org},
title = {{Computation at the edge of chaos: Phase transitions and emergent computation}},
url = {http://www.sciencedirect.com/science/article/B6TVK-46G4VN3-5/2/ba78385fe3d12571ab9b97f04093b9be},
volume = {42},
year = {1990}
}
@incollection{Bar-yam1997,
author = {Bar-yam, Yaneer},
booktitle = {Methods},
chapter = {0 Overview},
edition = {First Edit},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/TDT22 - Complex and Bio-Inspired Systems/Papers/1{\_}Bar-Yam.pdf:pdf},
isbn = {0201557487},
pages = {1--15},
publisher = {Westview Press},
title = {{Dynamics of Complex Systems}},
url = {http://necsi.edu/publications/dcs/},
year = {1997}
}
@article{Newman2003,
abstract = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0303516},
author = {Newman, MEJ E. J.},
doi = {10.1137/S003614450342480},
eprint = {0303516},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/TDT22 - Complex and Bio-Inspired Systems/Papers/5{\_}Newman.pdf:pdf},
isbn = {00361445},
issn = {1958-5969},
journal = {arXiv:Cond-Mat/0303516},
number = {2},
pages = {167--256},
pmid = {24174898},
primaryClass = {cond-mat},
title = {{The structure and function of complex networks}},
url = {http://arxiv.org/abs/cond-mat/0303516},
volume = {45},
year = {2003}
}
@article{Bertolero2015,
author = {Bertolero, Maxwell A. and Yeo, B. T. Thomas and D’Esposito, Mark},
doi = {10.1073/pnas.1510619112},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertolero, Yeo, D’Esposito - 2015 - The modular and integrative functional architecture of the human brain.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
pages = {201510619},
title = {{The modular and integrative functional architecture of the human brain}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1510619112},
year = {2015}
}
@article{Srivastava2014,
abstract = {Connectionist models of memory storage have been studied for many years, and aim to provide insight into potential mechanisms of memory storage by the brain. A problem faced by these systems is that as the number of items to be stored increases across a finite set of neurons/synapses, the cumulative changes in synaptic weight eventually lead to a sudden and dramatic loss of the stored information (catastrophic interference, CI) as the previous changes in synaptic weight are effectively lost. This effect does not occur in the brain, where information loss is gradual. Various attempts have been made to overcome the effects of CI, but these generally use schemes that impose restrictions on the system or its inputs rather than allowing the system to intrinsically cope with increasing storage demands. We show here that catastrophic interference occurs as a result of interference among patterns that lead to catastrophic effects when the number of patterns stored exceeds a critical limit. However, when Gram-Schmidt orthogonalization is combined with the Hebb-Hopfield model, the model attains the ability to eliminate CI. This approach differs from previous orthogonalisation schemes used in connectionist networks which essentially reflect sparse coding of the input. Here CI is avoided in a network of a fixed size without setting limits on the rate or number of patterns encoded, and without separating encoding and retrieval, thus offering the advantage of allowing associations between incoming and stored patterns. PACS Nos.: 87.10.+e, 87.18.Bb, 87.18.Sn, 87.19.La.},
author = {Srivastava, Vipin and Sampath, Suchitra and Parker, David J},
doi = {10.1371/journal.pone.0105619},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava, Sampath, Parker - 2014 - Overcoming catastrophic interference in connectionist networks using gram-schmidt orthogonalization.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
number = {9},
pages = {e105619},
pmid = {25180550},
title = {{Overcoming catastrophic interference in connectionist networks using gram-schmidt orthogonalization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25180550},
volume = {9},
year = {2014}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCulloch, W S and Pitts, W},
doi = {10.1007/BF02478259},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/mcculloch and pitts.pdf:pdf},
issn = {0007-4985},
journal = {Bulletin of Mathematical Biophysics},
pages = {115--133},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/{~}coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Hochreiter97{\_}lstm.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@incollection{McClelland1997,
abstract = {(From the chapter) these reflections extend the approach taken in D. E. Rumelhart's chapter [see record 97-066019-024] on emotion to the nature of consciousness and explicit memory / argue that a cognitive neuroscience perspective provides a framework in which we can account for the principle aspects of consciousness and explicit memory stressed by J. F. Kihlstrom [see record 97-066019-023], but that allows us to consider the question of the centrality of the concept of self in a somewhat different light / consider G. Mandler's [see record 97-066019-025] search for the functions of consciousness, and suggest how it may be useful to think of consciousness, not so much as a separate faculty with its own functions, but as a manifestation of certain properties of overall system function / comment on emotion and its relation to consciousness and memory, relating the central theme of Rumelhart's chapter to issues raised both by Kihlstrom and Mandler suggest how aspects of consciousness, memory, and emotion can be understood within the context of a connectionist/brain systems account of the organization of the cognitive system (PsycINFO Database Record (c) 2005 APA )},
author = {McClelland, James and Cohen, Jonathan D and Schooler, Jonathan},
booktitle = {Scientific approaches to consciousness. Carnegie Mellon Symposia on cognition.},
isbn = {080581471X (hardcover); 0805814728 (paperback)},
keywords = {2340 Cognitive Processes,Cognitive Processes,Connectionism,Consciousness States,Emotions,Explicit Memory,Neurophysiology,Self Concept,commentary,connectionist/brain systems account of organizati,neural basis of self in consciousness {\{}{\&}{\}} memory,system},
pages = {499--509},
title = {{The neural basis of consciousness and explicit memory: Reflections on Kihlstrom, Mandler and Rumelhart}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=1997-97394-026},
year = {1997}
}
@incollection{Rumelhart1986,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
address = {US, Cambridge MA 02142-1209},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
isbn = {026268053X},
issn = {1-55860-013-2},
keywords = {Adaptive systems,Learning,Learning machines,Perceptrons,and Back propagation.,networks},
mendeley-groups = {Master-related},
pages = {318--362},
publisher = {The MIT Press},
title = {{Learning internal representations by error propagation}},
volume = {1},
year = {1986}
}

@article{Pineda1987,
abstract = {An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the $\delta$ rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.},
author = {Pineda, Fernando J.},
doi = {10.1103/PhysRevLett.59.2229},
issn = {00319007},
journal = {Physical Review Letters},
number = {19},
pages = {2229},
pmid = {10035458},
title = {{Generalization of back-propagation to recurrent neural networks}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.59.2229},
volume = {59},
year = {1987}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
doi = {10.1073/pnas.79.8.2554},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {8},
pages = {2554--2558},
pmid = {6953413},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
volume = {79},
year = {1982}
}
@article{Yamashita2008a,
abstract = {It is generally thought that skilled behavior in human beings results from a functional hierarchy of the motor control system, within which reusable motor primitives are flexibly integrated into various sensori-motor sequence patterns. The underlying neural mechanisms governing the way in which continuous sensori-motor flows are segmented into primitives and the way in which series of primitives are integrated into various behavior sequences have, however, not yet been clarified. In earlier studies, this functional hierarchy has been realized through the use of explicit hierarchical structure, with local modules representing motor primitives in the lower level and a higher module representing sequences of primitives switched via additional mechanisms such as gate-selecting. When sequences contain similarities and overlap, however, a conflict arises in such earlier models between generalization and segmentation, induced by this separated modular structure. To address this issue, we propose a different type of neural network model. The current model neither makes use of separate local modules to represent primitives nor introduces explicit hierarchical structure. Rather than forcing architectural hierarchy onto the system, functional hierarchy emerges through a form of self-organization that is based on two distinct types of neurons, each with different time properties ("multiple timescales"). Through the introduction of multiple timescales, continuous sequences of behavior are segmented into reusable primitives, and the primitives, in turn, are flexibly integrated into novel sequences. In experiments, the proposed network model, coordinating the physical body of a humanoid robot through high-dimensional sensori-motor control, also successfully situated itself within a physical environment. Our results suggest that it is not only the spatial connections between neurons but also the timescales of neural activity that act as important mechanisms leading to functional hierarchy in neural systems.},
author = {Yamashita, Yuichi and Tani, Jun},
doi = {10.1371/journal.pcbi.1000220},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamashita, Tani - 2008 - Emergence of functional hierarchy in a multiple timescale neural network model a humanoid robot experiment.pdf:pdf},
isbn = {10.1371/journal.pcbi.1000220},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Generalization, Stimulus,Generalization, Stimulus: physiology,Higher Nervous Activity,Higher Nervous Activity: physiology,Humans,Models, Neurological,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Robotics,Robotics: methods,Synapses,Synapses: physiology,Systems Biology,Systems Biology: methods,Time Factors},
number = {11},
pages = {e1000220},
pmid = {18989398},
title = {{Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment.}},
url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000220},
volume = {4},
year = {2008}
}
@article{Rifai2012a,
abstract = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.},
archivePrefix = {arXiv},
arxivId = {1206.6434},
author = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann and Vincent, Pascal},
eprint = {1206.6434},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifai et al. - 2012 - A generative process for sampling contractive auto-encoders.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {arXiv preprint arXiv:1206.6434},
keywords = {deep learning},
number = {1},
pages = {1855--1862},
title = {{A generative process for sampling contractive auto-encoders}},
year = {2012}
}
@article{Freeman2003,
abstract = {The currency of brains is primarily meaning and only secondarily information. Brain select and pre-process the information carried by sensory stimuli before they construct meaning, and they post-process cognitive meaning into informative commands that control appropriate action and express meaning. A grasp of the intervening process of perception by which meaning is created from information requires observation of neural activity in brains during meaningful behavior of humans and other animals. Measurement is followed by detailed analysis and modeling of the neural activity in order to deduce brain dynamics from brain operations. Unlike computer operation, brain function is hierarchical with many levels. Neurobiologists infer sensory and motor information by recording pulses of microscopic axons of neurons. They infer meaning by recording the local mean fields that are generated by the dendrites of neurons forming interactive mesoscopic and macroscopic populations. Neural populations express meaning in spatial pattern of amplitude modulation of electroencephalographic (EEG) waveforms in the higher frequency ranges of cortical oscillations. Improved theory is needed to understand how brains create meaning. The aim of the theory is to apply population neurodynamics to device new ways to treat clinical brain disorders, and to devise new machines with capacities for autonomy and intelligence that might approach those of simpler free-living animals.},
author = {Freeman, W.J.},
doi = {10.1109/IJCNN.2003.1223896},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/A Neurobiological Theory of Meaning in Perception.pdf:pdf},
isbn = {0-7803-7898-9},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
keywords = {eeg,electroencephalogram,information,meaning,neurodynamics,perception},
pages = {1--28},
title = {{A neurobiological theory of meaning in perception}},
volume = {2},
year = {2003}
}
@article{Izhikevich2003,
abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
archivePrefix = {arXiv},
arxivId = {ArXiv},
author = {Izhikevich, E M and Izhikevich, E M},
doi = {10.1109/TNN.2003.820440},
eprint = {ArXiv},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number = {6},
pages = {1569--72},
pmid = {18244602},
title = {{Simple model of spiking neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18244602},
volume = {14},
year = {2003}
}
@book{Ebbinghaus1885,
abstract = {My aim in this essay is to raise the question "Is there such a thing as mental illness?" and to argue that there is not. Since the notion of mental illness is extremely widely used nowadays, inquiry into the ways in which this term is employed would seem to be especially indicated. Mental illness, of course, is not literally a "thing" -- or physical object -- and hence it can "exist" only in the same sort of way in which other theoretical concepts exist. Yet, familiar theories are in the habit of posing, sooner or later -- at least to those who come to believe in them -- as "objective truths" (or "facts"). During certain historical periods, explanatory conceptions such as deities, witches, and microorganisms appeared not only as theories but as self-evident causes of a vast number of events. I submit that today mental illness is widely regarded in a somewhat similar fashion, that is, as the cause of innumerable diverse happenings. As an antidote to the complacent use of the notion of mental illness -- whether as a self-evident phenomenon, theory, or cause-- let us ask this question: What is meant when it is asserted that someone is mentally ill? In what follows I shall describe briefly the main uses to which the concept of mental illness has been put. I shall argue that this notion has outlived whatever usefulness it might have had and that it now functions merely as a convenient myth.},
author = {Ebbinghaus, Hermann},
booktitle = {Memory: A Contribution to Experimental Psychology},
doi = {52, 281-302},
isbn = {978-1614271666},
issn = {0009-4978},
pages = {1--7},
pmid = {25206041},
title = {{Memory: A Contribution to Experimental Psychology}},
volume = {15},
year = {1885}
}
@article{Ans1997,
abstract = {Gradient descent learning procedures are most often used in neural network modeling. When these algorithms (e.g., backpropagation) are applied to sequential learning tasks a major drawback, termed catastrophic forgetting (or catastrophic interference), generally arises: when a network having already learned a first set of items is next trained on a second set of items, the newly learned information may completely destroy the information previously learned. To avoid this implausible failure, we propose a two-network architecture in which new items are learned by a first network concurrently with internal pseudo-items originating from a second network. As it is demonstrated that these pseudo-items reflect the structure of items previously learned by the first network, the model thus implements a refreshing mechanism using the old information. The crucial point is that this refreshing mechanism is based on reverberating neural networks which need only random stimulations to operate. The model thus provides a means to dramatically reduce retroactive interference while conserving the essentially distributed nature of information and proposes an original but plausible means to copy and paste a distributed memory from one place in the brain to another.},
author = {Ans, Bernard and Rousset, St{\'{e}}phane},
doi = {10.1016/S0764-4469(97)82472-9},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Read/Ans {\&} Rousset. 1997. Avoiding catastrophic forgetting by coupling two reverberating neural networks.pdf:pdf},
issn = {07644469},
journal = {Comptes Rendus de l'Acad{\'{e}}mie des Sciences-Series III-Sciences de la Vie},
keywords = {catastrophic forgetting,connectionist,connectionist models of learning and memory,mod{\`{e}}les connectionnistes de l'apprentissage et de,oubli catastrophique,processus de r{\'{e}}-injection,pseudo-rafra{\^{\i}}chissement,pseudo-rehearsal.,re-injection process,reverberating neural networks,r{\'{e}}seaux neuronaux r{\'{e}}verb{\'{e}}rants},
number = {12},
pages = {989--997},
title = {{Avoiding catastrophic forgetting by coupling two reverberating neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/S0764446997824729},
volume = {320},
year = {1997}
}
@article{Ratcliff1990,
abstract = {Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.},
author = {Ratcliff, R},
doi = {10.1037/0033-295X.97.2.285},
isbn = {1939-1471},
issn = {0033-295X},
journal = {Psychological review},
number = {2},
pages = {285--308},
pmid = {2186426},
title = {{Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.}},
volume = {97},
year = {1990}
}
@incollection{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
booktitle = {Parallel Distributed Processing},
doi = {10.1146/annurev-psych-120710-100344},
isbn = {0-262-68053-X},
issn = {1534-7362},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@article{Tani2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tani, Jun},
doi = {10.1109/JPROC.2014.2308604},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Tani. (2014). Self-Organization and Compositionality in Cognitive Brains.pdf:pdf},
isbn = {9780874216561},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {brains is how they,chaos,cognition,compositionality,dynamical,enables,neural network models,neurorobotics,self-organization,sensory-motor systems,support the compositionality that,systems},
number = {4},
pages = {586--605},
pmid = {15991970},
title = {{Self-Organization and Compositionality in Cognitive Brains : A Neurorobotics Study}},
volume = {102},
year = {2014}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce},
annote = {Interesting analysis of how transferability is affected in neural networks. they use images to see how image features are segmented throughout the layers, and detect specialisation of neurons and optimisation of splitting networks as issues. Skimmed through this proceedings paper.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
eprint = {1411.1792},
pages = {1--9},
title = {{How transferable are features in deep neural networks ?}},
year = {2014}
}
@article{Goodfellow2014,
abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
annote = {dropout is a method designed to regularise neural networks (to improve generalisation)
a modification to SGD
“Drouput can be seen as an extremely efficient means of training exponentially many neural networks that share weights, then averaging together their predictions.” - this might be because dropout results in a slower learning from of training sets, with some nodes unaffected for some training cases leading to a kind of interleaving
experiments as similar as possible
using four different activation functions for each of the methods
testing benchmark test (first task test set) after training on the first task, then after training on a second task, having obtained approximately state of the art classification for the validation set for the current task
using MNIST classification - might be biased because of very domain-specific data?
maxout only activation function that consistently performed reasonably well in the authors’ findings},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6211v2},
author = {Goodfellow, Ian J. and Mirza, Mehdi and Da, Xiao and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1312.6211v2},
journal = {ICLR},
title = {{An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks}},
url = {http://arxiv.org/abs/1312.6211},
year = {2014}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
annote = {This is the paper that Axel Tidemann referred to in his talk at Yahoo labs on Thursday the 15th of Octobre. An RNN with a bottleneck in between two larger networks is used for learning to compress input through a vector, where the first “network” then will compress information (encode it) and the second will decompress it from the vector (decode it). The model is verified on language translation.
The fact that the Encoder-Decoder scheme is good at capturing linguistic regularities may suggest that an Encoder-Decoder like scheme is biologically plausible, and transferable to capturing regularities in any kind of compressed patterns. Linking this scheme to the proposed dual-network memory model of McClelland et al. (1995), this could explain partly why their model would work better to capture regularities in interleaving patterns/memories. Furthermore, it strengthens the suggestion that it is biologically plausible (?). It is also worth noting that encoding and decoding may eliminate catastrophic forgetting because of a certain created sparsity (?) and focus on the most important features. A question that remains is whether this compression leaves out crucial information. One way of fixing this, possibly biologically plausible, is to have the compression network self-organise to a certain extent, thus enabling it to select a compression vector of varying length with varying complexity. 
The model of the paper: Networks are jointly trained to maximise the conditional log-likelihood for the model parameters and input sequences to output sequences. A gradient-based approach can be used.
The hidden unit node is a simplified LSTM unit. May be considered an adaptive variant of a leaky-integrate and fire unit (Bengio et al., 2013).
Adadelta and SGD used to train the RNN Encoder-Decoder.
The proposed model captures both semantic and syntactic structure in the languages. This suggests that the model does indeed capture some invariant features of patterns. Furthermore, what is captured in the model is interestingly somewhat orthogonal to previously existing language models.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
eprint = {1406.1078},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/ChoGru.pdf:pdf},
journal = {arXiv},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
annote = {focus of paper: “optimization of stochastic objectives with high-dimensional parameter spaces”
adam: adaptive moment esitmation
designed to combine AdaGrad and RMSProp
“a simple and computationally efficient algorithm for gradient-based optimization of stochastic objective functions” for large datasets or high-dimensional parameter spaces.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@article{Chorowski2015,
abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7{\%} phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18{\%} PER in single utterances and 20{\%} in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6{\%} level.},
annote = {speech recognition and sequences: very long, noisy input sequences
a hybrid system, state-of-the-art uses a deep neural acoustic model, a triphone HMM model and an n-gram language model{\ldots}
an attention mechanism used in the paper’s proposed model
the attention mechanism works by somehow achieving attention through convolving attention weights, i.e. a filter
.. so their solution is purely neural, but uses convolutional filters
attention-based recurrent sequence generator (ARSG)
[contemplation] how does convolution translate to focus in the brain? is it very different, or can networks possibly constitute this kind of behaviour by their interconnectedness and distributedness?
the model focuses on the relevant elements of the hidden layer using a generator, taking into account attention weights and hidden node content
why did the model support recognition of longer sequences?
an end-to-end trainable model
convolution similar to compression leads to handling greater and arbitrary lengths},
author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Chorowski. 2015. Attention-Based Models for Speech Recognition.pdf:pdf},
journal = {arXiv:1506.07503 [cs, stat]},
title = {{Attention-Based Models for Speech Recognition}},
url = {http://arxiv.org/abs/1506.07503$\backslash$nhttp://www.arxiv.org/pdf/1506.07503.pdf},
year = {2015}
}
@article{Martens2011,
abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of sch (2002) which is used within the HF approach of Martens. Copyright 2011 by the author(s)/owner(s).},
annote = {Said to outperform LSTM approach (in 2011).
THey demonstrated that training RNNs using a modified version of a HS optimiser, using their structural damping, may converge quicker that LSTMs.
Concluding the authors suggest that their approach may be more widely applicable to sequence modeling, from their results from experiments with natural problems},
author = {Martens, James and Sutskever, Ilya},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
keywords = {Learning systems,Optimization},
pages = {1033--1040},
title = {{Learning recurrent neural networks with Hessian-free optimization}},
year = {2011}
}
@article{Bengio2013a,
abstract = {Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many un- labeled examples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build a deep architecture, either as initialization for a supervised predictor, or as a generative model. Deep learning algorithms can yield representations that are more abstract and better disentangle the hidden factors of variation underlying the unknown generating distribution, i.e., to capture invariances and discover non-local structure in that distribution. This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead, focusing on the questions of invariance and disentangling.},
annote = {“This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead.”


representations in deep learning: review and recent trends
greedy layer-wise pre-training of unsupervised and supervised models (is this biologically implausible)
AEs
sparse coding (inspired by visual cortex V1) - linear directed graphical model with a continuous-latent variable associated with sparsity prior.
Predictive Sparse Decomposition
score matching: ML intractable. tractable energy function; score, “the partial derivative of the energy with respect to the input”. rewritten in terms of expectation etc. -{\&}gt; tractable
denoising AEs. only meant to learn a “bottleneck”


(feature) pooling: some operation over a local neighbourhood to create invariance to small transformations Boureau, Ponce {\&}amp; LeCun, 10).


example: facial recognition algorithms using pixel space-based recognition very sensitive to pose, and thus destined to fail. key: finding what disentangles the faces, whether it be low- or high-dimensional


the problem of invariant feature extraction is central within deep learning. conv. nets: max pooling layers


learning invariance
article says it’s not clear through which “mechanism sparsity promotes the learning of invariant features” - same as compression?},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v1},
author = {Bengio, Yoshua and Courville, Aaron},
doi = {10.1007/978-3-642-36657-4},
eprint = {arXiv:1305.0445v1},
isbn = {978-3-642-36656-7},
issn = {18684394},
journal = {Intelligent Systems Reference Library},
pages = {1--28},
title = {{Deep Learning of Representations}},
volume = {49},
year = {2013}
}
@article{Bordes2014,
abstract = {Large-scale relational learning becomes crucial for handling the huge$\backslash$namounts of structured data generated daily in many application domains$\backslash$nranging from computational biology or information retrieval, to natural$\backslash$nlanguage processing. In this paper, we present a new neural network$\backslash$narchitecture designed to embed multi-relational graphs into a flexible$\backslash$ncontinuous vector space in which the original data is kept and enhanced.$\backslash$nThe network is trained to encode the semantics of these graphs in order$\backslash$nto assign high probabilities to plausible components. We empirically$\backslash$nshow that it reaches competitive performance in link prediction on$\backslash$nstandard datasets from the literature as well as on data from a$\backslash$nreal-world knowledge base (WordNet). In addition, we present how our$\backslash$nmethod can be applied to perform word-sense disambiguation in a context$\backslash$nof open-text semantic parsing, where the goal is to learn to assign a$\backslash$nstructured meaning representation to almost any sentence of free text,$\backslash$ndemonstrating that it can scale up to tens of thousands of nodes and$\backslash$nthousands of types of relation.},
annote = {a semantic matching energy function to train a network to disambiguate between entities and relations - i.e. in a semantic network. An interesting aspect about this algorithm is that there is no conceptual difference between the lhs and rhs and relation types, being more bio. plausible in the case of for instance natural language data},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3485v1},
author = {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
doi = {10.1007/s10994-013-5363-6},
eprint = {arXiv:1301.3485v1},
issn = {08856125},
journal = {Machine Learning},
keywords = {Multi-relational data,Neural networks,Word-sense disambiguation},
number = {2},
pages = {233--259},
title = {{A semantic matching energy function for learning with multi-relational data: Application to word-sense disambiguation}},
volume = {94},
year = {2014}
}
@article{Visin2015,
abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
annote = {This approach replaces the convolutional networks, and thus could possibly create an interesting alternative to Corowski et al.’s (2015) model, although evaluated on image datasets, which isn’t nearly as long nor noisy as speech recognition sequences.
basic idea: replace convolutional layer with convolution + pooling “with four RNNs that sweet over lower-layer features in different directions: (1) bottom to top, (2) top to bottom, (3) left to right and (4) right to left”
model use usual sequence RNN instead of multidimensional RNN (as of Graves {\&}amp; Schmidhuber. (2009)). this leads to:
plain RNN
number of RNNs scales linearly with the dimensionality of the input (as opposed to exponentially)
much more easily parallelizable, as the different networks are only dependent along their own dimension’s patches
each RNN doing the input processing can be regarded as learning a filter like the gradient function, only that it isn’t restricted to a particular kind of function
hidden state may be simple tanh layer, gatet rec. layer or LSTM layer
weights can be estimated using SGD (maximising the LOG-likelihood)
as to compared with LeNet, ReNet propagates information laterally across the entire image, whereas LeNet is highly localised. -{\&}gt; more compact feature rep.? + less prone to small redundant features across patches
viewing the ReNet as more suitable for decoding, as it is differentiable (and end-to-end smooth)
training used: Adam (Kingma {\&}amp; Ba, 2014). Additionally: dropout (Srivastava et al., 2014).
the algorithm did not outperform state-of-the-art algorithms using convolution},
archivePrefix = {arXiv},
arxivId = {1505.00393},
author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
eprint = {1505.00393},
journal = {Arxiv},
pages = {1--9},
title = {{ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks}},
url = {http://arxiv.org/abs/1505.00393},
year = {2015}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the varia- tional autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of vari- ability observed in highly-structured sequential data (such as speech). We em- pirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
annote = {An interesting paper on using latent variables in combination with RNNs. the latent variables being embedded into the hidden nodes by combining elements of the variational AE. their resulting model, a VRNN performs better than previous state-of-the-art on speech recognition and hand-writing tasks (highly-structured sequential data).},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02216v1},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1506.02216v1},
journal = {arXiv},
pages = {1--9},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
year = {2015}
}
@article{Alain2014,
abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
annote = {Keywords: auto-encoders, denoising auto-encoders, score matching, unsupervised repre-
sentation learning, manifold learning, Markov chains, generative models
“Deep learning algorithms learn multiple levels of representation, where the number of levels is data-dependent. There are theoretical arguments and much empirical evidence to suggest that when they are well-trained, deep learning algorithms (Hinton et al., 2006; Bengio, 2009; Lee et al., 2009; Salakhutdinov and Hinton, 2009; Bengio {\&}amp; Delalleau, 2011; Bengio et al., 2013b) can perform better than their shallow counterparts, both in terms of learning features for the purpose of classification tasks and for generating higher-quality samples.“
the auto-encoder “estimates the first derivative of the log-density”
the reconstruction error isn’t necessarily an energy function
-{\&}gt; minimizing the regularised reconstruction error an alternative to ML for UL},
author = {Alain, Guillaume and Bengio, Yoshua and Courville, Aaron and Fergus, Rob and Manning, Christopher},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alain et al. - 2014 - What Regularized Auto-Encoders Learn from the Data-Generating Distribution.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Markov chains,auto-encoders,denoising auto-encoders,generative models,manifold learning,score matching,unsupervised repre-sentation learning},
pages = {3563--3593},
title = {{What Regularized Auto-Encoders Learn from the Data-Generating Distribution}},
volume = {15},
year = {2014}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
annote = {The paper demonstrated that RNNs benefits from having a deeper structure
no algorithm managed to perform polyphonic music prediction well
they didn’t test methods such as dropout for traditional networks (improved performance on the music prediction task)},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
journal = {arXiv preprint arXiv:1312.6026},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1312.6026},
year = {2013}
}
@article{Bengio2013b,
abstract = {After a more than decade-long period of relatively little research ac- tivity in the area of recurrent neural networks, several new develop- ments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more effi- cient training of recurrent networks. These advances have been mo- tivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms ofmodeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gra- dients to help symmetry breaking and credit assignment. The ex- periments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
annote = {Issues with training the networks using GD and SGD. 
Recent advances (2013) with Hessian-free optimisation (their main finding). 
The gradient can be decomposed into Jacobian products.
LSTM units are recurrently connected to themselves, thus enabling them to be connected over longer time spans. Another view is that of seeing units as low-pass filters (different frequencies in focus).},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0901v2},
author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
doi = {10.1109/ICASSP.2013.6639349},
eprint = {arXiv:1212.0901v2},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Recurrent networks,deep learning,long-term dependencies,representation learning},
pages = {8624--8628},
title = {{Advances in optimizing recurrent networks}},
year = {2013}
}
@article{Rifai2012,
abstract = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.},
annote = {Contractive auto-encoder learns rep. from input data capturing local manifold stucture around every data point, through the leading singular vectors of the Jacobian of the transformation from input to rep.
Learning process seems to converge quicker and mix better between “modes” than DBNs and RBMs. 
Furthermore, the procedure can help learn and represent invariances present in data and improve classification
“A central objective of machine learning is to generalize from training examples to new configurations of the observed variables, and in the most general setup this means deciding how to redistribute the probability mass associated with each training example form the empirical dsitribution” - In constrast, my aim is to create something which is able to infer any relationships in actual patterns. Furthermore, without catastrophic forgetting (more human-like).
Mixture model may identify the model in some cases with non-linear structures, and thus have the variances in directions orthogonal to the manifold small and constant. Also possible w/ kernel trick
Curse of dimensionality to capture the manifold with local generalisation (isn’t this the case with humans as well? the reason for why it doesn’t happen is largely because we’re constantly training, and have such a huge knowledge base?)
The parameters of the model, theta, is learned by minimising the Jacobian of the contractive encoder equation. The Contractive AE, in comparison with the AE, also adds a regularisation term which penalises the sensitivity of the features to the input, measured as the Frobenius norm of the Jacobian
the CAEs proposed in the paper generalises well, and work well for classification problems. However, they do not for pure unsupervised learning. (Q: Is it really pure unsupervised in the case that I want to do between the networks?)},
archivePrefix = {arXiv},
arxivId = {1206.6434},
author = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann and Vincent, Pascal},
eprint = {1206.6434},
isbn = {978-1-4503-1285-1},
journal = {arXiv preprint arXiv:1206.6434},
keywords = {deep learning},
number = {1},
pages = {1855--1862},
title = {{A generative process for sampling contractive auto-encoders}},
year = {2012}
}
@article{Bengio2011a,
abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher- level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y|x). This paper focusses on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution},
annote = {deep learning to exploit the structure in the input distribution
higher-level learned features from lower-level features
invariant to most representations (more abstract)
supervised learning abstracted in terms of tuples, using input to predict labels. here: labels are irrelevant. self-taught learning (purely unsupervised)
some difficulties that deep learning (and specifically a dual-network memory model?) may address:
input distr very different in test set
very few labels are available on the test set
no labels for the classes of interest are available during learning
“it’s not always clear what is signal and what is noise” - manifold learning may disregard signal when orthogonalizing the variance which is viewed as noise to the coordinate system.
Goodfellow et al. (2009): sparse auto-encoders gave rise to more invariant representations (compared to non-sparse AEs)


why deep architectures? 
representational advantages when compared to shallow circuits
the brain appears to be deep
abstraction, higher-levels


auto-encoders
if it is trained by minimising the average of reconstruction errors, it becomes equivalent (up to a rotation) to PCA.
a non-linear encoder (often) obtains a better representation, which can also be greedily stacked (Bengio et al., 07).
“to efficiently search for a good learning rate, a greedy heuristic that we used is based on the following strategy. Start with a large learning rate and reduce it (by a factor 3) until training does not diverge. The largest learning rate which does not give divergent training (increasing training error) is usually a very good choice of learning rate.”
stopping criterion could be chosen according to the derivative (because only using a convergence on the denoising error could overfit to the training data)
deep learning suitable for transfer learning because it learns high-level abstract representations
a deep learner generalises more (when it comes to out of example input)
to the author, the transfer learning would work better if the algorithms could: “disentangle the underlying factors of variation”},
author = {Bengio, Yoshua},
journal = {JMLR: Workshop and Conference Proceedings 7},
keywords = {autoencoders,deep learning,domain adaptation,ing,multi-task learning,neural networks,re-,representation learning,self-taught learning,stricted boltzmann machines,transfer learn-,unsupervised learning},
pages = {1--20},
title = {{Deep Learning of Representations for Unsupervised and Transfer Learning}},
volume = {7},
year = {2011}
}
@article{Graves2008,
abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to lo- calised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent archi- tecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the align- ment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmen- tation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models. ii},
annote = {LSTM completely online: weight updates can be done for every node after every time step because long time dependencies are dealt with by the memory blocks!
bi-directional compatible!},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Graves, Alex},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
isbn = {2000201075},
issn = {01406736},
journal = {Image Rochester NY},
pages = {124},
pmid = {7491034},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks}},
year = {2008}
}
@inproceedings{Hattori2010,
abstract = {In neural networks, when new patterns are learned by a network, the new information radically interferes with previously stored patterns. This drawback is called catastrophic forgetting or catastrophic interference. In this paper, we propose a biologically inspired neural network model which overcomes this problem. The proposed model consists of two distinct networks: one is a Hopfield type of chaotic associative memory and the other is a multilayer neural network. We consider that these networks correspond to the hippocampus and the neocortex of the brain, respectively. Information given is firstly stored in the hippocampal network with fast learning algorithm. Then the stored information is recalled by chaotic behavior of each neuron in the hippocampal network. Finally, it is consolidated in the neocortical network by using pseudopatterns. Computer simulation results show that the proposed model has much better ability to avoid catastrophic forgetting in comparison with conventional models.},
author = {Hattori, Motonobu},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2010.5596896},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Dual-Network Memory Model Using a Chaotic Neural Network.pdf:pdf},
isbn = {978-1-4244-6916-1},
issn = {1098-7576},
keywords = {Artificial neural networks,Biological neural networks,Computational modeling,Hippocampus,Hopfield type,Neurons,Nonhomogeneous media,Training,biologically inspired neural network model,brain,brain models,catastrophic forgetting,catastrophic interference,chaos,chaotic associative memory,chaotic neural network,computer simulation,content-addressable storage,dual-network memory model,fast learning algorithm,hippocampal network,learning (artificial intelligence),multilayer neural network,neocortex,neural nets},
pages = {1--5},
title = {{Dual-network memory model using a chaotic neural network}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5596896},
year = {2010}
}
@article{Maniadakis2014,
author = {Maniadakis, Michail and Trahanias, Panos},
doi = {10.3389/fnbot.2014.00007},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Maniadakis et al. (2014). Time models and cognitive processes a [review].pdf:pdf},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {artificial sense of time,artificial sense of time, temporal cognition, lite,computational modeling,literature review,temporal cognition,time perception},
number = {February},
pages = {1--6},
title = {{Time models and cognitive processes: a review}},
url = {http://journal.frontiersin.org/article/10.3389/fnbot.2014.00007/abstract},
volume = {8},
year = {2014}
}
@article{Zhang2009,
author = {Zhang, Qi},
doi = {10.1016/j.cogsys.2008.06.002},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2009 - A computational account of dreaming Learning and memory consolidation.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {computational model,dream,learning,memory consolidation,random activation},
number = {2},
pages = {91--101},
publisher = {Elsevier B.V.},
title = {{A computational account of dreaming: Learning and memory consolidation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S138904170800034X},
volume = {10},
year = {2009}
}
@article{Tani1998,
author = {Tani, J},
doi = {10.1118/1.598310},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Tani. (2011). An Interpretation of the Self From the Dynamical Systems Perspective.pdf:pdf},
isbn = {9780203450000},
issn = {00942405},
journal = {Journal of Consciousness Studies},
pages = {1235--1237},
pmid = {24801165},
title = {{An interpretation of theself'from the dynamical systems perspective: a constructivist approach}},
url = {http://www.ingentaconnect.com/content/imp/jcs/1998/00000005/F0020005/880},
volume = {25},
year = {1998}
}
@article{Subagdja2015,
author = {Subagdja, Budhitama and Tan, Ah-Hwee},
doi = {10.1016/j.neucom.2015.02.038},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Subagdja, Tan - 2015 - Neural modeling of sequential inferences and learning over episodic memory.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Adaptive resonance theory,Episodic memory,Transitive inference},
pages = {229--242},
publisher = {Elsevier},
title = {{Neural modeling of sequential inferences and learning over episodic memory}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215001873},
volume = {161},
year = {2015}
}
@article{Sussillo2014,
abstract = {Many recent studies of neurons recorded from cortex reveal complex temporal dynamics. How such dynamics embody the computations that ultimately lead to behavior remains a mystery. Approaching this issue requires developing plausible hypotheses couched in terms of neural dynamics. A tool ideally suited to aid in this question is the recurrent neural network (RNN). RNNs straddle the fields of nonlinear dynamical systems and machine learning and have recently seen great advances in both theory and application. I summarize recent theoretical and technological advances and highlight an example of how RNNs helped to explain perplexing high-dimensional neurophysiological data in the prefrontal cortex. ?? 2014 Elsevier Ltd.},
author = {Sussillo, David},
doi = {10.1016/j.conb.2014.01.008},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sussillo - 2014 - Neural circuits as computational dynamical systems.pdf:pdf},
isbn = {0959-4388},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
pages = {156--163},
pmid = {24509098},
publisher = {Elsevier Ltd},
title = {{Neural circuits as computational dynamical systems}},
url = {http://dx.doi.org/10.1016/j.conb.2014.01.008},
volume = {25},
year = {2014}
}
@article{Yamashita2008,
author = {Yamashita, Yuichi and Tani, Jun},
doi = {10.1371/journal.pcbi.1000220},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Yamashita {\&} Tani. (2008). Emergence of Functional Hierarchy in a Multiple.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
number = {11},
pages = {e1000220},
title = {{Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1000220},
volume = {4},
year = {2008}
}
@article{Insel2015,
author = {Insel, Nathan and Frankland, Paul W.},
doi = {10.1016/j.beproc.2014.09.015},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Insel, Frankland - 2015 - Mechanism, function, and computation in neural systems.pdf:pdf},
issn = {03766357},
journal = {Behavioural Processes},
pages = {4--11},
publisher = {Elsevier B.V.},
title = {{Mechanism, function, and computation in neural systems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0376635714002113},
volume = {117},
year = {2015}
}
@article{Shamir2014,
author = {Shamir, Maoz},
doi = {10.1016/j.conb.2014.01.002},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Maoz Shamir. (2014). Emerging-principles-of-population-coding-in-search-for-the-neural-code{\_}2014{\_}Current-Opinion-in-Neurobiology.pdf:pdf},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
pages = {140--148},
publisher = {Elsevier Ltd},
title = {{Emerging principles of population coding: in search for the neural code}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438814000105},
volume = {25},
year = {2014}
}
@article{Jung2015,
author = {Jung, Minju and Hwang, Jungsik and Tani, Jun},
doi = {10.1371/journal.pone.0131214},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung, Hwang, Tani - 2015 - Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequen.pdf:pdf},
issn = {1932-6203},
journal = {Plos One},
number = {7},
pages = {e0131214},
title = {{Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences}},
url = {http://dx.plos.org/10.1371/journal.pone.0131214},
volume = {10},
year = {2015}
}
@article{Norman2006,
abstract = {We present a new learning algorithm that leverages oscillations in the strength of neural inhibition to train neural networks. Raising inhibition can be used to identify weak parts of target memories, which are then strengthened. Conversely, lowering inhibition can be used to identify competitors, which are then weakened. To update weights, we apply the Contrastive Hebbian Learning equation to successive time steps of the network. The sign of the weight change equation varies as a function of the phase of the inhibitory oscillation. We show that the learning algorithm can memorize large numbers of correlated input patterns without collapsing and that it shows good generalization to test patterns that do not exactly match studied patterns.},
author = {Norman, K a and Newman, E and Detre, G and Polyn, S},
doi = {10.1162/neco.2006.18.7.1577},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Norman et al. (2006). [Letter] How Inhibitory Oscillations Can Train Neural Networks and Punish Competitors.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural Computation},
pages = {1577--1610},
pmid = {16764515},
title = {{How inhibitory oscillations can train neural networks and punish competitors.}},
volume = {18},
year = {2006}
}
@article{Hattori2014,
annote = {Were the white (-1) elements enabling the network to somehow analyse the noise/random input?

Idea: Embedding the timing of firing in the Oja rule?

about pp. 3 {\&}quot;catastrophic forgetting can be avoided due to the forgetting factor, gamma{\&}quot;, and Hebbian learning is actually used - experiments have shown that catastrophic forgetting can indeed occur in short term memory as long as it is succinctly stimulated.

the chaotic nature of the CA3 layer might constitute what creates the working pseudopatterns. this could be further analysed. (3.2, last part)

neuronal birth and extinction is also known as neuronal turnover

[summary] In this excellent paper Hattori presents a novel model, which is an improved version of a former model of his. Namely that of an HPC module as well as a PFC module. He demonstrates in various experiments that the catastrophic forgetting is reduced to a large extent in the new model as opposed to a traditional model, and that the new one is superior in several other aspects as well. He does not however use it to solve any complex tasks, as the model presented is rather heavy in terms of computational complexity. He further demonstrates that the HPC network is capable of acquiring information rapidly, and also to consolidate this into the neocortical network (when it is extracted successfully).

Another aspect that could be investigated: Episodic and semantic memory. Combination of abstractions in the proposed model.},
author = {Hattori, Motonobu},
doi = {10.1016/j.neucom.2013.08.044},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hattori - 2014 - A biologically inspired dual-network memory model for reduction of catastrophic forgetting.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Catastrophic forgetting,Chaotic neural network,Complementary learning systems,Dual-network,Hippocampus,Neuronal turnover,catastrophic forgetting,chaotic neural network,complementary learning systems},
pages = {262--268},
publisher = {Elsevier},
title = {{A biologically inspired dual-network memory model for reduction of catastrophic forgetting}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231214001076},
volume = {134},
year = {2014}
}
@article{McNaughton2010,
author = {McNaughton, Bruce L.},
doi = {10.1016/j.artint.2009.11.013},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNaughton - 2010 - Cortical hierarchies, sleep, and the extraction of knowledge from memory.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {2},
pages = {205--214},
publisher = {Elsevier B.V.},
title = {{Cortical hierarchies, sleep, and the extraction of knowledge from memory}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S000437020900143X},
volume = {174},
year = {2010}
}
@article{McKenzie2014,
author = {McKenzie, Sam and Frank, Andrea J. and Kinsky, Nathaniel R. and Porter, Blake and Rivi{\`{e}}re, Pamela D. and Eichenbaum, Howard},
doi = {10.1016/j.neuron.2014.05.019},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenzie et al. - 2014 - Hippocampal Representation of Related and Opposing Memories Develop within Distinct, Hierarchically Organized N.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {202--215},
title = {{Hippocampal Representation of Related and Opposing Memories Develop within Distinct, Hierarchically Organized Neural Schemas}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S089662731400405X},
volume = {83},
year = {2014}
}
@article{Schlichting2015,
author = {Schlichting, Margaret L. and Mumford, Jeanette a. and Preston, Alison R.},
doi = {10.1038/ncomms9151},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Schlichting et al. (2015). Learning-related representational changes reveal.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
pages = {8151},
publisher = {Nature Publishing Group},
title = {{Learning-related representational changes reveal dissociable integration and separation signatures in the hippocampus and prefrontal cortex}},
url = {http://www.nature.com/doifinder/10.1038/ncomms9151},
volume = {6},
year = {2015}
}
@article{Maniadakis2012,
abstract = {In our daily life, we often adapt plans and behaviors according to dynamically changing world circumstances, selecting activities that make us feel more confident about the future. In this adaptation, the prefrontal cortex (PFC) is believed to have an important role, applying executive control on other cognitive processes to achieve context switching and confidence monitoring; however, many questions remain open regarding the nature of neural processes supporting executive control. The current work explores possible mechanisms of this high-order cognitive function, transferring executing control in the domain of artificial cognitive systems. In particular, we study the self-organization of artificial neural networks accomplishing a robotic rule-switching task analogous to the Wisconsin Card Sorting Test. The obtained results show that behavioral rules may be encoded in neuro-dynamic attractors, with their geometric arrangements in phase space affecting the shaping of confidence. Analysis of the emergent dynamical structures suggests possible explanations of the interactions of high-level and low-level processes in the real brain. © 2012 Elsevier Ltd.},
annote = {{\&}quot;The studies mentioned above indicate that PFC internal mechanisms are based on a dynamic rather than a stationary pattern of neural activity. In other words, active maintenance of neural activity, does not necessarily mean static representations.{\&}quot;},
author = {Maniadakis, Michail and Trahanias, Panos and Tani, Jun},
doi = {10.1016/j.neunet.2012.04.005},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maniadakis, Trahanias, Tani - 2012 - Self-organizing high-order cognitive functions in artificial agents Implications for possible prefr.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Attractor,Cognitive robot,Executive control,Meta-cognition,Neural networks,Prefrontal cortex,Self-organization},
pages = {76--87},
pmid = {22609533},
title = {{Self-organizing high-order cognitive functions in artificial agents: Implications for possible prefrontal cortex mechanisms}},
volume = {33},
year = {2012}
}
@article{Sugita2005,
abstract = {We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the ``compositionality'' of semantics and examine how it can be generated through experiments. Our experimental results showed that the essential structures for situated semantics can self-organize themselves through dense interactions between linguistic and behavioral processes whereby a certain generalization in learning is achieved. Our analysis of the acquired dynamical structures indicates that an equivalence of compositionality appears in the combinatorial mechanics self-organized in the neuronal nonlinear dynamics. The manner in which this mechanism of compositionality, based on dynamical systems, differs from that considered in conventional linguistics and other synthetic computational models, is discussed in this paper.},
annote = {It is basically a mirroring system, using a PB (parametric bias) vector to {\&}quot;synchronise{\&}quot; the two networks. I.e. learn that one representation for the motor-network's {\&}quot;hit left{\&}quot; corresponds to both {\&}quot;hit red{\&}quot; and {\&}quot;hit left{\&}quot;..

The grounding problem is addressed, but here solved with a dynamical systems approach, minimising a parametric bias vector error which connects the systems. It could be hypothesised that all behaviour could be compressed and expressed in terms of language with this method - but does it really interleave the language with the behaviour in such a way that they are {\&}quot;associated{\&}quot; in a more biological sense? I think the network topologies should be inherently different in order to achieve this. Furthermore, the PB-vector itself should be self-organised.

{\&}quot;In contrast to using symbolic representations, some connectionist schemes have investigated the acquisition of both syntax and semantics in a co-dependent manner. It has been shown that a recurrent neural network (RNN) can acquire grammatical structures (Elman, 1990; Pollack, 1991) and semantics (Miikkulainen, 1993) from examples of sentences or string sequences. An RNN also demonstrated the ability to emulate the symbolic structures of finite state machines, hidden in the interaction between a real robot and its environment, in a robot navigation task (Tani, 1996). These results suggest the possibility of self-organizing certain classes of combinatorial structures that are required in compositional semantics without using symbolic representation.{\&}quot; - Pp. 3},
author = {Sugita, Y.},
doi = {10.1177/105971230501300102},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sugita - 2005 - Learning Semantic Combinatoriality from the Interaction between Linguistic and Behavioral Processes.pdf:pdf},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {compositionality,dynamical systems,embodied language,organization,recurrent neural network,robot,self-},
number = {1},
pages = {33--52},
title = {{Learning Semantic Combinatoriality from the Interaction between Linguistic and Behavioral Processes}},
volume = {13},
year = {2005}
}
@article{Schacter2012,
abstract = {During the past few years, there has been a dramatic increase in research examining the role of memory in imagination and future thinking. This work has revealed striking similarities between remembering the past and imagining or simulating the future, including the finding that a common brain network underlies both memory and imagination. Here, we discuss a number of key points that have emerged during recent years, focusing in particular on the importance of distinguishing between temporal and nontemporal factors in analyses of memory and imagination, the nature of differences between remembering the past and imagining the future, the identification of component processes that comprise the default network supporting memory-based simulations, and the finding that this network can couple flexibly with other networks to support complex goal-directed simulations. This growing area of research has broadened our conception of memory by highlighting the many ways in which memory supports adaptive functioning.},
annote = {Jeff Hawkins like {\&}quot;neuronal computation as prediction{\&}quot;. In a sense, this is literally what they always do - as one of the fundamental mechanisms is to fire when a known pattern is present; partially, or completely.

Just because some areas are activated both when past and future events are imagined, this does not imply that the same mechanisms/pathways are used for the two aspects of cognition. It has been shown that irrespective of the task, these parts are active during several forms of mental activity. Furthermore, the brain is thought to have a {\&}quot;default network{\&}quot; ((Raichle et al., 2001; for reviews, see Buckner et al., 2008; Andrews-Hanna, 2012), which would then almost always be active.

Brown et al. (2012), demonstrated that patients that believe they can effectively cope with stress have a more detailed episodic memory (both past and future), which could mean that there is both an automatic, and an effortful memory process associated with episodic memory. Anderson et al. (2012) showed that direct and controlled memories were mediated by distinct pathways.},
author = {Schacter, Daniel L. and Addis, Donna Rose and Hassabis, Demis and Martin, Victoria C. and Spreng, R. Nathan and Szpunar, Karl K.},
doi = {10.1016/j.neuron.2012.11.001},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schacter et al. - 2012 - The Future of Memory Remembering, Imagining, and the Brain.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$n0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
number = {4},
pages = {677--694},
pmid = {23177955},
title = {{The Future of Memory: Remembering, Imagining, and the Brain}},
volume = {76},
year = {2012}
}
@article{SchlichtingM.L.;MumfordJ.A.;Preston2015,
abstract = {The episodic memory system enables accurate retrieval while maintaining flexibility by representing both specific episodes and generalizations across events. Although theories suggest that the hippocampus (HPC) is dedicated to represent specific episodes while the medial prefrontal cortex (MPFC) generalizes, other accounts posit that HPC can also integrate related memories. Here we use high-resolution functional magnetic resonance imaging in humans to examine how representations of memory elements change to either differentiate or generalize across related events. We show that while posterior HPC and anterior MPFC maintain distinct memories for individual events, anterior HPC and posterior MPFC integrate across memories. Integration is particularly likely for established memories versus those encoded simultaneously, highlighting the greater impact of prior knowledge on new encoding. We also show dissociable coding signatures in ventrolateral PFC, a region previously implicated in interference resolution. These data highlight how memory elements are represented to simultaneously promote generalization across memories and protect from interference.},
author = {{Schlichting, M. L.; Mumford J. A. ; Preston}, A. R},
journal = {NATURE COMMUNICATIONS},
keywords = {ACTIVATION,DECLARATIVE MEMORY,DISTINCT,INFERENCE,INFORMATION,MEDIAL TEMPORAL-LOBE,PATTERN SEPARATION,RETRIEVAL,RETROGRADE-AMNESIA,SCHEMA},
title = {{Learning-related representational changes reveal dissociable integration and separation signatures in the hippocampus and prefrontal cortex}},
volume = {6},
year = {2015}
}
@article{McCloskey1989,
abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
author = {McCloskey, Michael and Cohen, Neal J.},
doi = {10.1016/S0079-7421(08)60536-8},
isbn = {0079-7421},
issn = {00797421},
journal = {Psychology of Learning and Motivation - Advances in Research and Theory},
number = {C},
pages = {109--165},
title = {{Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem}},
volume = {24},
year = {1989}
}
@misc{Ans2000,
abstract = {We explore a dual-network architecture with self-refreshing memory (Ans and Rousset 1997) which overcomes catastrophic forgetting in sequential learning tasks. Its principle is that new knowledge is learned along with an internally generated activity reflecting the network history. What mainly distinguishes this model from others using pseudorehearsal in feedforward multilayer networks is a reverberating process used for generating pseudoitems. This process, which tends to go up to network attractors from random activation, is more suitable for capturing optimally the deep structure of previously learned knowledge than a single feedforward pass of activity. The proposed mechanism for ‘transporting memory’ without loss of information between two different brain structures could be viewed as a neurobiologically plausible means for consolidation in long-term memory. Knowledge transfer is explored with regard to learning speed, ability to generalize and vulnerability to network damages. We show that transfer is more efficient when two related tasks are sequentially learned than when they are learned concurrently. With a self-refreshing memory network knowledge can be saved for a long time and therefore reused in subsequent acquisitions.},
author = {Ans, Bernard and Rousset, Stephane},
booktitle = {Connection Science},
doi = {10.1080/095400900116177},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Read/Ans {\&} Rousset. 2000. Neural networks with a self-refreshing memory knowledge transfer in sequential learning tasks without catastrophic forgetting.pdf:pdf},
issn = {0954-0091},
number = {1},
pages = {1--19},
title = {{Neural networks with a self-refreshing memory: Knowledge transfer in sequential learning tasks without catastrophic forgetting}},
volume = {12},
year = {2000}
}
@book{Hebb1949,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists-the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology-a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
address = {New York},
author = {Hebb, DO O},
booktitle = {The Organization of Behavior: A Neuropsychological Theory},
doi = {10.2307/1418888},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/The{\_}Organization{\_}of{\_}Behavior-Donald{\_}O.{\_}Hebb.pdf:pdf},
isbn = {0805843000},
issn = {00107549},
mendeley-groups = {Master-related},
pages = {335},
pmid = {1605},
publisher = {JOHN WILEY {\&} SONS, Inc.},
title = {{The organization of behavior: a neuropsychological theory}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1279737/},
year = {1949}
}
@article{Carpenter1987,
abstract = {Adaptive resonance architectures are neural networks that self-organize stable pattern recognition codes in real-time in response to arbitrary sequences of input patterns. This article introduces ART 2, a class of adaptive resonance architectures which rapidly self-organize pattern recognition categories in response to arbitrary sequences of either analog or binary input patterns. In order to cope with arbitrary sequences of analog input patterns-ART 2 architectures embody solutions to a number of design principles, such as the stability-plasticity tradeoff, the search-direct access tradeoff, and the match-reset tradeoff. In these architectures, top-down learned expectation and matching mechanisms are critical in self-stabilizing the code learning process. A parallel search scheme updates itself adaptively as the learning process unfolds, and realizes a form of real-time hypothesis discovery, testing, learning, and recognition. After learning selfstabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time for familiar inputs does not increase with the complexity of the learned code. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. A parameter called the attentional vigilance parameter determines how fine the categories will be. If vigilance increases (decreases) due to environmental feedback, then the system automatically searches for and learns finer (coarser) recognition categories. Gain control parameters enable the architecture to suppress noise up to a prescribed level. The architecture's global design enables it to learn effectively despite the high degree of nonlinearity of such mechanisms.},
author = {Carpenter, G A and Grossberg, S},
doi = {10.1364/AO.26.004919},
isbn = {0003-6935 (Print)$\backslash$r0003-6935 (Linking)},
issn = {0003-6935},
journal = {Applied optics},
number = {23},
pages = {4919--4930},
pmid = {20523470},
title = {{ART 2: self-organization of stable category recognition codes for analog input patterns.}},
volume = {26},
year = {1987}
}
@article{Hassabis2012,
abstract = {To celebrate the centenary of the year of Alan Turing’s birth, four scientists and entrepreneurs assess the divide between neuroscience and computing},
annote = {{\&}quot;AI researchers should not only immerse themselves in the latest brain research, but also conduct neuroscience experiments to address key questions such as: “How is conceptual knowledge acquired?” Conversely, from a neuroscience perspective, attempting to distil intelligence into an algorithmic construct may prove to be the best path to understanding some of the enduring mysteries of our minds, such as consciousness and dreams.{\&}quot; - Demis Hassabis, UCL (neuro, comp. games, chess master).},
author = {Hassabis, Demis and Brooks, Rodney},
doi = {10.1038/482462a},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassabis, Brooks - 2012 - Turing centenary Is the brain a good model for machine intelligence.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7386},
pages = {462--463},
pmid = {22358812},
title = {{Turing centenary: Is the brain a good model for machine intelligence?}},
volume = {482},
year = {2012}
}
@article{French2001,
author = {French, Robert M., and Ans, Bernard, and Rousset, Stéphane},
journal = {Connectionist Models of Learning, Development and Evolution},
pages = {13--22},
title = {{Pseudopatterns and dual-network memory models : Advantages and shortcomings}},
year = {2001}
}
@article{Elman2005,
abstract = {Over the past two decades, connectionist models have generated a lively debate regarding the underlying mechanisms of cognitive development. This debate has in turn led to important empirical research that might not have occurred otherwise. More recently, advances in developmental neuroscience present a new set of challenges for modelers. In this article, I review some of the insights that have come from modeling work, focusing on (1) explanations for the shape of change; (2) new views on how knowledge may be represented; (3) the richness of experience. The article concludes by considering some of the new challenges and opportunities for modeling cognitive development. ?? 2005 Elsevier Ltd. All rights reserved.},
annote = {Concluding that we need to go in several directions next, outlining areas for future research within range of domains, multi-tasking, scaling, and cascading effects over time. [This could be used in my thesis if any of these questions/topics are (implicitly) addressed]},
author = {Elman, Jeffrey L.},
doi = {10.1016/j.tics.2005.01.005},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Read/Elman. Connectionist models of cognitive development where next.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {3 SPEC. ISS.},
pages = {111--117},
pmid = {15737819},
title = {{Connectionist models of cognitive development: Where next?}},
volume = {9},
year = {2005}
}
@article{French1999,
abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Consequently, plausible models of human cognition should exhibit similar patterns of gradual forgetting old information as new information is acquired. Only rarely (see Box 3) does new learning in natural cognitive systems completely disrupt or erase previously learned information. In other words, natural cognitive systems do not, in general, forget catastrophically. Unfortunately, however, this is precisely what occurs under certain circumstances in distributed connectionist networks. It turns out that the very features that give these networks their much-touted abilities to generalize, to function in the presence of degraded input, etc., are the root cause of catastrophic forgetting. The challenge is how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article, we examine the causes, conseque...},
annote = {Outstanding questions:
- Is representational separation necessary? Using Buzsaki (05), it could be argued that it is not... What, then, is the role of the Hippocampus?
- Are dual network systems necessary? (again, Buzsaki (05) addresses this)
- Episodic memory? (... Buzsaki (05))
- Neural correlate for the pseudopattern mechanism?
- Restrictions of systems for reaching an ideal functonality (generalisation, discrimination, immunity to catastrophic interference, good episodic memory - one system?) (Buzsaki (05)?)
- Animal correlates for catastrophic forgetting?},
author = {French, R. M.},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/French - 1999 - Catastrophic forgetting in connectionist networks.pdf:pdf},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
number = {4},
pages = {128--135},
pmid = {10322466},
title = {{Catastrophic forgetting in connectionist networks: Causes, consequences and solutions}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.3676},
volume = {3},
year = {1999}
}
@article{French1997,
annote = {Summary [could be an assignment 2 draft]: 
The authors present a series of experiments that address the sensitivity-stability problem. They show that a pseudo-recurrent network model, mainly inspired by McNaughton et al. (1995) performs significantly better than traditional FFBP networks. The different experiments are constructed to illuminate different aspects of the pseudo-recurrent network model. Their key confirmed finding is that pseudo-recurrent networks using pseudopatterns perform significantly better in terms of less catastrophic forgetting, according to both the Ebbinghaus-like savings measure as well as the percentage of explicit recognition of previously learned patterns. From this they hypothesise that the brain may perform some pseudopattern-like compression as well as pseudo-recurrent storage of information. Furthermore, they also demonstrate that this new model in contrast to older models is prone to the exact opposite of catastrophic forgetting - namely catastrophic remembering.
[A critique of this article would include] It should be noted that as the paper shows that catastrophic remembering does occur in special case events where incredibly overlapping inputs are presented, this does not necessarily demonstrate a direct biological implausibility as implied by the authors. This is because the nature of the input is biologically implausible in its own right (in being that overlapping). A biological input would indeed be much more sparse and varied (a much more large scale input). However, from an information theoretic perspective, it is interesting to look at the implications of very dense and overlapping inputs. It does however appear as if the brain itself boggles up certain memories and inputs that is sometimes presented to it. Thus it would be reasonable to state that some point of overlap is in fact biologically plausible. Secondly, the networks simulated are of such a small scale that they will only be able to generalise to a certain extent. This seems like a fact that has been largely ignored by the authors. Therefore it would be interesting to look at the implications of both increasing complexity as well as scale of the network simulated.

Other notes:
A model based on McClelland, McNaughton, {\&}amp; O’Reilly (1995)!
Addressing the sensitivity-stability dilemma (D. O. Hebb, 1949), or the Stability-plasticity problem (Carenter {\&}amp; Grossberg, 1987). Catastrophic interference is a sensitivity-stability problem. Connectionist networks and lookup tables at opposite ends of the stability-sensitivity spectrum (the latter is completely stable for new information, but no generalisation). Standard FFBP are highly sensitive to new input. The paper proposes an architecture that relies on separating the previously learned representations from those that are currently being learned. A method for approx. the previously learned data will be extracted and mixed with new patterns.

“Unlike French (1992, 1994), however, in which specific learning algorithms were incorporated to produce the semi-distributed representations that alleviated catastrophic interference, in the case of the pseudo-recurrent network, these internal semi-distributed representations emerge naturally.”
Perhaps the ability to fill in the missing information is a crucial mechanism. Not only as in autoassociative memory, but also in filling in the missing gaps of a story, as well as more importantly: To perform a procedure similar to that of regression. Is interleaving and presenting pseudo-recurrent patterns a way of gradually filling in the outcome space?},
author = {French, Robert M},
doi = {10.1080/095400997116595},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/French - 1997 - Pseudo-recurrent Connectionist Networks An Approach to the 'Sensitivity-Stability' Dilemma.pdf:pdf},
issn = {0954-0091},
journal = {Connection Science},
number = {4},
pages = {353--380},
title = {{Pseudo-recurrent Connectionist Networks: An Approach to the 'Sensitivity-Stability' Dilemma}},
volume = {9},
year = {1997}
}
@phdthesis{Solbakken2009,
abstract = {In this thesis we develop a novel network model that extends the traditional artificial neural network (ANN) model to include oscillatory behaviour. This model is able to correctly classify combinations of previously learned input patterns by grouping features that belong to the same category. This grouping process is termed segmentation and we show how synchrony of oscillations is the necessary missing component of ANNs to be able to perform this segmentation. Using this model we go on to show that top-down modulatory feedback is necessary to enable separation of multiple objects in a scene and segmentation of their individual features. This type of feedback is distinctly different than recurrency and is what enables the rich dynamics between the nodes of our network. Additionally, we show how our model's dynamics avoid the combinatorial explosion in required training repetitions of traditional feed-forward classification networks. In these networks, relations between objects must explicitly be learned. In contrast, the dynamics of modulatory feedback allow us to defer calculation of these relations until run-time, thus creating a more robust system. We call our model Fuzzy Oscillations, and it achieves good results when compared to existing models. However, oscillatory neural network models successful in achieving segmentation are a relatively recent development. We thus feel that our model is a contribution to the field of oscillatory neural networks.},
author = {Solbakken, Lester Johan},
booktitle = {Institutt for datateknikk og informasjonsvitenskap, NTNU},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/masteroppgave Lester Solbakken.pdf:pdf},
pages = {85},
school = {The Norwegian University of Science and Technology, NTNU},
title = {{Fuzzy Oscillations: a Novel Model for Solving Pattern Segmentation}},
type = {Master's thesis},
url = {http://hdl.handle.net/11250/250363},
year = {2009}
}
@article{Roweis1999,
abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
author = {Roweis, S and Ghahramani, Z},
doi = {10.1162/089976699300016674},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roweis, Ghahramani - 1999 - A unifying review of linear gaussian models.pdf:pdf},
isbn = {0899766993000},
issn = {0899-7667},
journal = {Neural computation},
number = {2},
pages = {305--345},
pmid = {9950734},
title = {{A unifying review of linear gaussian models.}},
volume = {11},
year = {1999}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
annote = {a deep Q-network as presented in the yahoo-labs meeting, playing atari 2600 games},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Huber2012,
abstract = {The mechanisms linking sensation and action during learning are poorly understood. Layer 2/3 neurons in the motor cortex might participate in sensorimotor integration and learning; they receive input from sensory cortex and excite deep layer neurons, which control movement. Here we imaged activity in the same set of layer 2/3 neurons in the motor cortex over weeks, while mice learned to detect objects with their whiskers and report detection with licking. Spatially intermingled neurons represented sensory (touch) and motor behaviours (whisker movements and licking). With learning, the population-level representation of task-related licking strengthened. In trained mice, population-level representations were redundant and stable, despite dynamism of single-neuron representations. The activity of a subpopulation of neurons was consistent with touch driving licking behaviour. Our results suggest that ensembles of motor cortex neurons couple sensory input to multiple, related motor programs during learning.},
author = {Huber, D. and Gutnisky, D. a. and Peron, S. and O’Connor, D. H. and Wiegert, J. S. and Tian, L. and Oertner, T. G. and Looger, L. L. and Svoboda, K.},
doi = {10.1038/nature11039},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber et al. - 2012 - Multiple dynamic representations in the motor cortex during sensorimotor learning.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {7395},
pages = {473--478},
pmid = {22538608},
publisher = {Nature Publishing Group},
title = {{Multiple dynamic representations in the motor cortex during sensorimotor learning}},
url = {http://dx.doi.org/10.1038/nature11039},
volume = {484},
year = {2012}
}
@article{Lehman2014,
abstract = {Artificial intelligence (AI) is a sprawling field encompassing a diversity of approaches to machine intelligence and disparate perspectives on how intelligence should be viewed. Because researchers often engage only within their own specialized area of AI, there are many interesting broad questions about AI as a whole that often go unanswered. How should intelligence be abstracted in AI research? Which subfields, techniques, and abstractions are most promising? Why do researchers bet their careers on the particular abstractions and techniques of their chosen subfield of AI? Should AI research be "bio-inspired" and remain faithful to the process that produced intelligence (evolution) or the biological substrate that enables it (networks of neurons)? Discussing these big-picture questions motivated us to organize an AAAI Fall Symposium, which gathered participants across AI subfields to present and debate their views. This article distills the resulting insights.},
annote = {The authors summarise knowledge acquired at a symposium/meeting for AI researchers within various sub-fields that have begun to diverge. One of the most important thoughts presented here is that of Pierre-Yves Oudeyer, Inria and Ensta ParisTech, namely that {\&}quot;Adaptive thinking and acting is an embodied, situated, and dynamic complex system{\&}quot;. - quote the paper, Pp. 2},
author = {Lehman, Joel and Clune, Jeff and Risi, Sebastian},
doi = {10.1109/MIS.2014.92},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehman - 2014 - An Anarchy of Methods Current Trends in How Intelligence Is Abstracted in AI.pdf:pdf},
isbn = {1541-1672 VO - 29},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
keywords = {AI,AI abstraction,AI research,AI subfields,AI techniques,Adaptive systems,Artificial intelligence,Biological system modeling,Brain modeling,Computational modeling,Design methodology,Neural networks,Neuroscience,Robots,adaptive systems,artificial intelligence,bio-inspired research,cognitive science,computational neuroscience,deep learning,design automation,developmental robotics,evolving neural networks,intelligent systems,machine intelligence,neuroevolution},
number = {6},
pages = {56--62},
title = {{An Anarchy of Methods: Current Trends in How Intelligence Is Abstracted in AI}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6982117},
volume = {29},
year = {2014}
}
@article{Sutskever2010,
abstract = {A Recurrent Neural Network (RNN) is a powerful connectionist model that can be applied to many challenging sequential problems, including problems that naturally arise in language and speech. However, RNNs are extremely hard to train on problems that have long-term dependencies, where it is necessary to remember events for many timesteps before using them to make a prediction. In this paper we consider the problem of training RNNs to predict sequences that exhibit significant long-term dependencies, focusing on a serial recall task where the RNN needs to remember a sequence of characters for a large number of steps before reconstructing it. We introduce the Temporal-Kernel Recurrent Neural Network (TKRNN), which is a variant of the RNN that can cope with long-term dependencies much more easily than a standard RNN, and show that the TKRNN develops short-term memory that successfully solves the serial recall task by representing the input string with a stable state of its hidden units.},
annote = {A paper which nicely explains RNNs and TKRNNs (which the paper introduced).
Though a somewhat unusual (to me, at least) notation is used for the functions involved with the network, they introduce everything from scratch.
The main functionality of the TKRNN is to take into account what happened several timesteps in a leaky integrator fashion. This is done by weighting the contribution each timestep backwards in time exponentially less. We take into account the values of all connected nodes n time steps back in time. So if a node is connected to itself, the node's value is also taken into account (as part of the sum) n time steps backwards in time.},
author = {Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1016/j.neunet.2009.10.009},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Hinton - 2010 - Temporal-kernel recurrent neural networks.pdf:pdf},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Humans,Memory, Short-Term,Neural Networks (Computer),Neuropsychological Tests,Time Factors},
number = {2},
pages = {239--43},
pmid = {19932002},
title = {{Temporal-kernel recurrent neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19932002},
volume = {23},
year = {2010}
}
@article{LeRoux2008,
abstract = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.},
archivePrefix = {arXiv},
arxivId = {110008689868},
author = {{Le Roux}, Nicolas and Bengio, Yoshua},
doi = {10.1162/neco.2008.04-07-510},
eprint = {110008689868},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le Roux, Bengio - 2008 - Representational power of restricted boltzmann machines and deep belief networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1631--1649},
pmid = {18254699},
title = {{Representational power of restricted boltzmann machines and deep belief networks.}},
volume = {20},
year = {2008}
}
@article{Hedrick2013,
abstract = {The spatial component of input signals often carries information crucial to a neuron's function, but models mapping synaptic inputs to the transmembrane potential can be computationally expensive. Existing reduced models of the neuron either merge compartments, thereby sacrificing the spatial specificity of inputs, or apply model reduction techniques that sacrifice the underlying electrophysiology of the model. We use Krylov subspace projection methods to construct reduced models of passive and quasi-active neurons that preserve both the spatial specificity of inputs and the electrophysiological interpretation as an RC and RLC circuit, respectively. Each reduced model accurately computes the potential at the spike initiation zone (SIZ) given a much smaller dimension and simulation time, as we show numerically and theoretically. The structure is preserved through the similarity in the circuit representations, for which we provide circuit diagrams and mathematical expressions for the circuit elements. Furthermore, the transformation from the full to the reduced system is straightforward and depends on intrinsic properties of the dendrite. As each reduced model is accurate and has a clear electrophysiological interpretation, the reduced models can be used not only to simulate morphologically accurate neurons but also to examine computations performed in dendrites.},
author = {Hedrick, Kathryn R. and Cox, Steven J.},
doi = {10.1007/s10827-012-0403-y},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hedrick, Cox - 2013 - Structure-preserving model reduction of passive and quasi-active neurons.pdf:pdf},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Dendritic computation,Krylov methods,Model reduction,Passive model,Quasi-active model,Volterra series},
number = {1},
pages = {1--26},
pmid = {22714391},
title = {{Structure-preserving model reduction of passive and quasi-active neurons}},
volume = {34},
year = {2013}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v2},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {3C2DBCEE-8A96-493B-B88B-36B1F52ECA58},
eprint = {arXiv:1206.5538v2},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2013 - Representation Learning A Review and New Perspectives.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {1939-3539},
journal = {Tpami},
number = {1993},
pages = {1--30},
pmid = {23459267},
title = {{Representation Learning: A Review and New Perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23459267},
year = {2013}
}
@article{LeCun2015,
annote = {a good review article! :)},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Bengio2011,
abstract = {Deep architectures are families of functions corresponding to deep circuits. Deep Learning algorithms are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to train deep architectures, several successful algorithms have been proposed in recent years. We review some of the theoretical motivations for deep architectures, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges},
annote = {This article was published before great results were achieved by LSTM RNNs and CNNs.
“all of these algorithms will generalize well only to the extent that there are enough examples to cover all the regions that need to be distinguished from each other”.
Further questions: What about the nature of the distributions?
Deep architectures/networks may represent families of sum-product functions in exponentially less nodes than shallow networks.
On RBMs and stochastic maximum likelihood (SML) estimation: It gives rise to faster mixing. What about “mixing” (no pun intended) RBMs using an SML-like technique with oscillation for synchronisation in a sparse and distributed network? Shortly put; having some kind of a deep architecture, possibly a LSTM RNN constituting a DBN-component, pre-trained as RBMs using SML, having an auto-associative memory component as well, possibly with a special relay circuit determining the “focus” of the entire network (to enable the function of a sparse and distributed network) - which could make use of oscillations in the network for a WTA-mechanism, enabling a sparse activation.},
author = {Bengio, Yoshua and Delalleau, Olivier},
doi = {10.1007/978-3-642-24412-4{\_}3},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Delalleau - 2011 - On the expressive power of deep architectures.pdf:pdf},
isbn = {9783642244117},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {18--36},
title = {{On the expressive power of deep architectures}},
volume = {6925 LNAI},
year = {2011}
}
@misc{Hinton1989,
abstract = {Describes a deterministic Boltzmann machine (DBM) that learns much faster than the equivalent stochastic Boltzmann machine (SBM). By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A simple way of forcing the weights to become symmetrical makes the DBM more biologically plausible than back-propagation.},
author = {Hinton, Geoffrey E.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.1.143},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton - 1989 - Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space.pdf:pdf},
issn = {0899-7667},
number = {1},
pages = {143--150},
title = {{Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space}},
volume = {1},
year = {1989}
}
@article{French1994,
annote = {Context biasing. Once two different outputs has been presented to a standard FFBP-network, a context bias is calculated in the hidden layer, and propagated back to the input layer. Shortly put, this emphasises the differences of the distinct categories, focusing on segmenting and orthogonalising on the properties of difference - thus a more orthogonal, well distributed pattern is learned (and possibly more regularised?). 
Thoughts: Raw processing should focus on the key differences. But when this mechanism is exhausted - shouldn’t this kind of detailed segmentation possibly be employed?},
author = {French, R.M.},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/French - 1994 - Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophi.pdf:pdf},
journal = {Network},
pages = {00001},
title = {{Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.3558{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {1111},
year = {1994}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
annote = {The currently most successful techniques (2014): LSTM RNNs and GPU-MPCNNs (see the review paper sections 5.22, 5.23, etc. for references and other papers that will probably enable your to extract some knowledge by training your real NN).
The transformation of an entire network’s capacity into another part. How is this performed more biologically speaking?
...and: What about constructing a network that may perform several passes with different weightings, which are stored elsewhere - possibly it may be determined which ones to use by another network for a type of “memory recall” - effectively creating/simulating more networks. The question is: Is this no better than running two different networks sequentially, or might this be exploited further by interconnecting parts of the network? I.e. a morphological or partly topological blend determined by another slightly complex network? Just brainstorming.
See the conclusion of the paper that this section is notes on! -{\&}gt; Energy costs for neural computation in vivo. Thus a self-modularising allocation of RNNs for related cognition? This might be possible when extending the aforementioned idea in several networks. See other implementations of RNNs and similar as well as the numenta project.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {arXiv preprint arXiv:1404.7828},
pages = {1--66},
publisher = {Elsevier Ltd},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
volume = {61},
year = {2014}
}
@article{Buzsaki2005,
abstract = {Five key topics have been reverberating in hippocampal-entorhinal cortex (EC) research over the past five decades: episodic and semantic memory, path integration ("dead reckoning") and landmark ("map") navigation, and theta oscillation. We suggest that the systematic relations between single cell discharge and the activity of neuronal ensembles reflected in local field theta oscillations provide a useful insight into the relationship among these terms. In rats trained to run in direction-guided (1-dimensional) tasks, hippocampal cell assemblies discharge sequentially, with different assemblies active on opposite runs, i.e., place cells are unidirectional. Such tasks do not require map representation and are formally identical with learning sequentially occurring items in an episode. Hebbian plasticity, acting within the temporal window of the theta cycle, converts the travel distances into synaptic strengths between the sequentially activated and unidirectionally connected assemblies. In contrast, place representations by hippocampal neurons in 2-dimensional environments are typically omnidirectional, characteristic of a map. Generation of a map requires exploration, essentially a dead reckoning behavior. We suggest that omnidirectional navigation through the same places (junctions) during exploration gives rise to omnidirectional place cells and, consequently, maps free of temporal context. Analogously, multiple crossings of common junction(s) of episodes convert the common junction(s) into context-free or semantic memory. Theta oscillation can hence be conceived as the navigation rhythm through both physical and mnemonic space, facilitating the formation of maps and episodic/semantic memories.},
annote = {Might it be possible that this is what distinguishes sub-circuits constituting invariant representations? If these are combined in the hippocampus, it makes sense that there is some higher order of {\&}quot;phase-angle{\&}quot; that unifies abstractions of different {\&}quot;phase-angle{\&}quot;-sub-networks.},
author = {Buzs{\'{a}}ki, Gy{\"{o}}rgy},
doi = {10.1002/hipo.20113},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buzs{\'{a}}ki - 2005 - Theta rhythm of navigation Link between path integration and landmark navigation, episodic and semantic memory.pdf:pdf},
isbn = {1050-9631 (Print)},
issn = {10509631},
journal = {Hippocampus},
keywords = {Hippocampus,Oscillations,Place cells},
number = {7},
pages = {827--840},
pmid = {16149082},
title = {{Theta rhythm of navigation: Link between path integration and landmark navigation, episodic and semantic memory}},
volume = {15},
year = {2005}
}
@article{French1992,
annote = {On sparse distributed memory and semi-distributed representations in order to omit catastrophic forgetting. The SDM is an auto-associative, content-addressable memory using a very sparse way of distributing the memory. By having an incredibly large address space, information will rarely overlap and/or interfere. When it does, however (because the address space has been digested, etc.), neither new nor old information can be stored - thus a catastrophic forgetting will indeed occur. 
Another antidote for catastrophic forgetting is the ALCOVE algorithm, where the hidden nodes focuses more or less on local input nodes - making the network somewhat pre-specialised on certain inputs. From one PoV, it can be regarded as a regulariser. However, this network fails to generalise as well - for instance particularly when all of the hidden nodes should be specifically sensitive to say, one input node.
The main suggested solution: sharpening of activation of hidden units (log k, k is hidden nodes). It is observed to work fairly well. IMO: A lot has to do with the nature of the distribution that we’re working with. It’s also due to the nature of FFBP networks; it’s basically like PCA - a regulariser will work well to some extent when it’s chaotic - disregarding a lot of noise.},
author = {French, Robert M.},
doi = {10.1080/09540099208946624},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/French - 1992 - Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks.pdf:pdf},
issn = {0954-0091},
journal = {Connection Science},
number = {3-4},
pages = {365--377},
title = {{Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09540099208946624},
volume = {4},
year = {1992}
}
@misc{Buzsaki2002,
abstract = {Theta oscillations represent the "on-line" state of the hippocampus. The extracellular currents underlying theta waves are generated mainly by the entorhinal input, CA3 (Schaffer) collaterals, and voltage-dependent Ca2+ currents in pyramidal cell dendrites. The rhythm is believed to be critical for temporal coding/decoding of active neuronal ensembles and the modification of synaptic weights. Nevertheless, numerous critical issues regarding both the generation of theta oscillations and their functional significance remain challenges for future research.},
annote = {“The theta versus non-theta dichotomy” - two categories: “preparatory versus consummatory classes”},
author = {Buzs{\'{a}}ki, Gy{\"{o}}rgy},
booktitle = {Neuron},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/Read/buzsaki{\_}theta{\_}neuron2002.pdf:pdf},
number = {3},
pages = {325--340},
title = {{Theta oscillations in the hippocampus}},
volume = {33},
year = {2002}
}
@article{McClelland1995,
abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
annote = {Summary: The authors propose a memory model for the brain in which the hippocampus is responsible for the consolidation of memories into the neocortex, whereas it is the neocortex that stores the semantic, episodic, declarative, etc., memories. Whether the hippocampus functions as a working memory itself, or possibly operates on a short-term working memory present in the outer layers (2 and 3) of the neocortex, remains somewhat obscure (IMO the latter makes the most sense). In either way, the hippocampus is capable of fairly rapid information and knowledge acquisition based upon recent activation of the working memory. It is by the synthesis of recall from the deeper layers of the neocortex and representations in the working memory that contexts may be distinguished, or connected. The learning and consolidation to the neocortex is essentially performed in an interleaved fashion; slowly potentiating and instantiating these memories into the cortex.


“The answers we suggest to these questions arise from the study of learning in artificial neural network or connectionist models that adhere to many aspects of the account of the mammalian memory system provided earlier, but do not incorporate a special system for rapid acquisition of the contents of specific episodes and events. Such networks are similar to the neocortical processing system in that they may consist of several modules and pathways interconnecting the modules, but they are monolithic in the sense that knowledge is stored directly in the connections among the units of the system that carries out information processing, and there is no separate system for rapid learning of the contents of particular inputs.”
“(...) what one learns about something is stored in the connection weights among the units activated in representing it. That knowledge can be shared or generalized to other related things only if the patterns that represent these other things overlap (Hinton, McClelland, {\&}amp; Rumelhart, 1986).”
“ Dramatic confirmation of catastrophic effects of focused learning in real brains—and of the benefits of interleaved learning—can be found in the recent work of M. M. Merzenich (personal communication, January 18, 1995)”

‘Our claim is that experience can give rise to the gradual discovery of structure through interleaved learning but not through focused learning and that this gradual discovery process lies at the heart of cognitive, linguistic, and perceptual development.’ - Pp. 18
And that is precisely why, once we have developed natural language - we are able to gradually discover novel structure directly through the representations and structures that natural language provides us with.

{\&}quot;Reference versus working memory. The proposal of Olton, Becker, and Handelmann (1979) that the hippocampal system is necessary for what they called working memory (memory for recent information of specific current relevance) but not reference memory (memory for invariant aspects of a task situation) bears some similarity to our view that the cortex is specialized for the gradual discovery of the shared structure of events and experiences, whereas the hippocampus is necessary for the rapid storage of the contents of specific episodes and events. {\&}quot;},
author = {McClelland, J L and McNaughton, B L and O'Reilly, R C},
doi = {10.1037/0033-295X.102.3.419},
file = {:C$\backslash$:/Users/William/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, McNaughton, O'Reilly - 1995 - Why there are complementary learning systems in the hippocampus and neocortex insights from th.pdf:pdf},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {0033-295X},
journal = {Psychological review},
number = {3},
pages = {419--457},
pmid = {7624455},
title = {{Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.}},
volume = {102},
year = {1995}
}
@inbook{Russell2009chpt18,
address = {Essex CM20 2JE},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Pearson},
doi = {10.1017/S0269888900007724},
chapter = {18: Learning From Observations},
edition = {3rd editon},
isbn = {9780136042594},
issn = {0269-8889},
pages = {1152},
publisher = {Pearson Education Limited},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2009}
}
@book{Russell2009,
abstract = {The long-anticipated revision of this {\#}1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications.Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics.For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.},
address = {Essex CM20 2JE},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Pearson},
doi = {10.1017/S0269888900007724},
edition = {3rd editon},
isbn = {9780136042594},
issn = {0269-8889},
pages = {1152},
publisher = {Pearson Education Limited},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2009}
}
@book{Hertz1991,
abstract = {This book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest.},
author = {Hertz, J and Krogh, A and Palmer, R G},
booktitle = {Introduction to the Theory of Neural Computation},
isbn = {0201503956},
mendeley-groups = {Master-related},
pages = {327},
pmid = {25693158},
title = {{Introduction to the Theory of Neural Computation}},
url = {http://books.google.com/books?id=9a{\_}SyUG-A24C{\&}pgis=1},
volume = {1},
year = {1991},
publisher = {Westview Press}
}
@article{Bergstra2010,
abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
author = {Bergstra, James and Breuleux, Olivier and Bastien, Frederic Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
file = {:home/william/Downloads/Bergstra et al .2010. Theano A CPU and GPU Math Compiler in Python.pdf:pdf},
journal = {Proceedings of the Python for Scientific Computing Conference (SciPy)},
number = {Scipy},
pages = {1--7},
title = {{Theano: a CPU and GPU math compiler in Python}},
url = {http://www-etud.iro.umontreal.ca/{~}wardefar/publications/theano{\_}scipy2010.pdf},
year = {2010}
}
@article{Bastien2012,
abstract = {Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5590v1},
author = {Bastien, F and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian and Bergeron, Arnaud and Bouchard, Nicolas and Warde-Farley, David and Bengio, Yoshua},
eprint = {arXiv:1211.5590v1},
file = {:home/william/Downloads/Bastien et al. 2012. Theano new features and speed improvements.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
title = {{Theano: new features and speed improvements}},
url = {http://arxiv.org/abs/1211.5590},
year = {2012}
}
@misc{LISA-lab2015a,
author = {LISA-lab},
title = {Theano 0.7 Documentation},
note = {Internet www page at URL: \url{http://deeplearning.net/software/theano/index.html\#documentation} (accessed 15/12/2015)},
urldate = {2015-12-15},
year = {2015}
}
@misc{LISA-lab2015b,
author = {LISA-lab},
title = {Installing Theano},
note = {Internet www page at URL: \url{http://deeplearning.net/software/theano/install.html\#install} (accessed 15/12/2015)},
url = {http://deeplearning.net/software/theano/install.html\#install},
urldate = {2015-12-15},
year = {2015}
}
@misc{LISA-lab2015c,
author = {LISA-lab},
keywords = {Theano 0.7 Documentation - Tutorial},
mendeley-tags = {Theano 0.7 Documentation - Tutorial},
title = {{Using the GPU}},
note = {URL: \url{http://deeplearning.net/software/theano/tutorial/using\_gpu.html} (accessed 15/12/2015)},
url = {http://deeplearning.net/software/theano/tutorial/using\_gpu.html},
urldate = {2015-12-15},
year = {2015}
}
@misc{Raffel2015,
author = {Raffel, Colin},
title = {{Theano Tutorial. Example: MLP}},
note = {Internet www page at URL: \url{http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano Tutorial.ipynb} (accessed 01/12/2015)},
url = {http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano Tutorial.ipynb},
urldate = {2015-12-01},
year = {2015}
}
@inproceedings{Urmson2009,
abstract = {Boss is an autonomous vehicle that uses on-board sensors (global positioning system, lasers, radars, and cameras) to track other vehicles, detect static obstacles, and localize itself relative to a road model. A three-layer planning system combines mission, behavioral, and motion planning to drive in urban environments. The mission planning layer considers which street to take to achieve a mission goal. The behavioral layer determines when to change lanes and precedence at intersections and performs error recovery maneuvers. The motion planning layer selects actions to avoid obstacles while making progress toward local goals. The system was developed from the ground up to address the requirements of the DARPA Urban Challenge using a spiral system development process with a heavy emphasis on regular, regressive system testing. During the National Qualification Event and the 85-km Urban Challenge Final Event, Boss demonstrated some of its capabilities, qualifying first and winning the challenge.},
author = {Urmson, Chris and Anhalt, Joshua and Bagnell, Drew and Baker, Christopher and Bittner, Robert and Clark, M. N. and Dolan, John and Duggins, Dave and Galatali, Tugrul and Geyer, Chris and Gittleman, Michele and Harbaugh, Sam and Hebert, Martial and Howard, Thomas M. and Kolski, Sascha and Kelly, Alonzo and Likhachev, Maxim and McNaughton, Matt and Miller, Nick and Peterson, Kevin and Pilnick, Brian and Rajkumar, Raj and Rybski, Paul and Salesky, Bryan and Seo, Young Woo and Singh, Sanjiv and Snider, Jarrod and Stentz, Anthony and Whittaker, William and Wolkowicki, Ziv and Ziglar, Jason and Bae, Hong and Brown, Thomas and Demitrish, Daniel and Litkouhi, Bakhtiar and Nickolaou, Jim and Sadekar, Varsha and Zhang, Wende and Struble, Joshua and Taylor, Michael and Darms, Michael and Ferguson, Dave},
booktitle = {Springer Tracts in Advanced Robotics},
doi = {10.1007/978-3-642-03991-1{\_}1},
isbn = {9783642039904},
issn = {16107438},
pages = {1--59},
title = {{Autonomous driving in Urban environments: Boss and the Urban Challenge}},
volume = {56},
year = {2009}
}
@article{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {978-1-4799-5118-5},
issn = {10636919},
journal = {Nips},
pages = {1--9},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Wagle2013,
abstract = {At the International Science Conference in March, Kunal presented a poster about the new supercomputer developed by IBM called Watson and discussed the possibility that it could in fact revolutionize the way we view the current healthcare system. In this paper, he discusses what Watson is, how it works, how it rose to fame, and its possible uses for the future, as well as how this machine, or any other supercomputers of the future, can alter the healthcare system for ever.},
author = {Wagle, Kunal},
doi = {http://dx.doi.org/10.4103/0974-6102.107613},
issn = {09746102},
journal = {Young Scientists Journal},
number = {13},
pages = {17--19},
title = {{IBM Watson: Revolutionizing healthcare?}},
url = {},
volume = {6},
year = {2013}
}
@article{Kiernan2011,
author = {Kiernan, Matthew C},
doi = {10.1136/jnnp.2010.224998},
file = {:C$\backslash$:/Users/William/Dropbox/NTNU/Master/Papers/J Neurol Neurosurg Psychiatry-2011-Kiernan-120.pdf:pdf},
isbn = {1468-330X (Electronic)$\backslash$n0022-3050 (Linking)},
issn = {1468-330X},
journal = {Journal of neurology, neurosurgery, and psychiatry},
keywords = {History, 19th Century,History, 20th Century,Humans,Learning,Learning: physiology,Nerve Net,Nerve Net: physiology,Neurology,Neurology: history,Neuronal Plasticity,Neuronal Plasticity: physiology,Psychoanalysis,Psychoanalysis: history,Synapses,Synapses: physiology},
number = {2},
pages = {120},
pmid = {21097548},
title = {{Freud, neurology and the emergence of dynamic neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21097548},
volume = {82},
year = {2011}
}
@book{Rolls1998,
abstract = {This book describes the types of computation that can be performed by biologically plausible neural networks and shows how they may be implemented in different systems of the brain. It is structured in three sections, each of which addresses a different need. The first introduces and analyzes the operation of several fundamental types of neural networks. The second discusses real neural networks in several brain systems, and shows how it is becoming possible to construct theories about the way different parts of the brain work. This section also analyzes the various neuroscience and neurocomputation techniques that need to be combined to ensure further progress in understanding the mechanism of brain processes. The third section, a collection of appendices. introduces the formal quantitative approaches to many of the networks described. Neural Networks and Brain Function is an accessible, clear introduction for researchers and students in neuroscience and artificial intelligence to the fascinating problems of how the brain works and how behavior is determined.},
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
isbn = {0198524323},
mendeley-groups = {Master-related},
pages = {418},
publisher = {Oxford University Press},
title = {{Neural Networks and Brain Function}},
year = {1998}
}
@inbook{Rolls1998chpt2,
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
booktitle = {Neural Networks and Brain Function},
publisher = {Oxford University Press},
isbn = {0198524323},
title = {Neural Networks and Brain Function},
mendeley-groups = {Master-related},
pages = {23--41},
year = {1998},
chapter = {2: Pattern association memory}
}
@inbook{Rolls1998chpt3,
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
booktitle = {Neural Networks and Brain Function},
publisher = {Oxford University Press},
isbn = {0198524323},
title = {Neural Networks and Brain Function},
mendeley-groups = {Master-related},
pages = {42--53},
year = {1998},
chapter = {3: Autoassociation memory}
}
@inbook{Rolls1998chpt4,
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
booktitle = {Neural Networks and Brain Function},
publisher = {Oxford University Press},
isbn = {0198524323},
title = {Neural Networks and Brain Function},
mendeley-groups = {Master-related},
pages = {54--74},
year = {1998},
chapter = {4: Competitive networks, including self-organizing maps}
}
@inbook{Rolls1998chpt6,
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
booktitle = {Neural Networks and Brain Function},
publisher = {Oxford University Press},
isbn = {0198524323},
title = {Neural Networks and Brain Function},
mendeley-groups = {Master-related},
pages = {95--135},
year = {1998},
chapter = {6: The hippocampus and memory}
}
@inbook{Rolls1998chpt1,
address = {Oxford, UK},
author = {Rolls, Edmund T and Treves, Alessandro},
booktitle = {{Neural Networks and Brain Function}},
publisher = {Oxford University Press},
isbn = {0198524323},
title = {{Neural Networks and Brain Function}},
chapter = {1: Introduction},
mendeley-groups = {Master-related},
pages = {1--22},
year = {1998}
}
@misc{Bush2014,
abstract = {The unitary firing fields of hippocampal place cells are commonly assumed to be generated by input from entorhinal grid cell modules with differing spatial scales. Here, we review recent research that brings this assumption into doubt. Instead, we propose that place cell spatial firing patterns are determined by environmental sensory inputs, including those representing the distance and direction to environmental boundaries, while grid cells provide a complementary self-motion related input that contributes to maintaining place cell firing. In this view, grid and place cell firing patterns are not successive stages of a processing hierarchy, but complementary and interacting representations that work in combination to support the reliable coding of large-scale space. ?? 2014 Elsevier Ltd.},
author = {Bush, Daniel and Barry, Caswell and Burgess, Neil},
booktitle = {Trends in Neurosciences},
doi = {10.1016/j.tins.2013.12.003},
isbn = {0166-2236},
issn = {01662236},
keywords = {Border cells,Boundary vector cells,Grid cells,Hippocampus,Place cells},
mendeley-groups = {Master-related},
number = {3},
pages = {136--145},
pmid = {24485517},
title = {{What do grid cells contribute to place cell firing?}},
volume = {37},
year = {2014}
}
@article{Damasio1989,
abstract = {This article outlines a theoretical framework for the understanding of the neural basis of memory and consciousness, at systems level. It proposes an architecture constituted by: (1) neuron ensembles located in multiple and separate regions of primary and first-order sensory association cortices ("early cortices") and motor cortices; they contain representations of feature fragments inscribed as patterns of activity originally engaged by perceptuomotor interactions; (2) neuron ensembles located downstream from the former throughout single modality cortices (local convergence zones); they inscribe amodal records of the combinatorial arrangement of feature fragments that occurred synchronously during the experience of entities or events in sector (1); (3) neuron ensembles located downstream from the former throughout higher-order association cortices (non-local convergence zones), which inscribe amodal records of the synchronous combinatorial arrangements of local convergence zones during the experience of entities and events in sector (1); (4) feed-forward and feedback projections interlocking reciprocally the neuron ensembles in (1) with those in (2) according to a many-to-one (feed-forward) and one-to-many (feedback) principle. I propose that (a) recall of entities and events occurs when the neuron ensembles in (1) are activated in time-locked fashion; (b) the synchronous activations are directed from convergence zones in (2) and (3); and (c) the process of reactivation is triggered from firing in convergence zones and mediated by feedback projections. This proposal rejects a single anatomical site for the integration of memory and motor processes and a single store for the meaning of entities of events. Meaning is reached by time-locked multiregional retroactivation of widespread fragment records. Only the latter records can become contents of consciousness. ?? 1989.},
author = {Damasio, Antonio R.},
doi = {10.1016/0010-0277(89)90005-X},
isbn = {0010-0277 (Print)$\backslash$r0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
mendeley-groups = {Master-related},
number = {1-2},
pages = {25--62},
pmid = {2691184},
title = {{Time-locked multiregional retroactivation: A systems-level proposal for the neural substrates of recall and recognition}},
volume = {33},
year = {1989}
}
@article{Fyhn2007,
abstract = {A fundamental property of many associative memory networks is the ability to decorrelate overlapping input patterns before information is stored. In the hippocampus, this neuronal pattern separation is expressed as the tendency of ensembles of place cells to undergo extensive 'remapping' in response to changes in the sensory or motivational inputs to the hippocampus. Remapping is expressed under some conditions as a change of firing rates in the presence of a stable place code ('rate remapping'), and under other conditions as a complete reorganization of the hippocampal place code in which both place and rate of firing take statistically independent values ('global remapping'). Here we show that the nature of hippocampal remapping can be predicted by ensemble dynamics in place-selective grid cells in the medial entorhinal cortex, one synapse upstream of the hippocampus. Whereas rate remapping is associated with stable grid fields, global remapping is always accompanied by a coordinate shift in the firing vertices of the grid cells. Grid fields of co-localized medial entorhinal cortex cells move and rotate in concert during this realignment. In contrast to the multiple environment-specific representations coded by place cells in the hippocampus, local ensembles of grid cells thus maintain a constant spatial phase structure, allowing position to be represented and updated by the same translation mechanism in all environments encountered by the animal.},
author = {Fyhn, Marianne and Hafting, Torkel and Treves, Alessandro and Moser, May-Britt and Moser, Edvard I},
doi = {10.1038/nature05601},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Animals,Brain Mapping,Cues,Entorhinal Cortex,Entorhinal Cortex: cytology,Entorhinal Cortex: physiology,Hippocampus,Hippocampus: cytology,Hippocampus: physiology,Long-Evans,Models,Neurological,Rats,Synapses,Synapses: metabolism,Synapses: physiology},
mendeley-groups = {Master-related},
number = {7132},
pages = {190--4},
pmid = {17322902},
title = {{Hippocampal remapping and grid realignment in entorhinal cortex}},
url = {http://dx.doi.org/10.1038/nature05601},
volume = {446},
year = {2007}
}
@article{Hafting2005,
abstract = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the 'grid cell', which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
author = {Hafting, T. and Fyhn, M. and Molden, S. and Moser, M. and Moser, E. I.},
doi = {10.1038/nature03721},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Master-related},
number = {7052},
pages = {801--806},
pmid = {15965463},
title = {{Microstructure of a spatial map in the entorhinal cortex.}},
volume = {436},
year = {2005}
}
@article{Hasselmo1995,
abstract = {Hippocampal region CA3 contains strong recurrent excitation mediated by synapses of the longitudinal association fibers. These recurrent excitatory connections may play a dominant role in determining the information processing characteristics of this region. However, they result in feedback dynamics that may cause both runaway excitatory activity and runaway synaptic modification. Previous models of recurrent excitation have prevented unbounded activity using biologically unrealistic techniques. Here, the activation of feedback inhibition is shown to prevent unbounded activity, allowing stable activity states during recall and learning. In the model, cholinergic suppression of synaptic transmission at excitatory feedback synapses is shown to determine the extent to which activity depends upon new features of the afferent input versus components of previously stored representations. Experimental work in brain slice preparations of region CA3 demonstrates the cholinergic suppression of synaptic transmission in stratum radiatum, which contains synapses of the longitudinal association fibers.},
author = {Hasselmo, M E and Schnell, E and Barkai, E},
isbn = {0270-6474 (Print)},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {associative},
mendeley-groups = {Master-related},
number = {7 Pt 2},
pages = {5249--5262},
pmid = {7623149},
title = {{Dynamics of learning and recall at excitatory recurrent synapses and cholinergic modulation in rat hippocampal region CA3.}},
volume = {15},
year = {1995}
}
@article{Hawkins2004,
abstract = {Jeff Hawkins, the man who created the PalmPilot, Treo smart phone, and other handheld devices, has reshaped our relationship to computers. Now he stands ready to revolutionize both neuroscience and computing in one stroke, with a new understanding of intelligence itself. Hawkins develops a powerful theory of how the human brain works, explaining why computers are not intelligent and how, based on this new theory, we can finally build intelligent machines. The brain is not a computer, but a memory system that stores experiences in a way that reflects the true structure of the world, remembering sequences of events and their nested relationships and making predictions based on those memories. It is this memory-prediction system that forms the basis of intelligence, perception, creativity, and even consciousness. In an engaging style that will captivate audiences from the merely curious to the professional scientist, Hawkins shows how a clear understanding of how the brain works will make it possible for us to build intelligent machines, in silicon, that will exceed our human ability in surprising ways.},
author = {Hawkins, Jeff and Blakeslee, Sandra},
isbn = {978-0805074567},
journal = {Neural Networks},
mendeley-groups = {Master-related},
pages = {272},
title = {{On intelligence}},
url = {www.onintelligence.com},
year = {2004}
}
@article{Jeffery2013,
abstract = {<p>The study of spatial cognition has provided considerable insight into how animals (including humans) navigate on the horizontal plane. However, the real world is three-dimensional, having a complex topography including both horizontal and vertical features, which presents additional challenges for representation and navigation. The present article reviews the emerging behavioral and neurobiological literature on spatial cognition in non-horizontal environments. We suggest that three-dimensional spaces are represented in a quasi-planar fashion, with space in the plane of locomotion being computed separately and represented differently from space in the orthogonal axis – a representational structure we have termed “bicoded.” We argue that the mammalian spatial representation in surface-travelling animals comprises a mosaic of these locally planar fragments, rather than a fully integrated volumetric map. More generally, this may be true even for species that can move freely in all three dimensions, such as birds and fish. We outline the evidence supporting this view, together with the adaptive advantages of such a scheme.</p>},
author = {Jeffery, Kathryn J. and Jovalekic, Aleksandar and Verriotis, Madeleine and Hayman, Robin},
doi = {10.1017/S0140525X12002476},
isbn = {9780262016636},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {ethology,grid cells,head direction cells,hippocampus,navigation,neural encoding,place cells,spatial cognition,three-},
mendeley-groups = {Master-related},
number = {05},
pages = {523--543},
pmid = {24103594},
title = {{Navigating in a three-dimensional world}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X12002476},
volume = {36},
year = {2013}
}
@misc{Lisman1995,
abstract = {Psychophysical measurements indicate that human subjects can store approximately seven short-term memories. Physiological studies suggest that short-term memories are stored by patterns of neuronal activity. Here it is shown that activity patterns associated with multiple memories can be stored in a single neural network that exhibits nested oscillations similar to those recorded from the brain. Each memory is stored in a different high-frequency ("40 hertz") subcycle of a low-frequency oscillation. Memory patterns repeat on each low-frequency (5 to 12 hertz) oscillation, a repetition that relies on activity-dependent changes in membrane excitability rather than reverberatory circuits. This work suggests that brain oscillations are a timing mechanism for controlling the serial processing of short-term memories.},
author = {Lisman, J E and Idiart, M A},
booktitle = {Science (New York, N.Y.)},
doi = {10.1126/science.7878473},
isbn = {0036-8075},
issn = {0036-8075},
mendeley-groups = {Master-related},
number = {5203},
pages = {1512--1515},
pmid = {7878473},
title = {{Storage of 7 +/- 2 short-term memories in oscillatory subcycles.}},
volume = {267},
year = {1995}
}
@incollection{McNaughton1990,
abstract = {(from the chapter) illustrate how some simple network models are guiding the design and interpretation of neurophysiological research concerned with understanding the neural coding, storage, and transformation of spatial information / begins with an analysis of spatial cognition, moves on to a discussion of network models in the nervous system, continues with a description of certain aspects of the anatomy, physiology, and dynamics of a particular neural system--the hippocampal formation--known to be critical in spatial cognition, and concludes with the description of a conceptual model of spatial representation and cognition that reflects many of these structural and functional features (from the preface) show that the empirical physiological characteristics of plasticity (synaptic learning) in the hippocampus are generally consistent with the kinds of Hebbian autoassociative models frequently studied in connectionist theory / point out . . . the important role of the temporal interactions among diverse circuit elements, especially inhibitory interneurons, which are often ignored in more abstract formal models ((c) 1999 APA/PsycINFO, all rights reserved)},
author = {McNaughton, Bruce L and Nadel, Lynn},
booktitle = {Neuroscience and connectionist theory},
isbn = {0805805044},
mendeley-groups = {Master-related},
pages = {1--64},
title = {{Hebb-Marr Networks and the Neurobiological Representation of Space}},
year = {1990}
}
@misc{Mountcastle1978,
abstract = {This significant contribution to neuroscience consists of two papers, the first by Mountcastle an, the second by Edelman. Between them, they examine from different but complementary directions the relationships that connect the higher brain-memory, learning, perception, thinking-with what goes on at the most basic levels of neural activity, with particular stress on the role of local neuronal circuits.Edelman's major hypothesis is that "the conscious state results from phasic reentrant signaling occurring in parallel processes that involve associations between stored patterns and current sensory or internal input." This selective process occurs by the polling of degenerate primary repertoires of neuronal groups that are formed during embryogenesis and development. Edelman's theory extrapolates to the brain the selectionistic immunological theories for which he was awarded the 1972 Nobel Prize in Physiology or Medicine.Mountcastle's paper reviews what is known about the actual structure of various parts of the neo cortex. He relates the large entities of the neocortex to their component modules-the local neuronal circuits-and shows how the complex interrelationships of such a distributed system can yield dynamic distributed functioning.There are strong conceptual parallels between Mountcastle's idea of cortical columns and their functional subunits and Edelman's concept of populations of neurons functioning as processors in a brain system based on selectional rather than instructional principles. These parallels are traced and put into perspective in Francis Schmitt's Introduction.},
author = {Mountcastle, V.},
booktitle = {The Mindful Brain},
isbn = {0262550075},
mendeley-groups = {Master-related},
pages = {7--50},
pmid = {168},
title = {{An organizing principle for cerebral function: the unit model and the distributed system}},
url = {http://www.citeulike.org/group/8299/article/4545635},
year = {1978}
}
@article{OKeefe1996,
abstract = {The human hippocampus has been implicated in memory, in particular episodic or declarative memory. In rats, hippocampal lesions cause selective spatial deficits, and hippocampal complex spike cells (place cells) exhibit spatially localized firing, suggesting a role in spatial memory, although broader functions have also been suggested. Here we report the identification of the environmental features controlling the location and shape of the receptive fields (place fields) of the place cells. This was done by recording from the same cell in four rectangular boxes that differed solely in the length of one or both sides. Most of our results are explained by a model in which the place field is formed by the summation of gaussian tuning curves, each oriented perpendicular to a box wall and peaked at a fixed distance from it.},
author = {O'Keefe, J and Burgess, N},
doi = {10.1038/381425a0},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Master-related},
number = {6581},
pages = {425--428},
pmid = {8632799},
title = {{Geometric determinants of the place fields of hippocampal neurons.}},
volume = {381},
year = {1996}
}
@article{OKeefe1976,
abstract = {Single units were recorded from the CA1 field of the hippocampus in the freely-moving rat. They were classified as place units, displace units or others. Place units were defined as those for which the rat's position on the maze was a necessary condition for maximal unit firing. Some of these place units (misplace units) fired maximally when the animal sniffed in a place, either because it found something new there or failed to find something which was usually there. Displace units increased their rates during behaviors associated with theta activity in the hippocampal slow waves. In general these were behaviors which changed the rat's position relative to the environment. The influence of various environmental manipulations (e.g., turning off the room lights) on the firing pattern of the place units was tested and the results suggest that they were not responding to a simple sensory stimulus nor to a specific motor behavior. Nor could the unit firing be due purely to motivational or incentive factors. The results are interpreted as strong support for the cognitive map theory of hippocampal function. ?? 1976.},
author = {O'Keefe, John},
doi = {10.1016/0014-4886(76)90055-8},
isbn = {0014-4886},
issn = {10902430},
journal = {Experimental Neurology},
mendeley-groups = {Master-related},
number = {1},
pages = {78--109},
pmid = {1261644},
title = {{Place units in the hippocampus of the freely moving rat}},
volume = {51},
year = {1976}
}
@article{Rumelhart1985,
abstract = {This paper reports the results of our studies with an unsupervised learning paradigm which we have called "Competitive Learning." We have examined competitive learning using both computer simulation and formal analysis and have found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide a way to discover the salient, general features which can be used to classify a set of patterns. We show how a very simply competitive mechanism can discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We also show how these feature detectors can form the basis of a multilayer system that can serve to learn categorizations of stimulus sets which are not linearly separable. We show how the use of correlated stimuli con serve as a kind of "teaching" input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism a very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is an essentially nonassociative statistical learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in a more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features important in the description of the stimulus environment in which the system finds itself. ?? 1985.},
author = {Rumelhart, David E. and Zipser, David},
doi = {10.1016/S0364-0213(85)80010-0},
isbn = {0-262-68053-X},
issn = {03640213},
journal = {Cognitive Science},
mendeley-groups = {Master-related},
number = {1},
pages = {75--112},
title = {{Feature discovery by competitive learning}},
volume = {9},
year = {1985}
}
@article{Sargolini2006,
abstract = {Grid cells in the medial entorhinal cortex (MEC) are part of an environment-independent spatial coordinate system. To determine how information about location, direction, and distance is integrated in the grid-cell network, we recorded from each principal cell layer of MEC in rats that explored two-dimensional environments. Whereas layer II was predominated by grid cells, grid cells colocalized with head-direction cells and conjunctive grid x head-direction cells in the deeper layers. All cell types were modulated by running speed. The conjunction of positional, directional, and translational information in a single MEC cell type may enable grid coordinates to be updated during self-motion-based navigation.},
author = {Sargolini, Francesca and Fyhn, Marianne and Hafting, Torkel and McNaughton, Bruce L and Witter, Menno P and Moser, May-Britt and Moser, Edvard I},
doi = {10.1126/science.1125572},
isbn = {1095-9203 (Electronic)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
mendeley-groups = {Master-related},
number = {5774},
pages = {758--762},
pmid = {16675704},
title = {{Conjunctive representation of position, direction, and velocity in entorhinal cortex.}},
volume = {312},
year = {2006}
}
@article{Sharp1991,
abstract = {Hippocampal pyramidal cells show location-specific firing as animals navigate through an en- vironment. It has been suggested that this firing could result from the "local view" available in a cell's field. Hippocampal damage results in learning deficits on a wide variety oftasks. This, along with the fact that an associative form ofplasticity has been discovered in the hippocampus has led to the idea that this strueture might serve as a distributed, associative-matrix memory device. Here, these ideas are combined in a model in which pyramidal cells are the output layer of a competitive-learning, pattern-classification device. The inputs are patterns of environmen- tal stimuli as viewed by a computerized "rat" from various locations within a simulated environ- ment. These patterns are "classified" on the basis oftheir similarity. Since views available from contiguous regions of space are similar, single cells come to fire in a circumscribed region (place field). Firing-rate maps for these theoretical units show place fields remarkably similar to those of actual place cells. Also, they show remarkably similar behavior to that of real cells when tested under some of the probe conditions similar to those which have been used for actual cells.},
author = {Sharp, Patricia E},
doi = {10.3758/BF03327179},
isbn = {0889-6313(Print)},
issn = {0889-6313},
journal = {Psychobiology},
mendeley-groups = {Master-related},
number = {2},
pages = {103--115},
title = {{Computer simulation of hippocampal place cells}},
volume = {19},
year = {1991}
}
@article{Kruschke1992,
abstract = {ALCOVE (attention learning covering map) is a connectionist model of category learning that incorporates an exemplar-based representation (Medin {\&} Schaffer, 1978; Nosofsky, 1986) with error-driven learning (Gluck {\&} Bower, 1988; Rumelhart, Hinton, {\&} Williams, 1986). Alcove selectively attends to relevant stimulus dimensions, is sensitive to correlated dimensions, can account for a form of base-rate neglect, does not suffer catastrophic forgetting, and can exhibit 3-stage (U-shaped) learning of high-frequency exceptions to rules, whereas such effects are not easily accounted for by models using other combinations of representation and learning method.},
author = {Kruschke, J K},
doi = {10.1037/0033-295X.99.1.22},
isbn = {0033-295X$\backslash$n1939-1471},
issn = {0033-295X},
journal = {Psychological review},
keywords = {Attention,Discrimination Learning,Generalization (Psychology),Humans,Mental Recall,Models,Neural Networks (Computer),Pattern Recognition,Psychological,Reinforcement (Psychology),Visual},
mendeley-groups = {Master-related},
number = {1},
pages = {22--44},
pmid = {1546117},
title = {{ALCOVE: an exemplar-based connectionist model of category learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1546117},
volume = {99},
year = {1992}
}
@incollection{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
editor = {{F. Pereira, and C. J. C. Burges, and L. Bottou}, and K. Q. Weinberger},
eprint = {1102.0183},
file = {:home/william/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
mendeley-groups = {Master-related},
pages = {1097-1105},
pmid = {7491034},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}

@book{Campbell2015,
abstract = {«Platelets are pinched-off cytoplasmic fragments of specialized bone marrow cells. They are about 2-3µm in diameter and have no nuclei. Platelets serve both structural and molecular functions in blood clotting.},
address = {Edinburgh Gate, Harlow, Essex CM20 2JE, England},
author = {Campbell, Neil A. and Reece, Jane},
booktitle = {Biology},
edition = {Tenth edit},
isbn = {978-0321775658},
mendeley-groups = {Master-related},
pages = {1488},
publisher = {Pearson Education Limited},
title = {{Biology}},
year = {2015}
}
@inbook{Campbell2015chpt9,
address = {Edinburgh Gate, Harlow, Essex CM20 2JE, England},
author = {Campbell, Neil A. and Reece, Jane},
booktitle = {Biology},
edition = {Tenth edit},
isbn = {978-0321775658},
mendeley-groups = {Master-related},
pages = {216-237},
chapter = {9: Cellular Signaling},
publisher = {Pearson Education Limited},
title = {{Biology}},
year = {2015}
}

@article{Marr1971,
abstract = {It is proposed that the most important characteristic of archicortex is its ability to perform a simple kind of memorizing task. It is shown that rather general numerical constraints roughly determine the dimensions of memorizing models for the mammalian brain, and from these is derived a general model for archicortex. The addition of further constraints leads to the notion of a simple representation, which is a way of translating a great deal of information into the firing of about 200 out of a population of 10{\$}{\^{}}5{\$} cells. It is shown that if about 10{\$}{\^{}}5{\$} simple representations are stored in such a population of cells, very little information about a single learnt event is necessary to provoke its recall. A detailed numerical examination is made of a particular example of this kind of memory, and various general conclusions are drawn from the analysis. The insight gained from these models is used to derive theories for various archicortical areas. A functional interpretation is given of the cells and synapses of the area entorhinalis, the prcsubiculum, the prosubiculum, the cornu ammonis and the fascia dentata. Many predictions are made, a substantial number of which must be true if the theory is correct. A general functional classification of typical archicortical cells is proposed.},
author = {Marr, D},
doi = {10.1098/rstb.1971.0078},
isbn = {0962-8436 (Print)$\backslash$r0962-8436 (Linking)},
issn = {0962-8436},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
mendeley-groups = {Master-related},
number = {841},
pages = {23--81},
pmid = {4399412},
title = {{Simple memory: a theory for archicortex.}},
volume = {262},
year = {1971}
}
@article{Wakagi2008,
abstract = {Abstract—Recent studies have revealed that neurons are replaced in the dentate gyrus of the adult hippocampus. Although it is believed that the hippocampus is essential to store some kinds of memory, the role of neuronal turnover in the hippocampus have not been understood yet. In this paper, we examine the effect of neuronal turnover by using a hippocampal model. Computer simulation results show that the similarity of patterns to be stored is reduced by neuronal turnover, and this contributes to storing similar patterns easily and increasing the storage capacity. Moreover, we show that the number of learning epochs required to store all patterns can be reduced as the neuronal turnover rate becomes large.},
author = {{Wakagi, Yuko; Hattori}, Motonobu},
file = {:home/williapb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wakagi, Yuko Hattori - 2008 - A Model of Hippocampal Learning with Neuronal Turnover in Dentate Gyrus.a{\_}{\_}model{\_}{\_}:a{\_}{\_}model{\_}{\_}},
journal = {INTERNATIONAL JOURNAL OF MATHEMATICS AND COMPUTERS IN SIMULATION},
keywords = {dentate gyrus,genesis,hippocampus,however,in the hippocampus is,neural networks,neuro-,neuronal turnover,still,the role of neurogenesis},
mendeley-groups = {Master-related},
number = {2},
pages = {215--222},
title = {{A Model of Hippocampal Learning with Neuronal Turnover in Dentate Gyrus}},
volume = {2},
year = {2008}
}
@book{Kreyszig2011,
abstract = {This market leading text is known for its comprehensive coverage, careful and correct mathematics, outstanding exercises and self contained subject matter parts for maximum flexibility.New to This Edition: New problem sets. Modern engineering mathematics is mostly teamwork. It usually combines analytic work in the process of modeling and the use of computer algebra and numerics in the process of solution, followed by critical evaluation of results. Our problems- some straightforward, some more challenging, some "thinking problems" not accessible by a CAS, some open-ended- reflect this modern situation. Computer Experiments , using the computer as an instrument of "experimental mathematics" for exploration and research. These are mostly open-ended experiments, demonstrating the use of computers in experimentally finding results, which may be provable afterward or may be valuable heuristic qualitative guidelines to the engineer, in particular in complicated problems. More on modeling and selecting methods, tasks that usually cannot be automated. Many sections were rewritten in a more detailed fashion, to make it a simpler book. This also resulted in a better balance between theory and applications. Student Solutions Manual and Study Guide enlarged, upon explicit requests of the users. This manual contains worked-out solutions to carefully selected odd-numbered problems as well as general comments and hints on studying the text and working further problems. WileyPLUS, provides online algorithmically generated homework, ebook, and course management features that cannot be not found with any other engineering mathematics text.Hallmark Features: Simplicity of examples, to make the book teachable- why choose complicated examples when simple ones are as instructive or even better? Independence of chapters, to provide flexibility in tailoring courses to special needs. Self-contained presentation, except for a few clearly marked places where a proof would exceed the level of the book and a reference is given instead. Modern Standard Notation, to help students with other courses, modern books, and mathematical and engineering journals.},
author = {Kreyszig, Erwin},
doi = {10.2307/3612523},
edition = {10th},
issn = {00255572},
mendeley-groups = {Master-related},
pages = {1094},
publisher = {JOHN WILEY {\&} SONS, Inc.},
title = {{Advanced Engineering Mathematics}},
url = {http://books.google.com/books?id=7I3PPQAACAAJ{\&}pgis=1},
year = {2011}
}
@inbook{Kreyszig2011chpt204,
author = {Kreyszig, Erwin},
edition = {10th},
issn = {00255572},
pages = {864-871},
publisher = {JOHN WILEY {\&} SONS, Inc.},
title = {{Advanced Engineering Mathematics}},
chapter = {20.4},
year = {2011}
}
@book{Byrne2014,
abstract = {An understanding of the nervous system at virtually any level of analysis requires an understanding of its basic building block, the neuron. This book provides the solid foundation of the morphological, biochemical, and biophysical properties of nerve cells. All chapters have been thoroughly revised for this second edition to reflect the significant advances of the past five years. The new edition expands on the network aspects of cellular neurobiology by adding a new chapter, Information Processing in Neural Networks, and on the relation of cell biological processes to various neurological diseases. The new concluding chapter illustrates how the great strides in understanding the biochemical and biophysical properties of nerve cells have led to fundamental insights into important aspects of neurodegenerative disease. • Written and edited by leading experts in the field, the second edition completely and comprehensively updates all chapters of this unique textbook • Discusses emerging new understanding of non-classical molecules that affect neuronal signaling • Full colour, professional graphics throughout • Includes two new chapters: Information Processing in Neural Networks - describes the principles of operation of neural networks and the key circuit motifs that are common to many networks in the nervous system. Molecular and Cellular Mechanisms of Neurodegenerative Disease - introduces the progress made in the last 20 years in elucidating the cellular and molecular mechanisms underlying brain disorders, including Amyotrophic Lateral Sclerosis (ALS), Parkinson disease, and Alzheimer’s disease. • Companion website for students provides quizzes to aid their comprehension of this material. Manual site for instructors provides figures in Powerpoint and solutions to quizzes.},
address = {London},
author = {Byrne, John and Byrne, John H and Roberts, James L},
edition = {Third edition},
editor = {Byrne, John and Byrne, John H and Roberts, James L},
isbn = {978-0-12-397179-1},
keywords = {biology,biophysics,neuroscience},
mendeley-groups = {Master-related},
pages = {694},
publisher = {Academic Press, Elsevier Inc.},
title = {From Molecules to Networks: An Introduction to Cellular and Molecular Neuroscience},
year = {2014}
}
@inbook{Byrne2014chpt1p11,
address = {London},
author = {Byrne, John and Byrne, John H and Roberts, James L},
title = {From Molecules to Networks: An Introduction to Cellular and Molecular Neuroscience},
edition = {Third edit},
isbn = {978-0-12-397179-1},
keywords = {biology,biophysics,neuroscience},
mendeley-groups = {Master-related},
chapter = {1. Cellular Components of Nervous Tissue},
pages = {11},
publisher = {Academic Press, Elsevier Inc.},
year = {2014}
}
@inbook{Byrne2014chpt2p26,
address = {London},
author = {Byrne, John and Byrne, John H and Roberts, James L},
title = {From Molecules to Networks: An Introduction to Cellular and Molecular Neuroscience},
edition = {Third edit},
isbn = {978-0-12-397179-1},
keywords = {biology,biophysics,neuroscience},
mendeley-groups = {Master-related},
chapter = {1. Cellular Components of Nervous Tissue},
pages = {26},
publisher = {Academic Press, Elsevier Inc.},
year = {2014}
}
@article{Shapira2013,
abstract = {The emergence of social networks and the vast amount of data that they contain about their users make them a valuable source for personal information about users for recommender systems. In this paper we investigate the feasibility and effectiveness of utilizing existing available data from social networks for the recommendation process, specifically from Facebook. The data may replace or enrich explicit user ratings. We extract from Facebook content published by users on their personal pages about their favorite items and preferences in the domain of recommendation, and data about preferences related to other domains to allow cross-domain recommendation. We study several methods for integrating Facebook data with the recommendation process and compare the performance of these methods with that of traditional collaborative filtering that utilizes user ratings. In a field study that we conducted, recommendations obtained using Facebook data were tested and compared for 95 subjects and their crawled Facebook friends. Encouraging results show that when data is sparse or not available for a new user, recommendation results relying solely on Facebook data are at least equally as accurate as results obtained from user ratings. The experimental study also indicates that enriching sparse rating data by adding Facebook data can significantly improve results. Moreover, our findings highlight the benefits of utilizing cross domain Facebook data to achieve improvement in recommendation performance.},
author = {Shapira, Bracha and Rokach, Lior and Freilikhman, Shirley},
doi = {10.1007/s11257-012-9128-x},
issn = {09241868},
journal = {User Modeling and User-Adapted Interaction},
keywords = {Collaborative filtering,Cross-Domain recommendations,Evaluation,Facebook,Recommender systems},
mendeley-groups = {Master-related},
number = {2-3},
pages = {211--247},
title = {{Facebook single and cross domain data for recommendation systems}},
volume = {23},
year = {2013}
}
@inproceedings{Huang2013,
abstract = {Cross-domain image synthesis and recognition are typically considered $\backslash$nas two distinct tasks in the areas of computer vision and pattern recognition. $\backslash$nTherefore, it is not clear whether approaches addressing one task can be easily $\backslash$ngeneralized or extended for solving the other. In this paper, we propose a $\backslash$nunified model for coupled dictionary and feature space learning. The proposed $\backslash$nlearning model not only observes a common feature space for associating $\backslash$ncross-domain image data for recognition purposes, the derived feature space is $\backslash$nable to jointly update the dictionaries in each image domain for improved $\backslash$nrepresentation. This is why our method can be applied to both cross-domain image $\backslash$nsynthesis and recognition problems. Experiments on a variety of synthesis and $\backslash$nrecognition tasks such as single image super-resolution, cross-view action $\backslash$nrecognition, and sketch-to-photo face recognition would verify the effectiveness $\backslash$nof our proposed learning model.},
author = {Huang, De An and Wang, Yu Chiang Frank},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.310},
isbn = {9781479928392},
issn = {1550-5499},
mendeley-groups = {Master-related},
pages = {2496--2503},
title = {{Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition}},
year = {2013}
}
@article{Norman2003,
abstract = {The authors present a computational neural-network model of how the hippocampus and medial temporal lobe cortex (MTLC) contribute to recognition memory. The hippocampal component contributes by recalling studied details. The MTLC component cannot support recall, but one can extract a scalar familiarity signal from MTLC that tracks how well a test item matches studied items. The authors present simulations that establish key differences in the operating characteristics of the hippocampal-recall and MTLC-familiarity signals and identify several manipulations (e.g., target-lure similarity, interference) that differentially affect the 2 signals. They also use the model to address the stochastic relationship between recall and familiarity and the effects of partial versus complete hippocampal lesions on recognition.},
author = {Norman, K A and O'Reilly, R C},
doi = {10.1037/0033-295X.110.4.611},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {0033-295X},
journal = {Psychol Rev},
keywords = {Cerebral Cortex/*physiology,Hippocampus/*physiology,Humans,Learning/*physiology,Memory/*physiology,Recognition (Psychology)/*physiology},
mendeley-groups = {Master-related},
number = {4},
pages = {611--646},
pmid = {14599236},
title = {{Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14599236},
volume = {110},
year = {2003}
}
@article{Barnes1959,
abstract = {The A-B, A-C and the A-B, A-B' transfer paradigms were studied. For the former the hypothesis of extinction of first-list responses is preferred to the other alternatives. For the latter, the List 1 responses showed no appreciable loss and List 2 was given perfectly after 1 anticipation trial. This is most understandable in terms of a mediation hypothesis. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Barnes, J M and Underwood, B J},
doi = {10.1037/h0047507},
file = {:home/williapb/Downloads/Barnes and Underwood. 1959. Fate of First-list Associations in Transfer Theory.pdf:pdf},
isbn = {0022-1015},
issn = {0022-1015},
journal = {Journal of experimental psychology},
keywords = {LEARNING},
mendeley-groups = {Master-related},
number = {2},
pages = {97--105},
pmid = {13796886},
title = {{Fate of first-list associations in transfer theory.}},
volume = {58},
year = {1959}
}
@article{Robins1995,
abstract = {Reviews the problem of catastrophic forgetting in neural networks, and explores rehearsal mechanisms as potential solution. Some experiments described by R. Ratcliff (see record 1990-18992-001) are replicated, including those relating to a simple recency-based rehearsal regime. Further rehearsal regimes are developed that are more effective than recency rehearsal. Sweep rehearsal was very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal is that previously learned information may not be available for retraining. One solution may be pseudorehearsal, a method that provides the advantages of rehearsal without requiring any access to previous learned information. These rehearsal mechanisms are interpreted in the context of a function approximation-based account of neural network learning. (PsycINFO Database Record (c) 2010 APA, all rights reserved) TS  - PsycINFO 1806 to Present (OvidSP)},
author = {Robins, Anthony},
doi = {10.1080/09540099550039318},
isbn = {0954-0091},
issn = {1360-0494},
journal = {Connection Science},
keywords = {*Forgetting,*Neural Networks,*Practice,catastrophic forgetting {\&} rehearsal mechanisms,ne},
mendeley-groups = {Master-related},
pages = {123--146},
title = {{Catastrophic forgetting, rehearsal and pseudorehearsal}},
volume = {7},
year = {1995}
}
@article{Robins1996,
abstract = {In this paper we explore the topic of the consolidation of information in neural network learning. One problem in particular has limited the ability of a broad range of neural networks to perform ongoing learning and consolidation. This is 'catastrophic forgetting', the tendency for new information, when it is learned, to disrupt old information. We will review and slightly extend the rehearsal and pseudorehearsal solutions to the catastrophic forgetting problem presented in Robins (1995). The main focus of this paper is to then relate these mechanisms to the consolidation processes which have been proposed in the psychological literature regarding sleep. We suggest that the catastrophic forgetting problem in artificial neural networks (ANNs) is a problem that has actually occurred in the evolution of the mammalian brain, and that the pseudorehearsal solution to the problem in ANNs is functionally equivalent to the sleep consolidation solution adopted by the brain. Finally, we review related work by McClelland et al. (1995) and propose a tentative model of learning and sleep that emphasizes consolidation mechanisms and the role of the hippocampus.},
author = {Robins, Anthony},
doi = {10.1080/095400996116910},
issn = {0954-0091},
journal = {Connection Science},
mendeley-groups = {Master-related},
number = {2},
pages = {259--276},
title = {{Consolidation in Neural Networks and in the Sleeping Brain}},
url = {http://www.tandfonline.com/doi/abs/10.1080/095400996116910},
volume = {8},
year = {1996}
}
