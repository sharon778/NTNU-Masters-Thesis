%===================================== CHAP 4 =================================

\chapter{Experiments and results}\label{chpt:experiments}

Intro. Superb introduction. Brilliant introduction. Such intro.

\section{Plan}

Due to the initial discussion that arose related to synchronicity in the CA3-layer of the model after other model decisions had been made, and a working, debugged model had been attained, initial experiments are performed on a novel synchronous versus asynchronous model. From these findings, the model attaining the best performance will be used. In the event of similar results, the most algorithmically efficient model will be preferred.

Following the preliminary study on synchronicity is the extended preliminary experimentation on model calibration, in which I search to attain a good model configuration. The attained parametrization will then be employed in the experiments replicated from \citep{Hattori2014}. Results will then form a basis for a comparative analysis between the former model and the model of this thesis. This may lay the foundation for constructing novel experiments, testing aspects related to convergence criteria, data set dimensionality, generalisability and network design.

\section{Setup}

hpc = HPC([io\_dim, 240, 1600, 480, io\_dim], \\
          0.67, 0.25, 0.04,  \# connection rates: (in\_ec, ec\_dg, dg\_ca3) \\
          0.10, 0.01, 0.04,  \# firing rates: (ec, dg, ca3) \\
          0.7, 100.0, 0.1, turnover\_rate,  \# gamma, epsilon, nu, turnover rate \\
          0.10, 0.95, 0.8, 2.0, weighting\_dg)  \# k\_m, k\_r, a\_i, alpha. alpha is 2 in 4.1
          
\section{Experiments}
\subsection{Experiment pre: Model calibration}
\subsection{... Results}

\subsection{Experiment 1: Consolidation performance}
\subsection{Results experiment 1: consolidation performance}

\subsection{Experiment Y: Novel}



% ========================== section ============================
Experiment design
Results
Comparisons

\section{Preliminary results}

Enforce sparsity through weight updates corresponding only to the winners of kWTA - didn't work.

\textbf{100 \% connection ratio EC-CA3:}

Fairly rapid convergence for three patterns in HPC-module for turnover between every training set iteration. 
Not necessarily successful recall of all patterns. Does this have something to do with the synchronized CA3-layer during recall? Separation possible during recall when the desired pattern(s) are presented to the network - however, not all may be recalled.

-> New random pattern each time stability was reached resulted in better recall.

Is this also the case for heavier weighting of the DG-CA3 path during learning?

Spurious pattern reduction/correlation with occurrence when using turnover?

Convergence when turnover is removed between set iterations?

Heavier weighting DG. Based on paper \citep{Norman2003}. Empirical results. Chpt. 4. Figures. Nice.

\section{Model calibration}

Experiments designed for model calibration

Dimensions analyzed outlined above.

First: STM-network extraction rate (at first, empirically observed to be same as solely auto-associative Hopfield network).


\textbf{Notes}

experiments suite - two as outlined by \citep{Hattori2014}, originally retrieved from ... as outlined above
enabling several trials automatically.

Turnover between every training set iteration (?). Needs to include empirical data on decision making. Move to preliminary experimentation in chpt. 4?

\cleardoublepage