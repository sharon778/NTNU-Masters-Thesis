\section*{\begin{center}{\Huge Appendix B}\end{center}}
\addcontentsline{toc}{chapter}{Appendix B}
$\\[0.5cm]$

\section*{Can a mechanistic neuron-level understanding of some aspects of cognition be attained?}

\textit{The included essay was written by me in association with the course "Neural Computation: Models of Brain Function", at University College London, UCL, the autumn term of 2014.}
\\

\title{Can a mechanistic neuron-level understanding of some aspects of cognition be attained?}

The brain exists in the physical reality, and cognition emerges from the physical processes occurring in the brain. If a sound and complete framework describing the physical processes down to a minuscular level can be attained, then it is theoretically possible to derive cognition from the axioms of this framework. An analogy to this is how chemistry can be derived from physics. Now, the axioms of the physical processes of the brain may not be determined with complete certainty, but they can be hypothesized, as in physics. It is not yet known what level of detail is required in order to fully encompass cognition. So can a mechanistic neuron-level understanding of some aspects of cognition be attained? Some claim that this level of abstraction does not encompass the essential aspects of brain function. This paper argues that at least some aspects of cognition can be understood on a mechanistic neuron-level, and that there is clear evidence of progression in the field towards more sophisticated models that encompass greater detail of the emerging cognition. To support this, aspects of spatial orientation and navigation are elaborated on by using models of place cells and grid cells, further explaining how the neuronal level processes may enable us to perform cognitive tasks of spatial processing. Following is a paragraph on associative memory and the hippocampus, explaining how pattern-completion and error correction, content-addressable memory and auto-associative memory might be constituted at the physical level. Thus linking them to among other things how different memories may be associated with one another, and how the model is compatible with episodic memory. Lastly, a more detailed model for short-term memory is considered, along with the compatibility with long-term memory. To exemplify this, an analogy of several processes related to walking to the office will be outlined from a mechanistic and computational perspective. Finally, an alternative view of computational neuroscience is introduced; a framework regarding one of the higher aspects of cognition – intelligence – purely as a processes of prediction.

Computational models of place cells allow for a prediction of firing rate patterns, and thus an understanding of how spatial cognition may be constituted in the brain. Beginning with the computer simulation of place cells in the rat’s hippocampus by Sharp in 1991$^{[1]}$, building on the ideas reported by O’Keefe in 1976$^{[2]}$, Sharp found that firing rate maps for simulated place cells correlated with the firing rate maps in vivo. The simulated hippocampal pyramidal cells were adapted from Rumelhart and Zisper (1986)$^{[3]}$, modified into two types of input-cells simulating the sensory cells, and subsequent layers with a lateral inhibition type of an architecture, i.e. a one winner-takes-it-all in every cluster. These findings illustrate that the hippocampal pyramidal cells fire relative to where you are, and that a mechanistic understanding of neurons can be used to simulate such cells. By analogy, when you’re standing on the pavement and walk in the direction of your office, your place cells will fire accordingly. Sharp’s competitive learning model sufficed to explain the robustness of place cells firing when moving. However, simplifications were made, and the aspect of spatial navigation such as removal of environmental space field cues were not explained. In other words; the predictions yielded by the model$^{[1]}$ might render it valuable with regards to prediction and explanation under certain constraints, providing ground for further research and understanding. Addressing the analogy; how you notice that a car drives by fairly close to you, and what cues you receive from the environment is omitted. The former can be explained by O’Keefe and Burgess (1996)$^{[4]}$, who modelled place cells using so-called boundary vectors cells (BVCs) oriented at a certain angle to one another. These may in turn be approximated by a thresholded sum of Gaussians for the distance from the walls of the environment for the rat. This provides a model that illuminates the representation of space fields in place cells, predicting a firing rate peak when an object, such as the car in the previous example, is at a fixed distance from you (ranging with the BVCs thresholds). This further illucidates how the cognitive map may be constituted on a mechanistic neuron-level. While the mechanistic models outlined so far do not encompass all aspects of spatial cognition, it is important to emphasise that they still provide a demonstration of how spatial cognition can be constituted at a physical level.

Grid cells further illuminate the mechanistic picture of spatial cognition. Hafting et al. reported in 2005$^{[5]}$ that a neural map of the spatial environment was found in the dorsocaudal medial entorhinal cortex. The model and metric of grid cells explains how the map of an environment along with a subject’s position can be represented internally, additionally suggesting that grid cells might constitute path integration. Hafting et al. (2005)$^{[5]}$ emphasized that these cognitive functions were, however, poorly understood on a mechanistic level; constituting ground for their later research where Sargolini et al. (2006)$^{[6]}$ found that the representation of position, direction and velocity is strongly facilitated by the observed grid and head-direction cells. Their results imply that all layers of the neocortex act together as an integrated unit, where interactions occur between grid cells in the principal layers and head-direction cells in the higher layers. It should be noted that these findings may be significant in terms of the memory system of the brain, and that the theory implies the existence of a common cortical structure, such as outlined by Mountcastle, V. (1978)$^{[7]}$. The report on grid cells$^{[6]}$ raises the question of how the integration in cortex spans various cells. Fyhn et al. 2007$^{[8]}$ reported on rate remapping and stable grid fields and global remapping in the medial entorhinal cortex. Their findings show that grid cells maintain a locally constant spatial phase structure, and that representations from multiple environments could be globally coded by place cells. Thus providing a further explanation for how translation of position and maintenance of a path-vector, and thus the update of the internal model of the environment, might be constituted in the brain. Analogously, this explains how you might walk straight back home, in case you walked in circles to visit various nearby locations. Some observations, however, might raise doubt of the predictive power of the models as a whole. One such example is navigation in non-horizontal environments (i.e. three dimensions), which has not been tested in the reports given so far. That being said, Jeffery et al. reported in 2012$^{[9]}$ that evidence for a different coding when it comes to a three-dimensional environment is weak. And even though they found that the ability of processing information in a third spatial dimension was present, it still seems to be coded in a metrically flat dimension, suggesting the same underlying functions are used in the brain, thus explaining the processing in this dimension too. In a recent study by Bush, Barry and Burgess (2014)$^{[10]}$, the contribution of grid cells to place cell firing is considered. They suggest an alternative model for place cell firing, in which the role of grid cells as outlined by Sargolini et al. (2006)$^{[6]}$ is questonioned. Bush et al. (2014)$^{[10]}$ reported that it might not be the input modules of the entorhinal cortex that decide the spatial scales, as commonly assumed - but the synthesised input from various sensory channels, implementing information related to spatial cues. This is further underlined in viewing the firing patterns of place and grid cells as intertwined and complementary, which may be more biologically plausible, rather than working in a hierarchical manner. It is worth noting how the former models of Fyhn et al. (2007)$^{[8]}$ may be elaborated on. By further expanding on previous models, it may be possible to attainin an even more detailed understanding of the actual mechanistic processes underlying spatial cognition.

Memory is a central aspect of cognition, and mechanistic models have shown how neural networks might implement associative memory. In 1982 Hopfield$^{[11]}$ reported that a Hopfield network, a fairly simple artificial neural network, had several of the properties of associative memory. The network is a fully connected recurrent and symmetric binary artificial neural network, updating its weights using Hebbian learning. This results in a network that is content-addressable, i.e. which completes a previously learned pattern, having only a partial input. The model also explains how spurious memories may arise; one memory may evoke another auto-associatively due to the recurrent connections in the network, thus corrupting old memories in the sense of associating the new with the evoked old ones. Nonetheless, this model is indeed simple, and does not include the connections to episodic memory, switching from learning to recall, nor how more complex cognition may emerge from associative memory. This can be explained by a computational model for fast and slow learning in memory consolidation by Hasselmo et al. (1995)$^{[12]}$, where the learning dynamics of the medial septum and switching between learning and recall for novel experiences is simulated. This computational model is shown to be more biologically realistic, using cholinergic suppression to achieve stable excitatory connections and a regulation of learning and recall. Note that this model also allows for episodic memory by auto-association of one memory followed by another, but only a serial form of memories one at a time, which does not explain how the associations are stored temporarily in short-term memory. It is important to emphasise that this model explains how learning and recall may be regulated, and furthermore that it is compatible with associative memory and episodic memory in the hippocampus. To return to the recurring analogy of this paper, we can now associate cues in the environment with the path to the office. Furthermore, the peculiar function of suddenly remembering a particular memory, can be explained by this model. You ‘unconsciously’ remember it because the content which you are experiencing now trigger that memory.

Short-term memory, long-term memory, and memory consolidation are other important aspects of cognition. Modelling these may be required in order to acquire a more comprehensive understanding of the aspects of cognition that are related to memory. In 1990, McNaughton and Nadel$^{[13]}$ used Hopfield-like networks to construct memory matrices using both Hebbian learning, and memory matrices using feed-forward back-propagation to encode parts of the memory. These matrices could be used as models for short-term and long-term memory, respectively, and consecutively. McClelland et al. reported in 1995$^{[14]}$ that the neocortex learns slowly, which matches the fact that a feed-forward network converges slower than a network using Hebbian learning. These findings suggest that the hippocampus may teache the neocortex ensemble-structures over time, whilst learning novel patterns itself rapidly. This raises issues about how information is transferred, at what timescale, and how it might generalise in terms of semantic memory. These issues may be addressed by introducting the location of neuron ensembles in separate cortices, interlocking neuronal ensembles reciprocally in the different cortices, Damasio (1989)$^{[15]}$. Note that this may explain how the two different types of memory networks can interact on a mechanistic level. Considering that the recurrent networks will fire at certain rates, and that we have several cortices in the hippocampus, this may suggest that short-term memories are stored in different frequencies of oscillatory sub-cycles. Lisman and Idiart (1995)$^{[16]}$ reported that oscillations are a mechanism for time processing of short-term memories. They also showed that several short-term memories could be stored in the same neural network, and found evidence that is compatible with the amount of short-term memories humans can store. To summarise, the mechanistically described models now allow us to remember who you just walked past, and yet maintain a line of thought. You are able to, after repeatedly having experienced a particular pattern at the same time as a road-sign, extrapolate that the sign probably means that there is a dangerous crossing ahead. Short-term memory is essential for high-level cognition, and long-term memory and memory consolidation for remembering what you have learned, in addition to seeing more complex patterns.

Whether a complete mapping on a neural level encompasses all aspects of cognition, remains an open question. Such a model might have to encode the intertwined processes occurring at a refined level of our physical existence, such as at the molecular or even quantum-mechanical level. That being said – when a level at which it is believed to be possible to capture all that is required to constitute a particular aspect of cognition is found, it might be possible to extract the properties required to constitute the particular cognitive functions. I.e. an algorithm working on different principles, but still capturing the emerging cognitive functionality. 

One of the high level aspects of cognition is intelligence. In his book On Intelligence $^{[17]}$, Jeff Hawkins outlines a framework with which he seeks to capture the so-called “common cortical algorithm” – which he believes might suffice to constitute intelligence. Continuing along the same train of thought; it might be possible to simulate solely the quintessence of the processes constituting intelligence, providing for a more efficient simulation than the exact biological match of the desired processes, which are intractable. Successfully simulating an entity from which high-level cognition emerges might even allow for us to embed effective and sophisticated information processing algorithms directly into the entity. Furthermore, alternative forms of sensory input could be provided to possibly give the entity in a 'sense' a natural feel for various processes that are complex or abstract to the human mind. Imagine if doing statistical analysis was an as natural part of the mind as seeing different shapes and objects. One fact supporting this very fictional line of thought is the fact that human being are biological beings. In being so, not only do we still use older parts of the brain that might evolve to become more efficient with time, but we also need certain aspects of them to support our biological survival (hunger, reproductive need, etc.). This is not to mention the molecular function that is necessary to facilitate the information processing. Simulating biological evolution might be difficult or even impossible, but omitting the implementation of biological aspects that are solely needed for survival purposes, is theoretically plausible. For instance, who says that in the event of successfully creating an intelligent entity, it needs to have a goal? And does it necessarily need to have emotion? In Jeff Hawkins’ framework from On Intelligence, he assumes that intelligence is based entirely on prediction, which is constituted by mechanistic neuronal processes. It may appear that such a framework seems incompatible with some of the models outlined in this paper. However, with a closer examination, the models and observations can actually be considered to generally be compatible with Hawkins’ model. The major difference being that he regards the hippocampus as being on top of the cortical hierarchy. Regarding the findings of McNaughton and Nadel (1990)$^{[13]}$, and Hasselmo et al. (1995)$^{[12]}$, this might seem very intriguing, as memories are consolidated from the hippocampus to the neocortex. It is interesting to emphasise, however, that these findings can generally speaking be unified with the framework. There is evidently no strong observations in the two aforementioned studies implying that the hippocampus is on top of the cortical hierarchy. And just as Bush et al. (2014)$^{[10]}$ put grid cells in doubt by viewing the processes of grid and place cells as intertwined and working in a complementary fashion, the hippocampus and neocortex can be viewed as working in a complementary fashion. And it is worth noting that memory consolidation could still be outlined in a fairly similar way as in (Hasselmo et al. (1995)$^{[12]}$). Even though information is first passed through the neocortex, the hippocampus can work as a filter in choosing what to relay back, and the neocortex will still need several recurrent signals of the same pattern to consolidate the memory. Therefore, further research on the framework should be conducted. One possible topic being how a novel computational model enclosing a circle of information processing within simulated aspects of the hippocampus, the prefrontal cortex, and the thalamus, would behave.

There is undoubtedly cumulative evidence of that some aspects of cognition can be understood at a neural level. The physical processes can be hypothesized and simulated experimentally – if not fully, then at least in a simplified manner. Different processes may be combined, and in either way the conceptual step can be taken to higher levels. This can give rise to an emerging description, understanding and prediction of specific cognitive processes in the hypothesized environment, which might correlate with cognitive processes in living organisms. It is noticeable that by the course of time, computational and mechanistic models of the brain have evolved. By becoming increasingly sophisticated, they explain a larger part of the brain’s processes on a mechanistic neuron-level, simultaneously yielding a better understanding of the processes that constitute the cognitive map. In the same sense, it can be predicted how and why the underlying processes, such as the firing rate maps postulated by Sharp (1991)$^{[1]}$ approximately will fire in a mammalian brain, and partly how the resulting cognitive view will look, bearing in mind that the model is restrictive to a certain extent. Grid cells can further elaborate on the previous computational models of place cells and boundary vector cells, and further explain how for instance path integration is constituted in the brain. A mechanistic neuron-level understanding of how cognition enables you to spatially represent and navigate the environment, remember the way, recognise the office building, be reminded of thoughts from the past on the way, and learn patterns of related thoughts and events, can be attained. Further research on novel frameworks is a natural aspect both in the evolution of older models, as well as in the construction of novel models, potentially replacing the old ones. With the continued development of the multidisciplinary fields involved in neural computation, it will be interesting to see when most of the high-level cognitive concepts of cognition will be understood from a mechanistic neuron-level perspective.  Perhaps even more enthralling is the possible applications that such a sophisticated technology could give rise to.
\\

\noindent
\textbf{References and bibliography}

[1] Sharp, Patricia E. 1991. ‘Computer simulation of hippocampal place cells’. \textit{Psychobiology}, \textbf{19} (2): 103-115.

[2] O'Keefe, J. 1976. ‘Place units in the hippocampus of the freely moving rat’. \textit{Experimental neurology}, \textbf{51} (1): 78–109. 

[3] Rumelhart and Zipser. 1986. ‘Feature discovery by competitive learning’. In: Rumelhart et al. 1986. \textit{Parallel Distributed Processing, vol. 1}. MA, USA: MIT Press Cambridge. Pp. 151-193. 

[4] O’Keefe and Burgess. 1996. ‘Geometric determinants of the place fields of hippocampal neurons’. \textit{Nature}, \textbf{381}: 425-428. 

[5] Hafting et al. 2005. ‘Microstructure of a spatial map in the entorhinal cortex’. \textit{Nature}, \textbf{436}: 801-806. 

[6] Sargolini et al. 2006. ‘Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex’. \textit{Science}, \textbf{312} (no. 5774): 758-762. 

[7] Vernon Mountcastle. 1978. ‘An Organizing Principle for Cerebral Function: The Unit Model and the Distributed System’. In: Mountcastle and Edelman. 1978. \textit{The Mindful Brain}. Cambridge, MA: MIT Press. 

[8] Fyhn et al. 2007. ‘Hippocampal remapping and grid realignment in entorhinal cortex’. \textit{Nature}, \textbf{446}: 190-194.  

[9] Jeffery et al. 2012. ‘Navigating in a 3D world’. In: Menzel and Fisher. 2011. \textit{Animal Thinking: Contemporary Issues in Comparative Cognition}. Strüngmann Forum Reports, vol. 8. Cambridge, MA: MIT press. 

[10] Bush. Barry. Burgess. 2014. ‘What do Grid Cells Contribute to Place Cell Firing?’. \textit{Trends in Neuroscience}, \textbf{37}: 136-145. 

[11] Hopfield. 1982. ‘Neural networks and physical systems with emergent collective computational abilities’. \textit{Proceedings of the National Academy of Sciences of the United States of America}, vol. \textbf{79} (no. 8): 2554-2558. 

[12] Hasselmo et al. 1995. ‘Dynamics of learning and recall at excitatory recurrent synapses and cholinergic modulation in rat hippocampal region CA3’. \textit{The Journal of Neuroscience}, \textbf{15} (7): 5249-5262. 

[13] McNaughton and Nadel. 1990. ‘Hebb-Marr Networks and the Neurobiological Representation of Action in Space’. In: Rumelhart et al. 1990. \textit{Neuroscience and Connectionist Theory}. Lawrence Erlbaum Associates, Inc., Publishers. 

[14] McClelland et al. 1995. ‘Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory’. \textit{Psychological Review}, vol. \textbf{103} (3), 419-457. 

[15] Damasio. 1989. ‘Time-locked multiregional retroactivation: A systems-level proposal for the neural substrates of recall and recognition’. \textit{Cognition}, vol. \textbf{33} (1-2), 25-62. 

[16] Lisman and Idiart. 1995. ‘Short-Term Memories in Oscillatory Subcycles’. \textit{Science, New Series}, vol. \textbf{267}, no. 5203, 1512-1515. 

[17] Hawkins and Blakeslee. 2004. \textit{On Intelligence}. 1$^{st}$ ed. Times Books: Henry Holt and Company, LLC. 