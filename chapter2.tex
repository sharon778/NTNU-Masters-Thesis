%===================================== CHAP 2 =================================

\chapter{Background}\label{chpt:background}

% Connectionism
The processing unit in an artificial neural network (ANN) is the artificial neuron. This unit may be represented as a single activation value, symbolising a neuron's internal state. In order to process information, vectors of activation values representing the neurons of a layer may be multiplied with matrices representing corresponding weights between the neurons of the current and subsequent layer, i.e. the synapses and their synaptic strengths. This propagates information to the subsequent layer. This process is commonly known as feed-forward in the classical domain of neural networks. Such a network is usually trained using gradient-descent in weight-space by an algorithm such as back-propagation, which back-propagates a generated error signal backwards throughout the network, attempting to minimize the error, most often being a loss function such ass the l1-norm, i.e. the Euclidean distance in space, between two vectors. This technique of training a neural network, i.e. finding (sub-)optimal weights for it, namely back-propagation, was largely popularized by \cite{Rumelhart1986}. Furthermore, \cite{Rumelhart1986} did the important choice of choosing the sigmoidal-function as transfer function for propagating activation values through synapses to the anteceding neurons. That is, the sum of a neuron's input is run through the sigmoidal function, which has to important characteristics: (1) it puts a lower and upper bound on the values which a neuron may take, (-1, 1), and (2) it is continuous and differentiable, resulting in numeric methods of differentiation being applicable for weight adjusting relative to change in activation values.

When ANNs are constructed in this manner, as simple activation values, weights between the values, and transfer functions between the different layers, the algorithms are quite often referred to as connectionist models. An example includes the traditional feed-forward back-propagate (FFBP) neural networks of \citep{Rumelhart1986}.
% Other connectionist research here? Do I need further examples? Keith only mentioned that I should include and example from computational neuroscience. I feel that I should include some more stuff that condenses the context which I would like to establish for the model that I am actually investigating.
Note that deep learning is primarily an engineering discipline. Meaning that it is more concerned with how to create applicable systems and solutions. Rather than with explaining the biological systems from which it originates. I would like to emphasize that despite this, it is the synthesis of neuroscience, psychology, and computer science that has given rise to the field of artificial neural networks and its sub-fields, and continues to advance its application within deep learning. Exemplified only recently by deep learning algorithms where the biologically inspired long short-term memory (LSTM) unit \citep{Hochreiter1997}, and the even more recently proposed perhaps simpler gated recurrent unit (GRU) \citep{Mnih2015}, have enabled deep networks to capture temporal dependencies in data-sets. Adding a fundamental and crucial richness to what correlations and structures which may be captured by this class of general learning algorithms; namely long-term temporal dependencies within data. While a unit such as the GRU does not necessarily demonstrate the workings of the biological brain, it does demonstrate that looking at aspects which the biological brain captures, and translating them into algorithmic principles or requirements, may significantly improve engineered solutions and have great computer scientific value and possibly impact. Further exemplifying that attaining further knowledge within the domain of computational neuroscience may lead to algorithmic advances within deep learning, and vice versa. Had the GRU been discovered first, this could have led to the hypothesizing of recurrence being crucial for capturing temporal dependencies within neural functioning. Even though this is already a widely appreciated fact within neuroscience, the LSTM and GRU may still have an impact on the field of neuroscience, as we continue to discover why they enable the algorithms to perform more sophisticated types of processing.

% -> Computational Neuroscience
At the other end of the scale we have computational neuroscience,  where the principle of Hebbian learning is often used in order to attain a greater biological plausibility in the computational models.
\\

Hodgkin-Huxley:
\\\\

Approach topics that will be needed in the model and experiments.
Concepts
Terms
Definitions
Theory
Memory. Working memory.
The DNMA.
This chapter is essentially: Terms, definitions, concepts, papers, contemplations, background theory. Revisit DNMA.
Present previous model(s)?
Who are the reader..? A researcher with general knowledge within AI?


\cleardoublepage