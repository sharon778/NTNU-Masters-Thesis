%===================================== CHAP 2 =================================

\chapter{Background}\label{chpt:background}

% Connectionism
The processing unit in an artificial neural network (ANN) is the artificial neuron. This unit may be represented as a single activation value, symbolising a neuron's internal state. In order to process information, vectors of activation values representing the neurons of a layer may be multiplied with matrices representing corresponding weights between the neurons of the current and subsequent layer, i.e. the synapses and their synaptic strengths. This propagates information to the subsequent layer. This process is commonly known as feed-forward in the classical domain of neural networks. Such a network is usually trained using gradient-descent in weight-space by an algorithm such as back-propagation, which back-propagates a generated error signal backwards throughout the network, attempting to minimize the error, most often being a loss function such ass the l1-norm, i.e. the Euclidean distance in space, between two vectors. This technique of training a neural network, i.e. finding (sub-)optimal weights for it, namely back-propagation, was largely popularized by \cite{Rumelhart1986}. Furthermore, \cite{Rumelhart1986} did the important choice of choosing the sigmoidal-function as transfer function for propagating activation values through synapses to the following neurons. That is, the sum of a neuron's input is run through the sigmoidal function, which has to important characteristics: (1) it puts a lower and upper bound on the values which a neuron may take, (-1, 1), and (2) it is continuous and differentiable, resulting in numeric methods of differentiation being applicable for weight adjusting relative to change in activation values.

When ANNs are constructed in this manner, as simple activation values, weights between the values, and transfer functions between the different layers, the algorithms are quite often referred to as connectionist models. An example includes the traditional feed-forward back-propagate (FFBP) neural networks of \citep{Rumelhart1986}.
% Other connectionist research here? Do I need further examples? Keith only mentioned that I should include and example from computational neuroscience. I feel that I should include some more stuff that condenses the context which I would like to establish for the model that I am actually investigating.
Note that deep learning is primarily an engineering discipline. Meaning that it is more concerned with how to create applicable systems and solutions. Rather than with explaining the biological systems from which it originates. I would like to emphasize that despite this, it is the synthesis of neuroscience, psychology, and computer science that has given rise to the field of artificial neural networks and its sub-fields, and continues to advance its application within deep learning. Exemplified only recently by deep learning algorithms where the biologically inspired long short-term memory (LSTM) unit \citep{Hochreiter1997}, and the even more recently proposed perhaps simpler gated recurrent unit (GRU) \citep{Mnih2015}, have enabled deep networks to capture temporal dependencies in data-sets. Adding a fundamental and crucial richness to what correlations and structures which may be captured by this class of general learning algorithms; namely long-term temporal dependencies within data. While a unit such as the GRU does not necessarily demonstrate the workings of the biological brain, it does demonstrate that looking at aspects which the biological brain captures, and translating them into algorithmic principles or requirements, may significantly improve engineered solutions and have great computer scientific value and possibly impact. Further exemplifying that attaining further knowledge within the domain of computational neuroscience may lead to algorithmic advances within deep learning, and vice versa. Had the GRU been discovered first, this could have led to the hypothesizing of recurrence being crucial for capturing temporal dependencies within neural functioning. Even though this is already a widely appreciated fact within neuroscience, the LSTM and GRU may still have an impact on the field of neuroscience, as we continue to discover why they enable the algorithms to perform more sophisticated types of processing.

% -> Computational Neuroscience
At the other end of the scale we have computational neuroscience,  where the principle of Hebbian learning is often used in order to attain a greater biological plausibility in the computational models.
\\

\textbf{Notes from Rolls \& Treves:}

There is a fairly large body of evidence on the hippocampus most likely employing a distributed type of encoding, resulting in that the capacity of patterns which it may store is exponential to the number of neurons in a layer
\footnote{\label{footnote:Rolls98Intro}Rolls, E. T., \& Treves, A. 1998. 'Introduction', In: \textit{Neural Networks and Brain Function}. Oxford, UK: Oxford University Press. Pp. 13-14.}.
However, this does not imply that an exponential number of pattern associations may be stored, i.e. differing stimulus, as this has been found to increase only linearly with the number of neurons in empirical studies$^{\ref{footnote:Rolls98Intro}}$.

kWTA - lateral inhibition. Tends to increase the probability of similar patterns activating the same set of neurons later on. The result is categorization of patterns into similar types. In other words; it may be regarded as a type of preprocessor. (P. 15).

As neural activity is relayed from large parts of the brain to the hippocampus, this may enable the hippocampus to form memories which include information from all of these system. Such memory integration is believed to be one of the fundamental functions performed by the hippocampus (P. 22).

Linear separability - the classic XOR problem.

Auto-association \& pattern completion - Hopfield nets
Basins of attraction
Content addressable memory - with partial pattern
Graceful degradation
Perfect recall (P. 46)

Expansion encoding in DG for decorrelation of overlapping separate memories in CA3, enabling separate storage and recall. Marr (1969), Rosenblatt's original perceptron similar w/ pre-processor. (P. 41)

The DNMM of \cite{Hattori2014} seems like a competitive network, having sparsification through in-EC and kWTA, and to some extent orthogonalization from the DG-layer through expansion encoding, as well as neuronal turnover. (P. 54)

[CHPT. 4]
Principal cells biologically mostly excitatory; mutual inhibition implemented by inhibitory interneurons. (GABA)

Qualities of hippocampal model lets it acquire more patterns. However, functions as STM. Discuss: Are these qualities transferable to networks in general? Principles? What do the experiments demonstrate?

Example of emergent phenomena implemented quite algorithmically, i.e. not through mechanisms resembling the biological mechanisms, yet having approximately the same emergent behaviour/aspects: k-WTA for lateral inhibition.

Could embed Mexican hat functionality, i.e. topographic influence from closeness in weight updates.

Feature discovery and self-organization - kWTA.

Redundance reduction using kWTA.

Orthogonalization leads to improved classification and/or categorization.

Diluted connectivity?

PCA not bio. plausible. k-WTA plausible. However, similar, yet not same type of orthogonalization.

This slow memory consolidation to the "neocortical" network could potentially suggest methods for storing the maximum amount of information in a network, if it can train the network to work when detached from the hippocampal network.
\\
Furthermore; implications for neuro. Model mainly concerned with neuro. ? Comp. Sc. Parallels?
\\

Hodgkin-Huxley:

Would it be possible to fire using for instance 0.2 * sigmoid (in\_sum) for the neurons that do not fire 1?
\\\\

Approach topics that will be needed in the model and experiments.
Concepts
Terms
Definitions
Theory
Memory. Working memory.
The DNMA.
This chapter is essentially: Terms, definitions, concepts, papers, contemplations, background theory. Revisit DNMA.
Present previous model(s)?
Who are the reader..? A researcher with general knowledge within AI?

see notebook:

\begin{itemize}
    \item SLR DL \& CONN., including catastrophic forgetting in this context
    \item General (intro to) neuro. \& comp. neuro.
    \item + example
    \item may construct theories about brain function
    \item memory - largely connected to the hippocampus
    \item DNMA
    \item DNMA may solve catastrophic forgetting
\end{itemize}


\cleardoublepage