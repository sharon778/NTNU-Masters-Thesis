%===================================== CHAP 2 =================================

\chapter{Literary Review}
\section{Introduction}

Recent advances in deep learning may render several state-of-the-art approaches within other fields of artificial intelligence (AI) such as image classification and natural language processing obsolete, as they have been outperformed by more generally applicable algorithms of deep learning (\cite{LeCun2015, Schmidhuber2014}). These advances are due to both computational as well as algorithmic improvements within the field. New insights into how high-level cognition may be constituted and emerge in neural networks may further advance the field's capabilities, possibly beyond what can be achieved only by an increase in computational power. At the very core of such new insights lies the symbol grounding problem. This problem is addressed by several authors such as \cite{Yamashita2008, Tani2014}, who propose solutions to this long standing problem. However, there are still issues present in the models related to generalisation. The problem of generalisation has been addressed by other authors such as \cite{McClelland1995}, and more recently \cite{Hattori2014}. In their seminal paper, \cite{McClelland1995} propose a dual-network memory model, hypothesising that the brain solves the problem of long-term memory and memory consolidation by the use of a dual-network memory architecture. Because the body of research to the best of our knowledge on the dual-network memory architecture is fairly limited, the goal of this thesis is to further investigate the issues of catastrophic forgetting and generalisation, memory, and high-level cognition in such an architecture. Elucidating high-level abstraction in this architecture could provide key insights for further advances within deep learning, artificial intelligence, neuroscience and psychology. The problem of catastrophic forgetting is of central importance to memory formation in neural networks, and thus also plays a key role in forming abstract representations from different types of memory. We wish to investigate it outside the domain of traditional topologies, as well as the standard feed-forward back-propagate algorithm. Furthermore, building upon the work of \cite{Hattori2014}, and \cite{Yamashita2008}, we seek to embed an approach such as multiple-timescales recurrent neural networks in a dual-network memory model. The motivation for this stems from seeking to attain an artificial neural network model capable of attaining high-level cognitive behaviour as well as integration across different abstract memories.
In order to investigate the body of research on the topic, the databases of Scopus, Web of Science, IEEE, Nature, those indexed in Mendeley, and arXiv were used to locate references of interest. A process of following networks of citations was also employed, particularly by using the paper \cite{McClelland1995}. Furthermore, as some heavily cited authors appeared, such as Tani, LeCun, Bengio, and Schmidhuber, their list of publications were used as sources for references, using Google's search engine to find their lists of publications. Note that as there is a significant time limit to the thesis, the scope of the literary review had to be reduced accordingly. It is, nevertheless, our view that a fairly comprehensive overview of relevant articles related to the thesis topic is presented.

\section{Annotated Bibliography}

\textbf{French, R. M. (1992). Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks.\textit{ Connection Science, 4(3-4)}, 365-377.}

\cite{French1992} addresses the problem of catastrophic forgetting \cite{McCloskey1989, Ratcliff1990} in neural networks that are trained using feed-forward back-propagation, and argues that it occurs because of representational overlap. A method alleviating the problem is to make use of a sparse distributed memory (SDM), which works fairly well until the memory is digested. However, once it is digested severe catastrophic forgetting occurs, and old memories are disrupted in addition to that new ones may not be formed. Furthermore, it is not possible to combine different representations as easily due to the sparsity in an SDM, rendering the approach less generalisable. One way of viewing this is that the distributedness segments more tasks because it acts as a regularizer by constraining the state space. However, this simplification may oversimplify the architecture, causing it to lose out on crucial information due to locally constrained pattern formations. Node sharpening, which is the author's proposed solution, locally constrains input patterns to enhance the most prominent features, effectively resulting in sparse representations. This results in less "noise" being propagated throughout the network, the most principal input nodes being in focus, and a significant reduction in catastrophic forgetting. However, this is done at the cost of attaining a less generalisable model, as node sharpening only looks at the \textit{n} most prominent nodes. Furthermore, today's state-of-the-art algorithms employs different transfer functions, network topologies, and learning. This suggests that the paper's claim, being that that the trade-off between catastrophic forgetting and generalisation is inevitable, may not hold today.


\textbf{French, R. M. (1994). Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference.\textit{ Network, 1111, 00001}.}

Building on his former work (\cite{French1992}), \cite{French1994} proposes a model which dynamically sharpens the most relevant input nodes. Once two different outputs has been presented to a standard feed-forward back-propagate (FFBP) network, a context bias is calculated in the hidden layer and propagated back to the input layer. Shortly put, this emphasizes the differences of the distinct categories, focusing on segmenting and orthogonalizing on the most prominent properties of difference in a relative manner. Thus more orthogonal, well distributed patterns are learned.
Although this type of segmentation works well, it is still very similar to the former approach in \cite{French1992}, and suffers from the same trade-off between remembering and generalisation. As in other FFBP networks performing gradient descent in weight space (\cite{Hinton1989}), the algorithm will only succeed to find the most principal components, scaling with how much information the network is able to store (mainly affected by its size). As a consequence, failing to attain a sufficient accuracy for a given task could be due to ignoring the detailed information present in the segmentation process. Furthermore, it may be the case that only finding the most principal correlations in a distribution does not reveal the true nature of it, failing to extract the most precise correlations or properties. This is at the very core of the sensitivity-stability dilemma (\cite{Hebb1949}). One way way of addressing this dilemma is by multi-network systems , which \cite{French1994} suggests in his conclusion. This may produce refined solutions and abstractions and more sophisticated pattern-associations, although seemingly less computationally efficient.


\textbf{McClelland, J. L., McNaughton, B. L., \& O’Reilly, R. C. (1995). Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. \textit{Psychological Review, 102(3)}, 419-457.}

In this seminal paper, \cite{McClelland1995} propose a memory model of the brain in which the hippocampus is responsible for the consolidation of memories to the neocortex, with the neocortex storing semantic and episodic memory. The synthesis of recall from the deeper layers of the neocortex and representations in the working memory itself enables contexts to be distinguished or connected in the proposed model. The learning and consolidation to the neocortical module is essentially performed in an interleaved fashion; slowly potentiating and instantiating the memories from the hippocampal network. Thus we have a bottom-up and top-down synthesis where recall is combined with novel patterns. This raises the question of how such an interconnectedness is constituted both topologically and in terms of local information-processing in the actual brain. Furthermore, the question is whether principles from the environment of the neocortex and hippocampus need to be extracted and implemented to successfully have this functionality emerge in computational models, i.e. whether a synthesis with brain functionality constituted by other parts would be crucial in with regards to memory consolidation in an artificial model. The proposed model suggests that the hippocampus and neocortex constitute the mechanisms for successful integration across memories, as well as keeping memories fairly intact. The specifics on how these mechanisms are constituted, however, remains obscure or undiscovered. This constitutes a core inspiration and foundation for an artificial neural network (ANN) model in our thesis. It lays out the foundation for investigating long-term memory and memory consolidation, with which we hope to achieve a more generalisable and plastic ANN model.


\textbf{French, R. M. (1997). Pseudo-recurrent Connectionist Networks: An Approach to the “Sensitivity-Stability” Dilemma. \textit{Connection Science, 9(4)}, 353-380.}

A series of experiments that address the sensitivity-stability dilemma (\cite{Hebb1949}) is presented in this paper, in which the authors demonstrate that a pseudo-recurrent network model performs significantly better than traditional feed-forwad back-propagate networks, mainly inspired by \cite{McClelland1995}. Different experiments are used to illuminate several aspects of the pseudo-recurrent network model. The key finding is that pseudo-recurrent networks using pseudo-patterns perform significantly better in terms of less catastrophic forgetting, suggesting that the brain may perform a type of pseudo-pattern compression and storage of information, too. Another point worth noting is that the networks that are simulated are of a farirly small scale, making them generalisable only to a certain extent due to the network capacity. This seems to have been largely ignored by the authors. Therefore it would be interesting to look at the implications of both increasing the complexity as well as the scale of the network. Note that the semi-distributedness of this paper's model arises naturally from the pseudo-recurrent neural network, as opposed to in the former papers of \cite{French1992, French1994}. This may suggest that the mechanism, which acts as an auto-associative memory, may also act as a predictor. In addition to completing incomplete, partial or fuzzy memories and retrieving them, it might therefore also provide a mechanism to filling in a story, or even imagining a story, creating it on the go by using the pseudo-recurrent mechanisms. This could suggest that the interleaving of memories in a pseudo-reccurent manner is at the heart of creativity, prediction and not the least; cognition. In relation to our thesis, we wish to investigate various successful or semi-successful approaches for addressing convergence in dual-network memory architectures. A key finding that we will influence our work is that of pseudo-recurrence performing a crucial mechanism in interleaving and pattern generation. This may lay a foundation for a comparative analysis in our thesis, as well as providing insights into some underlying principles for the emergence of such a mechanism.

\textbf{French, R. M., Ans, B., \& Rousset, S. (2001). Pseudopatterns and dual-network memory models : Advantages and shortcomings. \textit{Connectionist Models of Learning, Development and Evolution}, 13-22.}

\cite{French2001} address the currently present issues of the dual-network memory architecture, illuminating key issues related to episodic memory, contextualisation, and pseudo-pattern generation and optimisation. In doing this, they conclude that the brain is likely to perform some kind of pseudo-pattern optimisation. Possibly in a stochastic way relative to how well it evaluates its performance and understanding of a currently perceived concept or state. When it comes to episodic memory, the work of \cite{Ans2000} is elaborated on, in which only dissimilar pseudo-inputs were used for consolidation to a neocortical network. Results demonstrated that the model was capable of generalising to and thus learning all patterns (20 patterns, where only 13 were explicitly taught to the neocortical network). This strengthens the view that a dual-network memory model is crucial for successful integration across memories. Another aspect that is addressed is contextualisation in the dual-network memory architecture. \cite{Ans2000} demonstrated that their implementation of a dual-network memory model performed better with pseudo-patterns with random initial input rather than when retrieving similar patterns from the neocortical module. This does pose an inconsistency both biologically and algorithmically speaking, because it is biologically implausible to retrieve an output representing the activity of the entire neocortex, and algorithmically inefficient or possibly intractable with increasing size, in order to interleave new memories with old. Furthermore, retrieving similar patterns mostly leads to a failure of convergence. This suggests that there may in fact be other, or more, principles that play a crucial role in the dual-network memory architecture. Summarising; the dual-network memory model offers significant benefits and advances in neural network models, but there seems to be an oversimplification related to pseudo-pattern formation and generation, affecting the integration across similar, yet separate memories. This is an aspect that we wish to illuminate in our thesis.


\textbf{Yamashita, Y., \& Tani, J. (2008). Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment. \textit{PLoS Computational Biology, 4(11)}, e1000220.}

Functional hierarchies of reusable neural patterns of activation are believed to constitute the mechanisms for among other things motor primitives in the brain. Such functional hierarchies have been earlier been realized by the explicit coding of a hierarchical structure in artificial neural networks. In this paper, however, \cite{Yamashita2008} propose a novel model in which such a functional hierarchy self-organizes due to the existence of multiple timescales within the network. Thus, the topological split is in a way transformed to a temporal separation, yet now enabling the coordination and segmentation to occur in a more generalisable manner as the reusable motor primitives are no longer hardwired by topological constraints. Their findings still demonstrate that performance is better with a topology supporting segmentation (with a type of bottle-neck between the networks of neurons that operate on different timescales), though segmentation is possible in both cases due to multiple timescales. These findings suggest that there are both spatial and temporal mechanisms which lead to the emergence of functional hierarchies within neural networks. Training is performed by the use of back-propagation through time, in which the desired output generates an error signal. It would be interesting to see what kind of implications different transfer functions (now a sigmoid transfer function is used) would have. Furthermore, always enabling a level of training could provide for a more dynamically suitable network. In such an event, the mechanisms for propagation would most likely have to be changed.


\textbf{Solbakken, L. J. (2009). Fuzzy Oscillations: a Novel Model for Solving Pattern Segmentation. Institutt for datateknikk og informasjonsvitenskap, NTNU. The Norwegian University of Science and Technology.}

In his master's thesis, \cite{Solbakken2009} demonstrates that oscillations between populations of spiking neurons in an ANN model can synchronise the activity of the network as a whole, as well as its sub-networks. This enables the segmentation of multidimensional data through modulatory feedback, also largely avoiding a superposition catastrophe. Namely where objects that are to be segmented share many similar features, and therefore fails to segment them into different categories. With slightly richer neuronal dynamics than what is commonly used, they employ Izhikevich neurons (\cite{Izhikevich2003}, attaining a simple model that manages to capture the dynamics of successful segmentation of complex scenes with static input. From this the authors conclude that such a model extends traditional ANNs in including temporal information, in which there is a competitive segmentation process that may separate features, also evading a combinatorial explosion due to the model's dynamics. However, it is unclear which information processing capabilities that emerge by introducing synchronization, and which capabilities emerge implicitly through introducing the Izhikevich neuron model. When it comes to the convergence towards a steady state for a feature that a neural population recognizes: This may potentially provide for richer dynamics in artificial neural networks. Furthermore, evading a combinatorial growth in embedding steady state dynamics in the network could drastically improve deep learning algorithms using deep neural networks. Another concept that \cite{Solbakken2009} addresses in his thesis is the binding problem. As illuminated in his thesis, analogously to how water droplets may condense from steam,  there has to be a change which links meaning to activity in a network model (\cite{Freeman2003}, cited in \cite{Solbakken2009}). A possible coupling between populations of neurons to other populations of neurons is suggested in this regard. It remains obscure, however, how this translates into meaning other than possibly constituting a mechanism for abstraction. Nevertheless, oscillatory dynamics may have the potential to solve several fundamental information processing problems related to neural networks, including memory and segmentation. This is something that we wish to build on in our thesis in including recurrence and spiking neuron dynamics in parts of our envisioned multi-network memory architecture.


\textbf{Maniadakis, M., Trahanias, P., \& Tani, J. (2012). Self-organizing high-order cognitive functions in artificial agents: Implications for possible prefrontal cortex mechanisms. \textit{Neural Networks, 33}, 76-87.}

\cite{Maniadakis2012} demonstrated the ability of recurrent neural networks (RNNs) to self-organize when being evolved by genetic algorithms. They successfully evolved networks which learned a dynamically changing task, navigating in a simulated physical robot environment. Learning such a complex behaviour is usually associated with higher order cognitive functions, suggesting that the presented model may relate to the workings of the brain, and more specifically the mechanisms of the prefrontal cortex.
Their study was conducted by using the Wisconsin Card Sorting Test, embedded by a betting function, performed by a robot in a control environment. Because of the nature of the problem domain, and the complexity of the RNNs, they divided the experiment into several phases, in order to better investigate the emergence of network and model properties. Their main findings include that a bottle-neck in the continuous time recurrent neural network architecture was favorable, or even a necessity, to attain satisfactory high-level cognitive behaviour. Note that their experiments were limited to very similar environments. Therefore it would be interesting to study the generalisation in different domains. Note that the paper differs from others in the field in that it regards cognition as embodied both in the model and the environment. As Y. Bar-yam points out; a complex system is in synthesis with its environment, making it crucial to include environmental influence on a complex network model in investigating the model's behaviour, \cite{Bar-yam1997}. It would be interesting to investigate the environmental correlate between the network topology and network dynamics in our thesis. Both with respect to a dynamic environment, and model evolution.


\textbf{Hattori, M. (2014). A biologically inspired dual-network memory model for reduction of catastrophic forgetting. \textit{Neurocomputing, 134}, 262-268.}

In this excellent paper, \cite{Hattori2014} presents a novel ANN model and dual-memory architecture consisting of two modules. Namely a hippocampal as well as a neocortical module, similar to the model of \cite{McClelland1995}. Hattori demonstrates in several experiments that the catastrophic forgetting is reduced to a large extent in the new model, and also that it is superior to the dual-network memory architectures of \cite{Hattori2010}. His former model does in turn outperform more conventional models such as the models attained in \cite{Ans1997, French1997}. In \cite{Hattori2014} the model is still not used to solve complex tasks, as the model is rather heavy in terms of computational complexity (from introducing more complex neuronal dynamics). Hattori further demonstrates that the hippocampal network is capable of acquiring information rapidly, consolidating this to the neocortical network when it is successfully extracted in the HPC module.
When it comes to the neuronal models, the hippocampal model contains McCulloch \& Pitts neurons, using the Oja rule for learning and updating connections between the different sub-modules, except for in the CA3 sub-module, where Hebbian learning with forgetting is used.
The fact that mean goodness and perfect recall is worse in the hetero-associative case suggests that a significant amount of plasticity is missing in the model. Resulting in being unable to find most of the actual correlations and correct patterns, as well as for the trivial case simply remembering the input. This is further supported by the observation that a much higher turnover rate than observed biologically speaking has to be employed for improving the model. As one way of achieving greater plasticity may namely be to introduce more randomness in searching for patterns. Looking to complex systems theory, some noise or chaos is required to arrive at a phase-transition between regular and chaotic dynamics, i.e. the critical phase, in which learning is made possible and efficient, \cite{Langton1990, Newman2003}. By using a very high neuronal turnover in the model, this suggests that the high turnover rate could be what alleviates the lack of plasticity in Hattori's model. Despite allowing for a more flexible search by the model, it might also introduce too much randomness in the search, making it too coarse-grained. It would be interesting to see parameter adjustments for the hetero-associative case in the hippocampal module, as this may pose different constraints on the network. Particularly an analysis of the edge of chaos for the CA3-part is something we wish to further investigate in our future work. Another important aspect would be attempting to temporally extend the model, in an attempt to capture how episodic memory may be constituted in a complementary memory model. Such a synthesis could potentially introduce novel aspects of high-level cognition. It is therefore our aim in this thesis to look at variations in neuronal dynamics as well as topologies in a proposed dual- or multi-network memory architecture.


\textbf{Tani, J. (2014). Self-Organization and Compositionality in Cognitive Brains : A Neurorobotics Study. \textit{Proceedings of the IEEE, 102(4)}, 586-605.}

In this insightful paper, \cite{Tani2014} further investigates high-level cognition in neural networks, addressing the key concept of compositionality and how this might constitute cognition. Reviewing evidence suggesting that the mammalian brain attains complex high-level cognition through a functional hierarchy, he seeks to gain further knowledge about how this may be constituted at the neural level, (\cite{Miyake2000, Koechlin2003, Fuster2008}, cited in \cite{Tani2014}). Tani introduces two RNN models, and applies each architecture to two different complex tasks, respectively. Both models share the feature of having a type of top-down bottom-up synthesis. In the first architecture using an RNN with a parametric bias (PB) connecting the networks, the top level can be regarded as constantly trying to predict what the lower level is perceiving. If something "unknown" is then perceived, this is learned by a learning mechanism which works on minimising an error criteria. In the other model, he introduces a multi-network CTRNN architecture, which performs iterative learning on the continuous flow of perceived input. Both models were successfully applied in four different experiments. The main findings include a successful extraction of linguistic properties of sentences and correlated actions and objects with the RNNPB model in the first and second experiment. In the third experiment, the MTRNN successfully attained a functional hierarchy of action primitives, allowing the model to perform complex motor-sensory tasks in a top-down bottom-up synthesis of the higher and lower levels of the MTRNN. A very interesting aspect of the MTRNN is the use of different timescales, with the slower high-level parts of the network enabling this synthesis. Note that this can be regarded as a solution to the symbol grounding problem; the motor-sensory input being a continuous flow of information from the environment, provided by sensory organs, with the symbols and the conscious arising in the synthesis of the functional hierarchy. Tani contemplates of consciousness as arising in the top-down error minimisation - for action selection when it comes to motor control. Analogously, such a functional hierarchy could be valid for any type of sensory data, giving rise to consciousness. In such a hypothesized scenario, the question remains how this could be topologically solved. After all, one of the brain's seemingly central properties is its distributedness and completely decentralized workings. Therefore it would be interesting to try to tackle such a problem from an as decentralized perspective as possible. In either case, we are inspired in our thesis by the work of Tani in that we seek to implement the principles for similar abstraction mechanisms, with the aim of using them in a multi-network memory architecture. This could potentially give rise to a more robust model, and our goal is to investigate the observed emergent or non-emergent properties this may result in.


\section{Summary and conclusions}

In investigating a body of research on deep learning, in particular addressing memory, generalisation, and plasticity in neural networks - the problem of catastrophic forgetting, and the symbol grounding problem appears as two of the field's central challenges. One of the main findings of this literature review is the dual-network memory architecture, which was addresses all of the aforementioned issues. The architecture is proposed by \cite{McClelland1995}, and was hypothesised to demonstrate how the brain may implement slow memory consolidation and long-term memory potentiation, constituted by the hippocampus and neocortex as a working memory and long-term memory, respectively. Later research building on this seminal paper includes demonstrates that such an architecture actually eliminates the problem of catastrophic forgetting to a large extent, \cite{French1997, Ans1997, Ans2000, French2001, Hattori2010, Hattori2014}. However, certain issues of how to implement such a model remains unclear, including:
\begin{itemize}
\item how pseudo-patterns can be employed
\item alternative mechanisms for interleaving
\item how patterns may be learned in the hippocampal module
\item how information might be retrieved from the neocortical module
\end{itemize}
In his later work, \cite{Hattori2014} demonstrates that chaotic neurons may have a crucial function with respect to consolidation in dual-network memory architectures, as well as enabling a more non-deterministic and plastic behaviour to emerge. It is evident that there are some very interesting phenomena that may emerge from dual-network architectures. Furthermore, \cite{Tani2014} demonstrates that an MTRNN is capable of extracting a functional hierarchy of action primivites by the use of a slower timescale in the higher levels of the spiking neural network. This raises the question of whether a dual-network memory architecture could be combined with MTRNNs, replacing the neocortical module and representing the prefrontal cortex and its connections to other cortical areas such as the motor cortex. In that case, the symbol grounding problem would be addressed by using MTRNNs to have a functional hierarchy emerge, as in the work of \cite{Tani2014}. This may suggest that the way in which the hippocampal and neocortical modules are coupled will change in a new model, contrasting previously suggested models in that input is not directly propagated through the hippocampus, but propagated through the neocortex to the hippocampus. This would address the problem of convergence in the hippocampus, potentially posing a solution in that the nature of the input to the hippocampal module will be different, as the dynamics of the neocortical module will change fundamentally from other proposed models such as \cite{Ans1997, Ans2000, French2001}. It would be interesting to investigate topological implications in such a model, experimenting with how and what parts are connected. One possible solution could be to couple the CA3 part of the HPC with the PFC, with the aim of being able to combine different abstract representations that may converge because of the spiking dynamics of the MTRNN. Such an intertwining could have the potential of capturing temporal sequences in a way that previous dual-network memory architectures has failed to. This is an idea that has been suggested by \cite{Hattori2014}, who proposes that spiking neurons could potentially capture the underlying principles for episodic memory. Concluding, it would be very interesting to embed spiking neurons in a dual-network memory architecture, building upon the work of among others \cite{Yamashita2008, McClelland1995, Hattori2014}. In this thesis our aim is to implement such a model, seeking to attain a more generalisable and plastic ANN model. Furthermore, we wish to illuminate methods for successful integration across memories, and to investigate the implications of neuronal dynamics and network topology on the network dynamics in the dual-network memory architecture.


\cleardoublepage