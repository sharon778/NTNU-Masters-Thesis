\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\section*{Memory and High-level Cognition in Artificial Dual-Network Memory Architectures}
\addcontentsline{toc}{chapter}{Abstract}
$\\[0.5cm]$

\noindent

[Written Assignment 3 in SPRÃ…K3501] In which we had to write a preliminary abstract in order to get training in writing abstracts, and to outline what we wanted to do in our theses.
NOTE: The Methodology and Results parts are entirely fictive (at least as of now).
\\
\\
\textbf{Abstract}

Recent advances in deep learning may render several state-of-the-art approaches within other fields of artificial intelligence (AI) such as image classification and natural language processing obsolete, as they have been outperformed by more generally applicable deep learning algorithms \cite{LeCun2015, Schmidhuber2014}.  These advances are due to computational as well as algorithmic improvements. New insights into how high-level cognition may be constituted and emerge in neural networks may further advance the capabilities of deep learning \cite{Tani2014}, possibly beyond what can be achieved only by increase in computational power. At the very core of such insights lies the symbol grounding problem. \cite{Tani2014} addresses the symbol grounding problem, and proposes models which alleviate the long standing symbol grounding problem. However, there are still issues present in the model related to generalisation. The problem of generalisation has been addressed by several other authors such as \cite{McClelland1995} and more recently \cite{Hattori2014}. In the former, a dual-network memory model is implemented, inspired by how the brain might solve the problem of catastrophic forgetting (\cite{McCloskey1989, French1992}). Because the body of research on such models is fairly limited, the goal of this thesis is to address the issue of catastrophic forgetting and generalisation. Furthermore, we seek to combine this with a deep learning algorithm such as \cite{Tani2014}, where high-level cognition may be attained. The motivation for this stems from seeking to attain a greater plasticity in the algorithm. Elucidating any previously unknown aspect of plasticity in artificial neural networks and deep learning could provide key insights for further advances within deep learning, artificial intelligence, neuroscience and psychology. The problem of catastrophic forgetting is of central importance to memory formation in neural networks. Thus it is important to investigate it outside of the domain of traditional topologies, as well as the standard feed-forward back-propagate algorithm.
\\\\
\textbf{Fictive part}
\\
We implemented a hybrid model using an multiple-timescales recurrent neural network (MTRNN) for decomposing tasks into different levels of abstractions by the use of different time-scales. Furthermore, this was combined with an implementation of a working memory, inspired by the hippocampus. That is, instead of using a parametric bias vector to synchronise the two units, we used an auto-encoder, connected to a quickly adapting network module which interleaved the memories present from the different sub-networks. The main findings are that the proposed model manages to generalise very well, being able to perform tasks previously only possible in distinctly trained MTRNN networks, also associated with high-level cognition or very domain or task specific algorithms. This thesis addresses the grounding problem and the problem of catastrophic forgetting by building on the works of (\cite{McClelland1995, Hattori2014, Tani2014}). Resulting in a unified, novel model which in some regards is more generally applicable than previous artificial neural network models. The model is fairly biologically plausible, and gives rise to high-level cognitive behaviour. There may be a substantial amount of knowledge and potential applications that could be attained from further research on the proposed model.

\clearpage