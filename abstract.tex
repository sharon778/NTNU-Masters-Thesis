\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\section*{Memory and Abstraction in Artificial Neural Networks}
\addcontentsline{toc}{chapter}{Abstract}
$\\[0.5cm]$

\noindent

[Written Assignment 3 in SPRÃ…K3501] In which we had to write a preliminary abstract in order to get training in writing abstracts, and to outline what we wanted to do in our theses.
NOTE: The Methodology and Results parts removed.
\\
\\
\textbf{Abstract}

Recent advances in deep learning may render several state-of-the-art approaches within other fields of artificial intelligence (AI) such as image classification and natural language processing obsolete, as they have been outperformed by more generally applicable deep learning algorithms \cite{LeCun2015, Schmidhuber2014}.  These advances are due to computational as well as algorithmic improvements. New insights into how high-level cognition may be constituted and emerge in neural networks may further advance the capabilities of deep learning \cite{Tani2014}, possibly beyond what can be achieved only by increase in computational power. At the very core of such insights lies the symbol grounding problem. \cite{Tani2014} addresses the symbol grounding problem, and proposes models which alleviate the long standing symbol grounding problem. However, there are still issues present in the model related to generalisation. The problem of generalisation has been addressed by several other authors such as \cite{McClelland1995} and more recently \cite{Hattori2014}. In the former, a dual-network memory model is implemented, inspired by how the brain might solve the problem of catastrophic forgetting (\cite{McCloskey1989, French1992}). Because the body of research on such models is fairly limited, the goal of this thesis is to address the issue of catastrophic forgetting and generalisation. Furthermore, we seek to combine this with a deep learning algorithm such as \cite{Tani2014}, where high-level cognition may be attained. The motivation for this stems from seeking to attain a greater plasticity in the algorithm. Elucidating any previously unknown aspect of plasticity in artificial neural networks and deep learning could provide key insights for further advances within deep learning, artificial intelligence, neuroscience and psychology. The problem of catastrophic forgetting is of central importance to memory formation in neural networks. Thus it is important to investigate it outside of the domain of traditional topologies, as well as the standard feed-forward back-propagate algorithm.


\clearpage