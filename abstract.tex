\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

% \begin{centering}
% \subsubsection{TDT4900 - Computer Science, Master's Thesis}

% \end{centering}

% $\\[0.2cm]$

% \begin{centering}
% \subsection*{Short- and Long-term Memory: A Complementary Dual-network Memory Model}
% \end{centering}
% \addcontentsline{toc}{chapter}{Abstract}
% $\\[0.1cm]$
% \begin{centering}
% Spring 2016

% \end{centering}
% $\\[0.025cm]$

% \begin{centering}
% William Peer Berg
% (williapb@stud.ntnu.no)
% \\
% Supervisor: Keith L. Downing

% \end{centering}
% $\\[0.025cm]$

% \begin{centering}
% The Norwegian University of Science and Technology, NTNU
% \\
% The Department of Computer and Information Science, IDI

% \end{centering}
% $\\[0.05cm]$

% Connectionism and recent advances.
% Computational neuroscience as origin?

% Dual-network memory architecture; tapping into both.
% Catastrophic forgetting in neural networks.
% (The symbol grounding problem)

% Building upon the work of... \cite{Hattori2014}: novel model, blablabla.

% Further towards comp. neuro. w/ some other characteristic.

% Introduce deep learning
% Quickly introduce the dual-network memory model. establish connection to computer science. 
% create a research space for the thesis to fill - mention aspects that haven’t yet been solved, addressed, or argue that they should be further illuminated. CARS. 
% briefly mention what’s been done and results.

% Originally inspired by neuroscience and biology, AI has spawned several sub-fields tackling questions within computer science, and neuroscience through modeling neural networks. Two of these branches are connectionism, and computational neuroscience. By implementing artificial neural network (ANN) models, aspects within neuroscience and psychology may be illuminated. Within connectionism by seeking to algorithmically implement principles that may yield a desired model behaviour. Within computational neuroscience by implementing more biologically realistic models. Furthermore, these insights may provide a basis for enhancing existing models and algorithms.

% \footnote{McClelland, J. L., B. L. McNaughton, and R. C. O’Reilly (1995).  Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes  and  failures  of  connectionist  models  of  learning  and  memory. \textit{Psychological review 102(3)}, 419–457.}

% {\fontsize{9}{8}\selectfont 
\newpage
\noindent
\begin{centering}
    \subsubsection*{Abstract}
\end{centering}

In recent years, the possible applications of artificial intelligence (AI) and deep learning have increased drastically. However, the algorithms which constitute the learning mechanisms in deep learning are based largely on the same principles as when formalised about half a century ago. Namely using feed-forward back-propagation (FFBP) and gradient based techniques in order to train the artificial neural networks (ANNs). When training an FFBP ANN within a novel domain, it seems inevitable that this training will largely, and quite rapidly entirely disrupt the information which was formerly stored in the network.
This phenomenon is called catastrophic interference, or forgetting, and remains a long-standing issue within the field.

An architecture addressing this issue is the dual-network memory architecture, which by addressing two fundamental aspects of memory acquisition in neural networks, namely short- and long-term memory, reduces or eliminates catastrophic forgetting, as well as suggests biological implications. However, former implementations reduce catastrophic forgetting by employing pseudorehearsal, by implicitly re-training on the former weight configuration. While this provides a means of interleaving the former information with the new, it remains a slightly unrealistic training scheme.

In order to address these crucial issues within the dual-network memory architecture, this thesis implements a more biologically plausible dual-network memory model, and a novel memory consolidation scheme.
Building upon the work of Hattori (2014)\footnote{Hattori, M. (2014), A biologically inspired dual-network memory model for reduction of catastrophic forgetting. \textit{Neurocomputing 134}, 262-268.}, a more biologically realistic short-term memory model is attained, from which information may be consolidated to a long-term memory model. The model and its associated behaviour is analyzed, and a novel parametrization and resulting memory consolidation mechanism is demonstrated. This mechanism reduces catastrophic forgetting without employing pseudorehearsal, when exposing the dual-network memory model to five consecutive and distinct, but correlated sets of training patterns. This demonstrates a potential neural mechanism for reducing catastrophic forgetting, which may operate in synthesis with, or instead of pseudorehearsal. This novel memory consolidation scheme is regarded as fairly biologically realistic, as it emerges from several hippocampal aspects that are empirically observed and documented within the literature. Furthermore, the mechanism illuminates several interesting emergent qualities of pattern extraction by chaotic recall in the attained hippocampal model.
% This results in that the entire training set may be consolidated to the neocortical network in a way which maintains the original patterns significantly better. Furthermore, this is in contrast to when only exposing these training directly to the neocortical network, in which the resulting recalled patterns will be completely distorted and unrecognizable for most of the former training patterns.
% }

\clearpage