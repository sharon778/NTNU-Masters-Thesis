\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\begin{centering}
\subsubsection{TDT4900 - Computer Science, Master's Thesis}

\end{centering}

$\\[0.2cm]$

\begin{centering}
\subsection*{Short- and Long-term Memory: A Complementary Dual-network Memory Model}
\end{centering}
\addcontentsline{toc}{chapter}{Abstract}
$\\[0.1cm]$
\begin{centering}
Spring 2016

\end{centering}
$\\[0.025cm]$

\begin{centering}
William Peer Berg
(williapb@stud.ntnu.no)
\\
Supervisor: Keith L. Downing

\end{centering}
$\\[0.025cm]$

\begin{centering}
The Norwegian University of Science and Technology, NTNU
\\
The Department of Computer and Information Science, IDI

\end{centering}
$\\[0.05cm]$

% Connectionism and recent advances.
% Computational neuroscience as origin?

% Dual-network memory architecture; tapping into both.
% Catastrophic forgetting in neural networks.
% (The symbol grounding problem)

% Building upon the work of... \cite{Hattori2014}: novel model, blablabla.

% Further towards comp. neuro. w/ some other characteristic.

% Introduce deep learning
% Quickly introduce the dual-network memory model. establish connection to computer science. 
% create a research space for the thesis to fill - mention aspects that haven’t yet been solved, addressed, or argue that they should be further illuminated. CARS. 
% briefly mention what’s been done and results.

% Originally inspired by neuroscience and biology, AI has spawned several sub-fields tackling questions within computer science, and neuroscience through modeling neural networks. Two of these branches are connectionism, and computational neuroscience. By implementing artificial neural network (ANN) models, aspects within neuroscience and psychology may be illuminated. Within connectionism by seeking to algorithmically implement principles that may yield a desired model behaviour. Within computational neuroscience by implementing more biologically realistic models. Furthermore, these insights may provide a basis for enhancing existing models and algorithms.

{\fontsize{9}{8}\selectfont 
In recent years, the possible applications of artificial intelligence (AI) and deep learning have increased drastically. However, the algorithms which constitute the learning mechanisms in deep learning are based largely on the same principles as when formalised about half a century ago. Namely using feed-forward back-propagation and gradient based techniques in order to train the ANNs.
Thus, catastrophic interference, the phenomenon of largely or entirely disrupting old information when learning new in FFBP ANNs, remains a a long-standing issue within the field.
An architecture addressing this issue is the dual-network memory architecture
% \footnote{McClelland, J. L., B. L. McNaughton, and R. C. O’Reilly (1995).  Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes  and  failures  of  connectionist  models  of  learning  and  memory. \textit{Psychological review 102(3)}, 419–457.}
, which further addresses two fundamental aspects of memory acquisition in neural networks, namely short- and long-term memory. Although models within the literature are able to significantly reduce catastrophic forgetting,  they do so by employing pseudorehearsal. While this provides a means of training a network on its old weight configuration along with a new, thus interleaving the information, it remains a slightly unrealistic training scheme. Furthermore, pseudorehearsal requires a continuous generation of pseudopatterns that are provided to the model. Additionally, there is fairly little work on employing more biologically realistic ANN models in the dual-network memory architecture. Building upon the work of Hattori (2014)\footnote{Hattori, M. (2014).  A biologically inspired dual-network memory model for reduction of catastrophic forgetting. \textit{Neurocomputing 134}, 262-268.}, this thesis implements a novel dual-network memory model, and a more biologically realistic short-term memory model. The model and its associated behaviour is analyzed, and a novel parametrization and resulting memory consolidation mechanism is demonstrated. This mechanism reduces catastrophic forgetting without employing pseudorehearsal. This suggests a potential neural mechanism for reducing catastrophic forgetting, which may operate in synthesis with, or instead of pseudorehearsal. This novel memory consolidation scheme is regarded as fairly biologically realistic, as it emerges from several hippocampal aspects that are empirically observed and documented within the literature. Furthermore, the mechanism illuminates several interesting emergent qualities of pattern extraction by chaotic recall in the attained hippocampal model.
}

\clearpage