\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\section*{Memory and Abstraction in Artificial Neural Networks}
\addcontentsline{toc}{chapter}{Abstract}
$\\[0.5cm]$

Recent advances in deep learning may render several state-of-the-art approaches within other fields of artificial intelligence (AI) such as image classification and natural language processing obsolete, as they have been outperformed by more generally applicable deep learning algorithms \citep{LeCun2015, Schmidhuber2014}. These advances are due to computational as well as algorithmic improvements.
However, issues related to generalisation, catastrophic forgetting and pattern abstraction remain of central importance to deep learning algorithms. These aspects have been addressed by authors such as \cite{McClelland1995} and more recently \cite{Hattori2014}. \cite{McClelland1995} propose a dual-network memory architecture, suggesting that the brain might solve the problem of catastrophic forgetting by such an architecture \citep{McCloskey1989, French1992}. \citet{Hattori2014} builds upon this work, proposing an implementation which to a large extent ameliorates the problem of catastrophic forgetting. However, issues are still present related to memory abstraction and generalisation, as well as catastrophic forgetting.
Elucidating any previously unknown aspect of abstraction and generalisation in artificial neural networks and deep learning could provide key insights for further advances within deep learning, artificial intelligence, neuroscience and psychology.
This thesis investigates memory and abstraction in the dual-network memory architecture, proposing future directions for building a novel model which may improve the performance of long-term memory in the aforementioned architecture. Furthermore, I address the mechanisms of abstraction, functional extraction and consolidation to long-term memory, and impact of gated recurrent units on long-term memory in the dual-network memory architecture. I also address the potential for further extending the architecture using multiple-timescales recurrent neural networks. These are all aspects that I wish to further investigate and aim to pursue in my future work.


\clearpage