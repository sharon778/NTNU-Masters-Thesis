%===================================== CHAP 3 =================================

\chapter{Basic Theory}

\section{Artificial Neural Networks}
Mathematically speaking ANNs may be summarised as follows;
\\
a network consists of a set of $N$ nodes. Furthermore, for every node $i\in N$: $i$ may be connected to a node $j \in N$.
\\
For every such connection, there exists a weight, $\omega_{i,j} \in \Re$, the sub-script denoting a connection weight from node $i$ to node $j$.
\\
A transfer function $f$ is a function $f(x)$ of a neuron's input, $x$. A commonly used transfer function is the sigmoid transfer function, simply being 
\begin{equation}
    f(x) = \frac{1}{1+e^{-x}}
\end{equation}
where $x$ is the sum of a neuron's input.
\\
The update of neuronal activation for a node $j$ ca be described as follows:
\\
\begin{equation}
    x_j = \sum_{i\in M} u_i \omega_{i,j}
\end{equation}
\begin{equation}
    u_j = f(x_j)
\end{equation}
where $M$ is the set of all nodes that are connected to neuron $j$, and $u_i$ is the activation value of node $i$.

Consider an example network where the input layer consists of three nodes. A trivial case is where the input nodes are all binary, for instance symbolising a traffic light with three states; either red, yellow, or green. Thus the input to the network would either be {1, 0, 0}, {0, 1, 0}, or {0, 0, 1}, respectively. These nodes could be connected to nodes in a so called hidden layer, a layer which is neither an input nor an output layer of nodes, which again would be connected to an output layer. For this trivial example, consider that these three nodes are directly connected to the output layer, consisting of one node, which corresponds to an action: Walk or wait. It can fairly easily be seen that a successful action selection can be extracted for weights of {1, -1, -1}, if we interpret the output as walk for positive values, and wait for negative values of output. The transfer function is in this case simply a node's value times the weight between a node \textit{i} and a node \textit{j}.


\section{Notes}
Classic RNNs.
\\
The theory of Hattori (2014).
\\
The theory of Tani (2014).
\\
Include Google's now open-sourced framework for deep learning?
\\


\cleardoublepage