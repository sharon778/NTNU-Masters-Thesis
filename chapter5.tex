%===================================== CHAP 5 =================================

\chapter{Discussion and Future Directions}\label{chpt:future_work}
% ============================ section ===============================
\section{Main Findings}

%\subsection{The Dual-network Memory Architecture}

In investigating a body of research on deep learning, in particular addressing memory, generalisation, and plasticity in neural networks - the problem of catastrophic forgetting, and the symbol grounding problem appears as two of the field's central challenges. One of the main findings of the literature review is the dual-network memory architecture, which particularly addresses catastrophic forgetting. \cite{McClelland1995} proposes this architecture, and demonstrate how the brain may implement slow memory consolidation and long-term memory potentiation. Which they propose is constituted by the hippocampus and neocortex as a working memory and long-term memory, respectively. Later research such as \citep{French1997, Ans1997, Ans2000, French2001, Hattori2010, Hattori2014} building on this seminal paper demonstrates that such an architecture actually eliminates the problem of catastrophic forgetting to a large extent. However, certain issues of how to implement such a model remain obscure, including:
\begin{itemize}
\item how pseudo-patterns can be employed
\item alternative mechanisms for interleaving
\item how patterns may be learned in the hippocampal module
\item how information might be retrieved from the neocortical module
\item alternative implementations of the neocortical network
\end{itemize}

\cite{Hattori2014} proposes a novel model which resolves several issues present in his former approach by introducing a novel hippocampal network. He demonstrates that the new model converges quicker than BP, and that catastrophic forgetting does not occur if the number training patterns is sufficiently small. Furthermore, introducing a high neuronal turnover, while biologically implausible, drastically increases the storage capacity of the hippocampal network in the novel architecture. This improves the quality of the patterns which are consolidated to the neocortical network, i.e. long-term memory, thereby also improving the quality of memory recall of the dual-network memory architecture in the novel model. Further, chaotic neurons may enable a more non-deterministic and plastic behaviour to emerge.

%\subsection{RNNs}

Because GRUs enable an RNN to remember long-term temporal dependencies, I propose that it may improve the quality of interleaved memories in the dual-network memory architecture. More specifically, by attaining a more robust long-term memory for the learned patterns. Furthermore, I wish to investigate whether this may also enable the neocortical module to learn more elaborate patterns and functions, i.e. that capturing temporal long-term dependencies may enhance the networks' internal representations in the dual-network memory architecture.
Another aspect that I wish to investigate in my future work is the ability of the hippocampal module to determine stability in pattern abstraction. As the hippocampal network exhibits a form of deterministic chaos, i.e. very long cycles appearing as chaos, or cycles that may be observed to be basins of attraction, I aim to formulate a convergence criterion for stability. This could for instance be done by regarding the rate of change in the HPC module for a given stream of input with learning. If the patterns appear to change the network at a low rate, they could be consolidated to a neocortical network, similar to what is done by \cite{Hattori2014}. Furthermore, I wish to investigate possible implications for determining the stability of abstraction, with the aim of gaining further insight into possible mechanisms for abstraction across memories in ANN models. This may have implications within the domain of unsupervised learning, deep learning, neuroscience, and psychology.

%\subsection{MTRNNs}

\cite{Tani2014} demonstrates that an MTRNN is capable of extracting a functional hierarchy of action primivites by the use of a slower timescale in the higher levels of a spiking neural network. This raises the question of whether the dual-network memory architecture could be combined with the MTRNN, for instance by replacing the neocortical module and representing the prefrontal cortex (PFC) and its connections to other cortical areas such as the motor cortex. In that case, the symbol grounding problem would be addressed by using MTRNNs to have a functional hierarchy emerge, as in the work of \cite{Tani2014}. This may suggest that the way in which the hippocampal and neocortical modules are coupled will change in a new model, contrasting previously suggested models in that input is not directly propagated through the hippocampus, but propagated through the neocortical network to the hippocampus.
This would address the problem of convergence in the hippocampus, although largely avoided by \cite{Hattori2014}, potentially posing a solution in that the nature of the input to the hippocampal module will be different. Furthermore, the dynamics of the neocortical module may be implemented in a fundamentally different way from other proposed models such as \cite{Ans1997, Ans2000, French2001, Hattori2010, Hattori2014}. 
It would be interesting to investigate topological implications in such a model, experimenting with how and what parts are connected.
Such an intertwining could have the potential of capturing temporal sequences in a way that previous dual-network memory architectures have failed to. This is an idea that has been suggested by \cite{Hattori2014}, who proposes that spiking neurons could potentially capture the underlying principles for episodic memory. Concluding, it would be very interesting to embed spiking neurons in a dual-network memory architecture, building upon the work of among others \cite{Yamashita2008, McClelland1995, Hattori2014}. In this thesis our aim is to implement such a model, seeking to attain more insight into generalisability and plasticity in ANN architectures. Furthermore, we wish to illuminate methods for successful integration across memories, and to investigate the implications of neuronal dynamics and network topology on the network dynamics in the dual-network memory architecture.


% ============================ section ===============================
\section{Discussion}

\cite{Hattori2014} implements a novel hippocampal network in his dual-network memory model. This model has a better performance than previous dual-network memory implementations, and a fair overall performance in terms of convergence for input-output patterns. Thus making further investigations of hippocampal module changes more directed towards the implementation of spiking neurons, which may enable the emergence of integration for episodic memory, as \cite{Hattori2014} outlines. In order to enable such an integration, the input may have to be of a greater dimensionality. This poses the question of how such an architecture might be designed.

One the other hand we have the neocortical network in the dual-network memory architecture, which remains unchanged in both models of \cite{Hattori2010, Hattori2014}. It would be interesting to integrate novel techniques such as GRUs in this network, investigating potential performance improvements, as this could enable the network to capture more dependencies, namely also those who are temporally extended.

Combining the suggestions as elaborated on above, if an MTRNN can be used to extract a functional hierarchy, how may this be consolidated to a type of long-term memory? Remaining biologically inspired, we propose that an MTRNN may be guided by a synthesis with a hippocampal module, which may consolidate successful (i.e. converging or stable) settings to long-term memory. The long-term memory, referred to as the neocortical network, would then be in a synthesis with the MTRNN.
\cite{Hattori2014} demonstrates the ability of a hippocampal network to map binary multi-dimensional hetero-associative patterns to one another, but further demonstrates that performance is much better for auto-associative patterns, suggesting that novel mechanisms for hetero-association might be needed. Nevertheless, the hippocampal module provides the foundation for extraction of pattern-associations for consolidation to another network through pseudo-patterns.

limitations. distinct networks for the emergent phenomena. difficulties in 'interleaving' them. biologically implausible, formal justification? (should probably use previous findings, and argumentation from that GRUs capture long-term temporal dependencies, and that a functional hierarchy could be extracted and somehow represented with temporal dependencies. use figures of sketches to illustrate your thoughts!)

Flow of information from the HPC and PFC modules. Redirect information through the PFC? Discuss (and possibly suggest): investigating a novel model in which there is more continuous learning?

% ============================ section ===============================
\section{Future work}\label{future_work}

We wish to symbolically define what programatically constitutes the sub-symbolic principles for the emergence of several of the neural mechanisms that are studied in this thesis. Furthermore, we wish to combine these mechanisms, investigating possible implications in the dual-network memory architecture. This could suggest further directions for the field of AI and neural networks.

The main goal for our future work is to extend the dual-network memory architecture by implementing a novel neocortical network, inspired by state-of-the-art techniques such as using GRUs. In doing so, we wish to investigate the quality of the memories/patterns that are learned by using pseudo-patterns from the hippocampal network as proposed by \cite{Hattori2014}.
\\
In order to perform these experiments, we first need to re-create the model of \cite{Hattori2014}. We plan to implement this model by using Theano for an efficient implementational and experimental phase.

Furthermore, we wish to investigate the possibility of embedding the dual-network memory architecture with an MTRNN. We plan to analyze the tangibility of these aspirations through experimental results from the replication of the model of \cite{Hattori2014}. These empirical data, together with a more in-depth formal investigation of the tangibility of a model synthesis, will provide clues as to how such a synthesis may be performed.

Addressing high-level cognition, which has remained largely 'ignored' throughout the thesis:
\\
Continuous neural activity, perhaps even entirely deterministic, in the mammalian brain may actually be constituted entirely by the sum of accumulated experience. However, there may be some crucial parts responsible for mediating a type of focus in directing activity towards certain associations. Whether such an explicit mechanism exists, and whether it too may be entirely deterministic, remains a philosophical question. It should be noted that it lies in our command to recall memories, and even to imagine.
Anyhow, as our approach remains biologically implausible, we cannot hope to capture the aspect of consciousness in an ANN. Neither is it our aim to do so in this thesis. However, as \cite{Tani2014} argues; consciousness may emerge in the error-minimization of a top-down bottom-up synthesis of continuous sensori-motor flow. Analogously, we speculate that a form of conscious may arise from the interleaving of memories in a hierarchy more like the functional hierarchy proposed in (\cite{Yamashita2008, Tani2014}). Implementing GRUs may enable the a network symbolising the neocortical module in a dual-network memory architecture to capture such a hierarchy more fully. Possibly providing novel insights into how such a process may occur in the dynamics of complex networks and artificial neural networks.

% ============================ section ===============================
\section{Conclusion}

Furthermore, building upon the work of \cite{Hattori2014}, I wish to investigate using gated recurrent units \citep{Cho2014} in the long-term memory module, and the potential for embedding the model with an approach such as multiple-timescales recurrent neural networks (MTRNNs). My motivation for this stems from seeking to attain an artificial neural network model capable of attaining more high-level cognitive behaviour as well as integration across different abstract memories.

research gap, something that should be done
\\
related work and findings, resolving
\\
formal background
\\
preliminary analysis, concluding
\\
future work


% ============================ section ===============================
\section{Notes}

The segmentation issue due to lack of plasticity as previously mentioned is most likely still present in today's deep neural networks. -> Should be addressed.



We believe that a central aspect in continuing to advance the frontier of deep learning is to investigate how high-level level cognitive behaviour and functionality may emerge in ANNs. More specifically, we wish to further investigate the mechanism of reasoning over different memories, potentially providing insights for attaining greater plasticity and generalisation in ANN models. A crucial aspect of being able to combine different memories is simply remembering what has previously been learned. Therefore, the foremost goal of the thesis is to investigate a dual-network memory architecture, such as the model which \cite{Hattori2014} proposes. 
Furthermore, we wish to study different variations of such a model. This includes experiments where other successful and novel approaches within the field are tested, using the dual-network memory architecture as a framework for studying potential emergent neural mechanisms and network behaviour.
One interesting aspect in such experiments is how recurrency supports specific functionality and emergence within a network. In other words: How recurrence is coupled to both avoiding catastrophic interference and forgetting, and how recurrence adds dimensions to the information processing capabilities of some ANNs. Furthermore, we wish to investigate alternative implementations of the neocortical network and the associated implications for long-term memory.

Idea, thoughts: GRUs and LSTMs work because they implement a type of Hebbian learning. 
Propose that an alternative mechanism for Hebbian learning in deep networks could outperform the use of GRUs \& LSTMs.

\cleardoublepage