%===================================== CHAP 5 =================================

\chapter{Discussion and Future Directions}\label{chpt:discussion_future_work}
% ============================ section ===============================
\section{Summary}

%\subsection{The Dual-network Memory Architecture}

In investigating a body of research on neural networks, in particular addressing memory, generalisation, and plasticity - the problem of catastrophic forgetting appears to be a central issue. One of the main findings of the literature review is the dual-network memory architecture, which particularly addresses catastrophic forgetting. \cite{McClelland1995} proposes that the brain may implement slow memory consolidation and long-term memory potentiation by using the hippocampus and neocortex as a working memory and long-term memory, respectively. Later research such as \citep{French1997, Ans1997, Ans2000, French2001, Hattori2010, Hattori2014} building on this seminal paper demonstrates that such an architecture actually eliminates the problem of catastrophic forgetting to a large extent. However, certain implementational issues remain, including:
\begin{itemize}
\item optimal pseudo-pattern generation
\item how patterns may be learned in the hippocampal module
\item alternative mechanisms for interleaving of memories
\item how information might be retrieved from the neocortical module
\item alternative implementations of the neocortical network
\end{itemize}

\cite{Hattori2014} proposes a novel model which resolves several issues present in his former approach by introducing a novel hippocampal network. He demonstrates that the new model converges quicker than BP, and that catastrophic forgetting does not occur if the number training patterns is sufficiently small. Furthermore, introducing a high neuronal turnover, while biologically implausible, drastically increases the storage capacity of the hippocampal network in the novel architecture. This improves the quality of the patterns which are consolidated to the neocortical network, i.e. long-term memory, thereby also improving the quality of memory recall of the dual-network memory architecture in the novel model. Further, chaotic neurons may enable a more non-deterministic and plastic behaviour to emerge.

%\subsection{RNNs}

RNNs have long been known to be able to capture temporal dependencies by far extending those of traditional networks. However, it is not until recently that RNNs could be sufficiently trained using gradient-based approaches. Which is largely the explanation for the recent successes of deep learning consisting of deep RNNs. The remedy for not being able to capture long-term temporal dependencies was first presented as LSTMs \citep{Hochreiter1997}. Later, \cite{Cho2014} presented the simpler GRU, which effectively enables an RNN to remember long-term temporal dependencies in a manner very similar to that of LSTMs.


%\subsection{MTRNNs}

\cite{Tani2014} demonstrates that an MTRNN is capable of extracting a functional hierarchy of action primivites by the use of a slower timescale in the higher levels of a spiking neural network. This raises the question of whether the dual-network memory architecture could be combined with the MTRNN, for instance by replacing the neocortical module and representing the prefrontal cortex (PFC) and its connections to other cortical areas such as the motor cortex.


% ============================ section ===============================
\section{Discussion}\label{discussion}

\cite{Hattori2014} implements a novel hippocampal network in his dual-network memory model. This model has a better performance than previous dual-network memory implementations, and a fair overall performance in terms of convergence for input-output patterns. Thus making further investigations on hippocampal module changes more directed towards the implementation of spiking neurons may provide insights about how integration across memories and episodic memory may emerge, as suggested by \cite{Hattori2014}. In order to enable such an integration, the input may have to be of a greater dimensionality. This raises the question of how such an architecture might be designed. A simple proposal could of course be to have the input layer(s) map high-dimensional input to lower-dimensional input. However, chances are that this will omit several temporal dependencies that are required in order to successfully capture the principles for integration across memories to emerge. It would be interesting to further investigate these aspects of the dual-network memory model.

One the other hand we have the neocortical network in the dual-network memory architecture, which remains unchanged in both models of \cite{Hattori2010, Hattori2014}. It would be interesting to integrate novel techniques such as GRUs in this network, investigating potential performance improvements, as this could enable the network to capture more dependencies, namely also those who are temporally extended.
I propose that doing so may improve the quality of interleaved memories in the dual-network memory architecture by having a more robust long-term memory emerge and enabling the network to capture temporally extended patterns. Furthermore, I wish to investigate whether this may also enable the neocortical module to learn more elaborate patterns and functions, i.e. whether capturing temporal long-term dependencies may enhance the networks' representations.

As the hippocampal network exhibits a form of deterministic chaos, i.e. very long cycles appearing as chaos, or cycles that may be observed to be basins of attraction, it is possible to formulate convergence criteria for stability, as outlined by \cite{Hattori2014}. This could for instance be done by regarding the rate of change in the HPC module for a given stream of input with learning. If the patterns appear to change the network at a low rate, they could be consolidated to a neocortical network, similar to what is done by \cite{Hattori2014}. 
It would be interesting to investigate possible implications for determining the stability of abstraction itself, with the aim of gaining further insight into possible mechanisms for abstraction across memories in ANN models. This may have implications within the domain of unsupervised learning, deep learning, neuroscience, and psychology.

If the dual-network memory architecture may be extended by using MTRNNs similar to those outlined by \cite{Tani2014}, the symbol grounding problem would be addressed by using MTRNNs to have a functional hierarchy emerge. Embedding the dual-network memory architecture with such a network may result in the coupling of the hippocampal and neocortical modules changing, contrasting previously suggested models in that input may be propagated through the neocortical network to the hippocampus.
This would address the problem of convergence in the hippocampus, although largely avoided by \cite{Hattori2014}, potentially posing a solution in that the nature of the input to the hippocampal module will be different. Furthermore, the dynamics of the neocortical module may be implemented in a fundamentally different way from other proposed models such as \cite{Ans1997, Ans2000, French2001, Hattori2010, Hattori2014}. 
It would be interesting to investigate topological implications in such a model, experimenting with how and what parts are connected.
Such an intertwining could have the potential of capturing temporal sequences in a way that previous dual-network memory architectures have failed to. This is an idea that has been suggested by \cite{Hattori2014}, who proposes that spiking neurons could potentially capture the underlying principles for episodic memory. Concluding, it would be very interesting to embed spiking neurons in a dual-network memory architecture, building upon the work of among others \cite{Yamashita2008, McClelland1995, Hattori2014}.

Note that all of the above have been studied at a fairly abstract level. The coupling of differential equations has not been studied extensively. Furthermore, how the combination of principles will in fact affect the emergent behaviour of the (complex) artificial neural networks remains to be estimated by empirical data. Having only fairly abstract couplings of networks may provide clues as to how the intertwining of the networks behaves and might behave in novel models, but it may also present issues related to having certain aspects emerge. There may for instance be intricate dependencies that are only present in certain topological settings, rendering novel combinations useless if they loose a foundational aspect of emergent phenomena. Having distinct networks which are thought to be glued together in a synthesis may analogously pose issues in regards to synchronicity and unforeseen low-level detail.
Therefore, as time is of the essence, a high-level language which supports low-level optimization and parallelization will be used as the technological framework in which the model and experiments will be implemented (see chapter \ref{chpt:tech} for details).
Furthermore, experiments need to be designed in a step-wise manner, reproducing the results previously attained by \cite{Hattori2014} before proceeding to test the novel model. Because an empirical study resulting in a comparative analysis is planned to be a part of my future work, I should design a test suite which may be run on the different models and configurations. Such a test suite should have enough diversity to capture all aspects that we wish to study, and also include tests for quality-assurance of attained performance measures.

Addressing high-level cognition, which has remained largely untouched throughout the thesis:
Continuous neural activity, perhaps even entirely deterministic, in the mammalian brain may actually be constituted entirely by the sum of accumulated experience, i.e. sensory flow, including the internal propagation of it. However, there are several crucial aspects responsible for directing activity, such as the synchrony we observe, resulting in the conscious. Whether such mechanisms are inferrable, and whether they may be entirely deterministic, remains a philosophical question.
My approach as outlined om \ref{future_work} for my future work will only remain biologically inspired and most likely biologically implausible. I would like to emphasize that I do not aspire to capture aspects of consciousness in the proposed model, neither is it my aim to do so in this thesis. However, as \cite{Tani2014} argues; consciousness may emerge in the error-minimization of a top-down bottom-up synthesis of continuous sensori-motor flow. Analogously, I speculate that a form of conscious may arise from the interleaving of memories in the dual-network memory architecture - in a hierarchy more like the functional hierarchy proposed in \citep{Yamashita2008, Tani2014}. Implementing GRUs may enable the network symbolising the neocortical module in the dual-network memory architecture to capture such a hierarchy more fully, possibly providing novel insights into how such a process may occur through the dynamics of complex networks and artificial neural networks.

% ============================ section ===============================
\section{Future work}\label{future_work}

The main goal for my future work is to extend the dual-network memory architecture by implementing a novel neocortical network, inspired by state-of-the-art techniques in using using GRUs. In doing so, I wish to investigate the quality of the memories and patterns that are learned, by using pseudo-patterns from the hippocampal network as proposed by \cite{Hattori2014}. Furthermore, I wish to investigate whether this may also enable the neocortical module to learn more elaborate patterns and functions, i.e. that capturing temporal long-term dependencies may enhance the networks' internal representations.
In order to perform these experiments, I plan to implement the model of \cite{Hattori2014}, using Theano for an efficient implementational and experimental phase. Furthermore, I wish to study the novel model by investigating possible implications for generalisation and abstraction. This could suggest further directions for AI and neural networks. Particularly, I seek to answer the following questions:
\begin{itemize}
\item What are the mechanisms that enable abstraction in the model?
\item What type of functionality does the model appear to be able to extract and consolidate to the neocortical network?
\item Will a modified neocortical network using GRUs improve the model? How/why?
\end{itemize}

Furthermore, I wish to investigate the possibility of extending the dual-network memory architecture with an MTRNN. I plan to analyze the tangibility of these aspirations through experimental results from the replication of the model of \cite{Hattori2014}. These empirical data, together with a more in-depth formal novel model proposition, will provide insights about the tangibility of a potential model synthesis. The resulting topical question may be formulated as follows:
\begin{itemize}
\item Would it be possible to improve and/or embed a dual-network memory model with a multiple-timescales recurrent neural network?
\end{itemize}

Combining the suggestions as elaborated on above, if an MTRNN can be used to extract a functional hierarchy, how may this then be consolidated to a type of long-term memory? Remaining biologically inspired, I hypothesize that an MTRNN may be in a synthesis with a hippocampal module, which may consolidate successful (i.e. converging or stable) settings to long-term memory. The long-term memory, referred to as the neocortical network, would then be in a synthesis with the MTRNN.
\cite{Hattori2014} demonstrates the ability of a hippocampal network to map binary multi-dimensional hetero-associative patterns, but further demonstrates that performance is much better for auto-associative patterns, suggesting that novel mechanisms for hetero-association might be needed. Nevertheless, the hippocampal module provides the foundation for extraction of pattern-association and hetero-associative mapping.


% ============================ section ===============================
\section{Conclusion}

This thesis forms a platform for the future work of my master's thesis. It touches upon the topics of generalisability and plasticity in ANN architectures. By investigating a body of research on the topics, the aspects are illuminated, and previously applied methods for successful elimination of catastrophic forgetting and interleaving of separate memories are outlined. Furthermore, methods for capturing long-term temporal dependencies in RNNs, and MTRNNs which may self-organize into a functional hierarchy of action primitives are discussed. 
I find that neuronal dynamics and network topology in the dual-network memory architecture are aspects of particular interest that I wish to further investigate in my future work. 
I believe that a central aspect in continuing to advance the frontier of deep learning is to investigate how high-level level cognitive behaviour and functionality may emerge in ANNs. Therefore I wish to further investigate the mechanism of abstraction, and the combination of previously learned information in neural networks, in the dual-network memory architecture. This may potentially provide insights for how one may attain greater plasticity and generalisability in ANNs. 
A crucial aspect of being able to combine different memories is simply to remember what has previously been learned. This forms the primary inspiration for further investigating alternative implementations of among others the neocortical network of \cite{Hattori2014} using GRUs, investigating implications for long-term memory.

Furthermore, I wish to use the dual-network memory architecture as a framework for studying potential emergent neural mechanisms and network behaviour.
One interesting aspect which could be investigated in future experiments in the framework is how recurrence appears to support emergent model properties. In other words: How recurrence is coupled to both avoiding catastrophic interference and forgetting, and how recurrence may add dimensionality to the information processing capability of ANNs.

Because microscopic network states may be indistinguishable, what needs to be done is a comparative analysis of the macro-scale behaviour of the proposed variations in the novel dual-network memory model. In order to implement the proposed future work, I plan to use Theano; a Python library for optimized mathematical computation. Theano appears to be a good candidate for efficiently implementing models and experiments, ensuring state-of-the-art run-time performance when compared with other competitive tools and languages.

\cleardoublepage