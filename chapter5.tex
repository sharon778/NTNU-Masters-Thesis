%===================================== CHAP 5 =================================

\chapter{Discussion}\label{chpt:discussion}
% ============================ section ===============================

\subsection*{Summary}

% Some thoughts from parametrization and debugging of the HPC-net.:
% Some sparsity in IN-EC - may be compared to node sharpening
% Neuronal turnover (DG) not necessary for single set. However, DG only used during learning. How does this affect the dynamics?
% CA3 recurrently connected and seemingly chaotic. Pseudo-chaotic in that it oscillates but forms basins of attraction for learned patterns (because it maps CA3-Output for these patterns). Thus itâ€™s more of a stable state with 

Building upon several experiments, a novel dual-network memory model is implemented. It is based upon Hattori's (2014) model, and attained through a novel model parametrization and memory consolidation scheme. Furthermore, the model employs synchronous neuronal activation value propagation and synaptic modification in all layers. The memory consolidation mechanism is implemented by using the chaotically extracted outputs of the hippocampal model as the training input and output to the neocortical neural network. The hippocampal model is comparatively speaking significantly better in performance relative to the state-of-the-art in terms of perfect recall, particularly for auto-associative patterns. I believe that by using the novel memory consolidation scheme, a more biologically realistic dual-network memory model is attained. Furthermore, the demonstration of reduction in catastrophic forgetting by employing the short-term memory constituted by the hippocampal model, \textit{without} using pseudorehearsal in the neocortical module, is a novel emergent model quality. Based upon this quality, it is possible to contribute to discussions about aspects of memory consolidation, pattern recoding, and pattern separation in biological neural networks and corresponding models.
While pseudopatterns transferring information as described in the literature review may describe a somewhat biologically plausible scenario for underlying mechanisms of memory consolidation in neural networks, it is not the aim in this thesis to replicate pseudorehearsal. Contrary, the implemented model contributes to the field by demonstrating information transfer from a more biologically realistic network, namely the hippocampal model, to a more traditional network; the neocortical network. By omitting the pseudorehearsal mechanism in the more traditional ANN and long-term memory model, an alternative transfer mechanism is demonstrated. This mechanism may be regarded as more biologically realistic.
% Note yet that it may remain limited in terms of explaining intermediary storage before consolidation to cortical regions biologically speaking, as the memory space of the short-term memory model is rather limited.


\subsection*{Research questions}

% Before discussing central model decisions and aspects worth criticising, I would like to address the research questions:

\begin{itemize}
    \item What are the limitations of pattern extraction in the hippocampal model?
    
    The experiments and results of the previous chapter demonstrate that the hippocampal model may learn both auto-associative and hetero-associative training patterns equally well. This suggests that pattern separation is successful even for highly correlated patterns. However, only patterns with an input and output dimensionality of 49 were used to test the model, and as in all neural network models, a given topology and model will necessarily have a maximum capacity before its 'memory' is digested - a set of weights can only store a certain amount of information. Furthermore, an IO-dimensionality of 49 greatly constrains the state space, as does having bipolar output values. That being said, the former chapter demonstrates that the model's memory capacity does not decline for the given training sets when training on the subsets, which is in contrast to when exposing different model parametrizations to the same training sets. However, when training on the complete sets of (global) patterns at once, such as 25 patterns simultaneously, the model's storage capacity is exhausted.
    That being said, it is still capable of having desired biologically resemblant qualities emerge, both in a homogeneous and heterogeneous outcome space.
    
    \item How does asynchronous or synchronous CA3-neuronal simulation affect hippocampal model behaviour?
    
    Asynchronicity introduces randomness to the model in its search, both during learning, and recall. This randomness might increase the model's separation abilities, as suggested by the results and figures of the previous chapter. However, this increased separation ability is shown to most likely come at the cost of an exponentially expanding state space. This results in successful convergence and a 100 \% extraction rate for only the smallest of the set sizes, i.e. 2x5. A more desirable trade-off, which works more locally by recoding the synaptic connections of the incoming and outgoing connections for a subset of the neurons in the DG-layer, is demonstrated as being part of the most successful model scheme. It becomes evident that solely using the hippocampal network topology as outlined in this thesis does not suffice to successfully separate and recode patterns. The artificially drastically high neuronal turnover rate may reflect that a truly synchronous CA3-updating scheme is biologically implausible. Suggesting that a scheme in which subsets of neurons, constituting layer or local networks, operate according to rhythms, may be what is required in order to attain a more realistic and possibly improved simulation.
    
    \item How does neuronal turnover, and a synaptic weight coefficient for the outgoing synapses of the DG impact hippocampal model behaviour?
    
    Addressing the first; neuronal turnover turned out to be crucial in attaining all of the desired model qualities. This includes successful pattern separation, extraction of all patterns during perfect recall, as well as the successful extraction of patterns that may be used to reduce catastrophic forgetting during memory consolidation. The discovered mechanism with which patterns may be consolidated to the long-term memory without the use of pseudorehearsal, and yet significantly reduce catastrophic interference, is one of the main findings of this thesis.
    Furthermore, the DG-weighting is demonstrated to give rise to a performance nearly twice as high as when being set to $25$, as opposed to being a neutral value of $1$ in the synchronous CA3-neuron updating scheme. Furthermore, it is demonstrated to be central with a stronger DG-weighting in order to have the neuronal turnover-associated behaviour as described above in this paragraph emerge.
    
    \item What information seems to be inherent in the patterns extracted by chaotic recall in the hippocampal module?
    
    Related to the former research question, the results attained in the previous chapter suggest that the hippocampal model, when being presented with random input, tends to converge towards outputs reflecting the functional mapping of the training patterns. This may possibly suggest a mechanism for compressing auto-associative patterns into a single layer's activity pattern, which may then be relayed to other parts of the model. Quite possibly also suggesting a mechanism which may be at play in biological neural networks, or which may be exploited in ANNs. Furthermore, catastrophic interference is significantly reduced by employing these extracted outputs. In fact, all of the original training patterns remain recognizable, unrelated to the number of neocortical training iterations (given that the number of iterations is above a certain low value such as 15), when only training the neocortical network on the chaotically extracted pattern outputs without using pseudorehearsal. This may further suggest that the extracted patterns are separated in weight space, due to the hippocampal model's recoding and separation qualities, which appears to be anchored in the neuronal turnover and topology of the DG-layer.
    
    \item Can the original training patterns be consolidated to the neocortical network by solely using chaotically recalled patterns?
    
    When considering the findings of the experiments and results, they demonstrate a clear and significant capability of memory consolidation and information transfer by solely employing chaotically extracted model outputs for the case of auto-associative training patterns. Thus, the scope remains that of auto-associative training patterns. It remains slightly obscure whether the model would easily generalise to hetero-associative patterns. Considering that the hippocampal model's extraction rate and performance did not decrease for hetero-associative training patterns, it may be hypothesised that it would generalise to this type of training patterns as well. The mechanism with which the pattern input could be obtained might be by bi-lateral hippocampal model action potential propagation. Another more realistic option is to extend the hippocampal model, such as by the inclusion of a CA1 layer and a relaying pathway from CA1 back to the EC. These may constitute pathways as well as a mechanism for relaying both auto-associative and hetero-associative patterns to the neocortical network.
    
    \item Can chaotic interference be reduced in the novel dual-network memory model without pseudorehearsal?
    
    All outputs of the neocortical model could be recognized for the corresponding training inputs, and the average goodness of fit was above or equal to the baseline average when training the model on the original training set. Catastrophic forgetting is regarded as significantly reduced by employing the hippocampal model as a short-term memory. Pattern consolidation is performed solely by relaying the outputs extracted by chaotic recall in short-term memory as both input and output to the neocortical model. I am not aware of any previous findings demonstrating a reduction of catastrophic interference solely through the use of emergent neural network model mechanisms and behaviour such as that acquired by the hippocampal model. It is without the use of pseudorehearsal in the long-term memory module that catastrophic interference and forgetting is reduced. Furthermore, the chaotic recall mechanism employed in the short-term memory resembles those believed to part-take in the biological brain during sleep. Namely random firing activity, which is thought to process the recently formed memories of among other parts; the hippocampus.
    
\end{itemize}

% \section{Criticism}


Limited training pattern dimensionality, a highly calibrated hippocampal model, and the fairly artificial memory consolidation mechanism, raise questions about the model generalizability. Further, they raise the question of whether it may provide a basis for drawing biological parallels.
Presumably, the model is both incomplete and not biologically sound. This should be taken into account when considering the model results, as well as if using the research for further analyses. However, in algorithmically encoding some principles of biological neural network behaviour, the emergent behaviour may be regarded as somewhat generalisable.
An example is employing $k$-WTA as a crude approximation to lateral inhibition. This is demonstrated within the literature to give rise to feature discovery and self-organization, as well as redundance reduction, which in turn are aspects thought to emerge partly from lateral inhibition (see chapter \ref{chpt:background}).
Furthermore, while algorithms such as standard FFBP using gradient descent is generally not regarded as biologically plausible, it may perform similar types of orthogonalization. Thus, in this regard, gradient-descent may be regarded as approximating the aspect of orthogonalization, also performed by $k$-WTA. However, gradient-descent orthogonalizes data in a different manner; by minimizing an error-signal, separating patterns solely by weight adjustment according to error-minimization. $k$-WTA, however is in one sense less sensitive to the explicit output target distance, and may also be regarded as more dynamic in that it considers $k$ values far more than the others. Note that as demonstrated in this thesis, combining $k$-WTA with weight re-instantiation may also introduce emergent behaviour of recoding which would not necessarily make sense using standard FFBP and gradient-descent. Simply because $k$-WTA ensures a certain stability, while FFBP's signal usually operates on floats. Synaptic re-instantiation may however allow FFBP ANNs to traverse a local minima, although not necessarily into a better solution.
Another point worth mentioning related to more traditional architectures, is that FFBP ANNs have recently been shown to be able to approximate biologically plausible neural network aspects under given constraints. An example being spike-timing-dependent plasticity, i.e. performing synaptic weight modification according to the relative timing of firings between neurons \citep{Bengio2015}.

Another example addressing the issue of the generalizability of the model behaviour, is regarding the order of pattern extraction by chaotic recall. More specifically, note that the order of pattern extraction is not considered in the model and experiments. Nor is the subset which extracted patterns stem from. This poses a potential discrepancy in the extraction scheme, raising the question of whether these details may impact model behaviour, or create room for systematic errors. 
I allowed the order of extraction through chaotic recall to be neglected, because the output is likely to only reflect the current training subset, particularly in the $i$ iterations training scheme. Furthermore, if reflecting former subsets, chances are that the current subset output will not have been extracted properly. This would in turn likely result in average model trends displaying catastrophic interference.

Because the model is a connectionistic model leaning towards computational neuroscience, rather than industry-oriented deep learning, it becomes natural to address the question of biological plausibility. One of the main aspects which may be criticised in this regard, is the biological plausibility of using spurious patterns for memory consolidation. In particular, simply relaying the output of the hippocampal model as both input and output to the neocortical module remains a fairly artificial and simplistic approach. However, this may be regarded as an approximation to using actual neural network processes to relay the information. Furthermore, the attained results display a clear capability for reducing catastrophic forgetting \textit{in the model}. It is worth mentioning in this regard that they are also statistically reliable, as they are average results of several trials displaying model trends.


% Expansion encoding in DG for decorrelation of overlapping separate memories in CA3, enabling separate storage and recall. Marr (1969), Rosenblatt's original perceptron similar w/ pre-processor. (P. 41)

% The DNMM of \cite{Hattori2014} seems like a competitive network, having sparsification through in-EC and kWTA, and to some extent orthogonalization from the DG-layer through expansion encoding, as well as neuronal turnover. (P. 54)


% Furthermore; implications for neuro. Model mainly concerned with neuro. ? Comp. Sc. Parallels?

% Role of DG-cells: 
% 1. Sparsification, orthogonalization, etc.
% 2. Reduce overlap and separate overlapping inputs using kWTA - i.e. inhibitory interneurons.
% 3. Low contact through MF (DG-CA3); sparsification.
% 4. May be powerful, i.e. force new pattern during learning.
% \citep{Rolls1998chpt6}


\subsection*{Future work}

% Biological plausibility of \citep{McClelland1995} architecture. Learning direct CA3-output implausible - however, may be regarded as an approximation to the neocortical back-projections through which learning may occur, and also produce fairly stable neocortical patterns \citep{Rolls1998chpt6}. Thus, the model is a very crude approximation to brain function on that level, without episodic memory which would be implemented by associating various CA3-ouputs in a CA1-layer.

One of the issues related to the model of this thesis is the limited dimensionality of the input and output patterns which are used to test the hippocampal model. Regarding this limitation, there is in fact a fairly large body of evidence suggesting that the hippocampus most likely employs a distributed type of encoding. Such an encoding may result in that the capacity of patterns which it may store is exponential to the number of neurons in a layer.
However, this does not imply that an exponential number of pattern associations may be stored, i.e. an exponential number of different stimulus-response patterns. In fact, this has been found to increase only linearly with the number of neurons in empirical studies \citep{Rolls1998chpt6}. These are aspects that may be tested in future work on the model.

Another aspect which is more closely related to computational neuroscience that I would like to address further, is the transfer and learning mechanism which chaotic recall may constitute.
While artificial neural networks do not usually process information using biologically plausible neural network models, the implemented hippocampal model does. More specifically, the short-term memory employs Hebbian learning, and thus model behaviour such as pattern extraction may be regarded as fairly realistic at the network level.
I would like to emphasize that the dual-network model constitutes in itself a mechanism for internal information transfer \textit{from} a fairly biologically realistic network, to a more traditional one. I.e. the exposure of data to the LTM is through the pre-processing of the hippocampal model and STM, which additionally does not require pseudorehearsal in the neocortical model and LTM - at least not for smaller data sets as suggested by the thesis results. Therefore, I would like to further investigate extending the training sets which are used to analyse both pattern extraction in the STM, as well as memory consolidation to the LTM. This may then confirm or disconfirm a potential further extent of generalizability of the entire model.

Orthogonalization is a general machine learning mechanism which may lead to improved classification and/or categorization. By separating patterns from one another such that they are orthogonal in the input and output space, it is easier for auto-associative networks such as the Hopfield network, or similarly the CA3-layer of the hippocampal model, to separate these patterns from one another, and thus perform pattern completion which converges towards the actual undistorted target.
Initial experiments show that a DG-weighting within the hippocampal model of this thesis approximately similar to that in \citep{Wakagi2008}, may improve model performance. More specifically, a weighting resulting in 25 times stronger connections between the DG- and CA3-layer, may yield better perfect recall rates and quicker convergence in the attained model. This suggests that pattern separation may be performed by the DG-layer, and confirms the hypothesis that it may be more strongly interconnected with the CA3-layer. This pattern separation mechanism is likely due to an orthogonalization, which occurs due to the pattern recoding, likely caused largely by neuronal turnover. It would be interesting to further investigate the relationship between $k$-WTA, the expansion encoding of the DG-layer, and the neuronal turnover which it may perform, relative to the recoding and separation abilities of the layer. These may be considered relative to activity within the layer itself. Visualizations and/or layer-wise activation activity analyses are aspects that should be further addressed in this regard.

If the mechanism which enables a larger memory space in the hippocampal model is in fact pattern separation, this may be regarded as somewhat distributing the corresponding functional mapping in a diversifying manner. This may suggest that information transfer using chaotically extracted outputs in fact enable training the neocortical model on the consecutive knowledge separately in a way which interferes \textit{less} with previous memories due to diversification in weight space.
It may further be hypothesised that this mechanism participates in internal knowledge transfer and memory consolidation within brain-like structures. However, it remains unclear if this would suffice in explaining the capability of storing more information without disrupting old. Empirical observations of catastrophic interference occurring to some extent in the biological brain, may suggest that a trade-off is inevitable. However, this does not imply that it \textit{must} occur, only that a type of it may occur in certain situations. Relating this to the artificial model of this thesis; as a network can only hold a certain amount of information, designing experiments exceeding this capacity will necessarily exhaust its memory, potentially resulting in catastrophic interference and forgetting. How and when such interference occurs in comparison with experiments performed on human subjects, should be addressed in future research. Using association training sets as outlined and used in this thesis provides the basis for such comparative analyses, where human subjects are taught pattern-associations. However, the generalizability of such comparisons remains somewhat limited, as these tasks may be highly context-dependent, and additionally harder to measure in human subjects.
Nevertheless, whether a more explicit type of interleaving occurs in order to preserve former memories, remains slightly obscure, and also an aspect which I would like to address in future work.
\\

% Episodic memory.
% \\

A major part of the work contained within this thesis is related to the hippocampus. One of the cognitive aspects constituted largely by this part of the brain is thought to be episodic memory. I would like to address some hypotheses that have emerged while working with material on the hippocampus, and relate these to the implemented model.
Firstly, the attained model demonstrates mechanisms that may suggest biological schemes for memory consolidation, which provides the basis for drawing parallels to episodic memory. Episodic memory consists of several consecutive memories that are linked, and presumably recalled, through sequential evocation of the following. I.e. these patterns include temporal information in that they contain information which results in the recall of consecutive patterns, and thus hypothetically events.
Cognition may be said to operate on a stream of input and output, relating a great deal of information temporally. 
In a simplified manner, the patterns of the model in this thesis may be regarded as integrating a temporal aspect through a single pattern-completion mechanism. This includes very little temporal information, but does provide the foundation for linking patterns in time. 
If extending the model such as proposed with respect to relaying hetero-associative patterns to the neocortical network model, this may in fact also provide the basis for a type of episodic memory.
More specifically, relaying patterns back through the entire model would potentially provide the entire model with another layer of pattern-association, only on the level between patterns. Note that this may be performed in synthesis with the auto-association of the CA3-layer, and that input which is relayed back to the EC may then possibly change once the output has been recalled for the current specific and fairly precisely recalled output and former pattern. This may potentially result in a consecutive series of temporally linked patterns being recalled, or episodic memory and recall.

A further extension of this model could be to include spiking networks, as noted by \cite{Hattori2014}, in addition to the more sophisticated CA1-EC mapping, both which may be required in order to encompass episodic memory.
It would indeed be interesting to analyse to what extent functional mappings including temporal correlations in data could be captured such an extended model. More specifically within the type of information transfer where the activity of the EC and CA1-layers are relayed to a neocortical network model.
One clear issue related to this is the fact that an FFBP ANN using gradient descent simply learns single pattern associations without relating them temporally.
It could be hypothesized that temporally related patterns are yet another type of compressed single pattern correlation. However, due to the temporal dimension it appears more likely that both the STM and LTM models may require another level of complexity in order to capture such mappings. One suggestion is simply the incusion of GRUs in the neocortical network. Another is modifying the neocortical network such that it implements a type of topological recurrence, possibly enabling capturing temporal relations.

% On a side-note: Slow learning may not enhance learning quality or memory capacity in a single FFBP, as noted by \cite{Ans1997} (cross-check reference). However, when using two reverberating networks, as noted by Vik (2006), it may enhance learning performance. This may also be a mechanism which participates in the reduction of catastrophic interference. It is therefore an additional aspect which may be included in future extensions of the dual-network memory model, or in the architecture in general.

% Final punchline?
As neural activity is relayed from large parts of the brain to the hippocampus, this may enable the hippocampus to form memories which include information from all of these systems, i.e. to integrate across memories. Such memory integration is believed to be one of the fundamental functions performed by the hippocampus \citep{Rolls1998chpt1}.
In fact, I hypothesise that the may hippocampus essentially and solely perform mapping. Whether spatial, temporal, or any combination of those. For external and more sensory related data, for abstract patterns, as well as for internal representations and abstract patterns.
Under the assumption that the hippocampus solely performs mapping, pattern association may be a central mechanism. Furthermore, being able to compress a pattern association into an activation pattern of excited neurons in a local network or layer, such as for the extracted chaotic outputs of the implemented model, may be thought of as a representation of this mapping.
Extending this model and the model's mechanisms for pattern compression and association, and more explicitly investigating its relation to mapping, may therefore provide valuable insights into how high-level cognitive behaviour may be constituted. Furthermore, it may potentially provide the basis for a framework in which more sophisticated artificial intelligence may emerge. This is one of the main goals which I wish to pursue in my future work.
\\

Before summarising the section on future work, I would like to describe a few biological parallels that I consider worth mentioning. The first is related to memory consolidation and dreaming in the biological brain. More specifically, the process of chaotic recall performed in the hippocampal model may be compared to the process thought to occur whilst dreaming. Note that these parallels may be questionable, as the hippocampal model's memory is quickly exhausted, requiring continuous consolidation or storage of extracted patterns. This raises questions both about how the model translates into these biological parallels, and how memory acquisition and storage may be constituted in such neural structures.
It may be argued that due to the continuous learning process constantly occurring in the biological brain, it might not be entirely biologically implausible to extract chaotically recalled patterns continuously during learning. Furthermore, by re-formulating the learning and convergence criterion in the implemented model, a certain stability is maintained, and model trends are observable. This introduces the extraction of several spurious patterns, which is to be expected as a type of interpolation and prediction mechanism. Thus, while a certain stability is maintained, there is a more constant flow of chaotically recalled patterns which is generated and consolidated to the long-term memory model, which may be considered more biologically realistic than the former model.

In drawing parallels between dreaming and memory consolidation by chaotic recall, it should be noted that certain skills, or the enhancement of skills such as playing an instrument, may be observed after sleep. Concluding, chaotic recall might be a process consolidating memory to a more constant, long-term memory network, possibly enhancing the skills by associating the corresponding patterns. This may occur through episodic memory, or as a type of pattern-association compression which may be learnt by other neural structures. Interestingly, the attained pattern outputs extracted by chaotic recall in the hippocampal model may be regarded as a fingerprint in a hypothesized cognitive map, i.e. although seemingly spurious in fact containing a compressed form of pattern information.
This may be a process which enhances, or supports the neural plasticity, although more precisely how remaining slightly obscure.

A central aspect in this thesis has been switching between learning and recall in the hippocampal model. Interestingly, it was recently shown that the neurotransmitter acetylcholine may mediate learning in the hippocampus. Furthermore, these neurotransmitters are also demonstrated to cause inhibition of \textit{dentate granule} cells, due to a GABAergic response \citep{Pabst2016}. In other words, the biological brain and hippocampus implements a type of depression of DG-cells, and may also use a type of switching between learning and recall through such neurotransmitters.

Another interesting aspect of note within biological parallels is the mutual inhibition simulated by $k$-WTA. This is mediated by inhibitory interneurons in the biological brain. It would be interesting to compare the neural network behaviour of model employing $k$-WTA, and in particular in the hippocampal model, with empirical data on network activity and mediation through neurotransmitters such as GABA, and receptors such as NMDA-receptors. 
Furthermore, the stability criterion which initiates extraction of the model output, and thus implicitly memory consolidation, may be more explicitly compared with the stability of biological neural activity in vertebrates. Potentially, this may inspire adjusting the convergence criterion, or designing a novel implementation in which biological aspects are modeled more extensively.
\\

To summarise, further research is needed on several of the dual-network memory model aspects. Designing and running further experiments is needed in order to test the current model more extensively, such as with respect to training pattern complexity and model behaviour. Furthermore, the model should be extended, including the synthesis of different network topologies and learning mechanisms in the hippocampal module. This includes implementing another layer of network recurrence in the hippocampal model, such as previously outlined by implementing a CA1-layer which may relay its activation values back to the EC.
One aim of investigating this model extension is to further illuminate the information transfer mechanism constituted by the chaotically extracted patterns of the hippocampal model, along with the qualities of the patterns. These may potentially be used to develop more sophisticated or intertwined neural network algorithms and models, and to draw further neuroscientific parallels.


% NOTES

% Principal cells biologically mostly excitatory; mutual inhibition implemented by inhibitory interneurons. (GABA)


% kWTA - lateral inhibition. Tends to increase the probability of similar patterns activating the same set of neurons later on. The result is categorization of patterns into similar types. In other words; it may be regarded as a type of preprocessor. \citep{Rolls1998chpt1}.

% Could connect place fields/cells or grid cells to memory formation and a contour plot, arguing that both perform only the same operation! See gedit text.

\cleardoublepage