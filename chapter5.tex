%===================================== CHAP 5 =================================

\chapter{Discussion and Future Directions}\label{chpt:discussion_future_work}
% ============================ section ===============================
\section{Summary}

%\subsection{The Dual-network Memory Architecture}

In investigating a body of research on neural networks, in particular addressing memory, generalisation, and plasticity - the problem of catastrophic forgetting appears to be a central issue. One of the main findings of the literature review is the dual-network memory architecture, which particularly addresses catastrophic forgetting. \cite{McClelland1995} proposes that the brain may implement slow memory consolidation and long-term memory potentiation by using the hippocampus and neocortex as a working memory and long-term memory, respectively. Later research such as \citep{French1997, Ans1997, Ans2000, French2001, Hattori2010, Hattori2014} building on this seminal paper, demonstrates that such an architecture actually eliminates the problem of catastrophic forgetting to a large extent. However, certain implementational issues remain obscure, including:
\begin{itemize}
\item optimal pseudopattern-generation
\item pattern extraction in the hippocampal module
\item alternative mechanisms for interleaving of memories
\item how information might be retrieved from the neocortical module
\item alternative implementations of the neocortical network
\end{itemize}

\cite{Hattori2014} proposes a novel model which resolves several issues present in his former approach by introducing a novel hippocampal network. He demonstrates that the new model converges quicker than BP, and that catastrophic forgetting does not occur if the number of training patterns is sufficiently small. Furthermore, introducing a high neuronal turnover, while biologically implausible, drastically increases the storage capacity of the hippocampal network in the novel architecture. This improves the quality of the patterns which are consolidated to the neocortical network, i.e. long-term memory, thereby also improving the quality of memory recall of the dual-network memory architecture in the novel model. Further, chaotic neurons may enable a more non-deterministic and plastic behaviour to emerge.

%\subsection{RNNs}

RNNs have long been known to be able to capture temporal dependencies, by far extending those which traditional networks may capture. However, it is not until recently that RNNs could be sufficiently trained using gradient-based approaches. This is a large part of the reason for the success of deep learning, consisting of deep RNNs. The remedy for not being able to capture long-term temporal dependencies was first presented as LSTMs \citep{Hochreiter1997}. Later, \cite{Cho2014} presented the simpler GRU, which enables an RNN to remember long-term temporal dependencies in a manner very similar to that of LSTMs. Furthermore, it also enables networks to be trained by gradient-based approaches.


%\subsection{MTRNNs}

\cite{Tani2014} demonstrates that an MTRNN is capable of extracting a functional hierarchy of action primivites by the use of a slower timescale in the higher levels of a spiking neural network. This raises the question of whether the dual-network memory architecture could be combined with an MTRNN. Perhaps by replacing the neocortical module and representing the prefrontal cortex (PFC) and its connections to other cortical areas such as the motor cortex. While remaining unclear, I found this to be an aspect of interest for analysis in my future work.


% ============================ section ===============================
\section{Discussion}\label{discussion}

\cite{Hattori2014} implements a novel hippocampal network in his dual-network memory model. This model has a better performance than previous dual-network memory implementations, and a fair overall performance in terms of convergence for input-output patterns. Thus having further investigations on hippocampal module changes more directed towards the implementation of spiking neurons, may provide insights about how integration across memories and episodic memory may emerge, as suggested by \cite{Hattori2014}. In order to enable such an integration, the input may have to be of a greater dimensionality. This raises the question of how such an architecture might be designed. A simple proposal could of course be to have the input layer(s) map high-dimensional input to lower-dimensional input. However, chances are that this will omit several temporal dependencies that are required in order to successfully capture the principles for integration across memories. It would be interesting to further investigate these aspects of the dual-network memory model.

One the other hand there is the neocortical network in the dual-network memory architecture, which remains unchanged in both models of \cite{Hattori2010, Hattori2014}. It would be interesting to integrate novel techniques such as GRUs in this network, investigating potential performance improvements. Implementing GRUs could enable the network to capture more dependencies, for instance those of temporal extension.
I propose that doing so may improve the quality of interleaved memories in the dual-network memory architecture by resulting in a more robust long-term memory. Furthermore, I wish to investigate whether this may also enable the neocortical module to learn more elaborate patterns and functions, i.e. whether capturing temporal long-term dependencies may enhance the networks' representations of whichever data that is presented to it. An example includes the data patterns as used in \citep{Hattori2014} of binary small-scale images of letters associated with the same letter in upper- or lowercase (auto-associatively or hetero-associatively).

As the hippocampal network exhibits a form of deterministic chaos, i.e. very long cycles appearing as chaos, or cycles that may be observed to be basins of attraction, it is possible to formulate convergence criteria for stability, as outlined by \cite{Hattori2014}. This could for instance be done by regarding the rate of change in the HPC module for a given stream of input with learning, or by considering the length of a cycle when in a basin of attraction. In the former approach; if the patterns appear to change in the network at a low rate, they could be consolidated to a neocortical network, similar to what is done by \cite{Hattori2014}.
It would be interesting to investigate possible implications for changing the stability-criterion for abstraction, with the aim of gaining further insight into possible mechanisms for abstraction across memories in ANN models. This may have implications within the domains of unsupervised learning, deep learning, neuroscience, and psychology.

If the dual-network memory architecture may be extended by using MTRNNs similar to those outlined by \cite{Tani2014}, the symbol grounding problem could be addressed by using MTRNNs to have a functional hierarchy emerge. Embedding the dual-network memory architecture with such a network may result in the coupling of the hippocampal and neocortical modules changing, contrasting previously suggested models in that input may be propagated through the neocortical network to the hippocampus.
This would address the problem of convergence in the hippocampus, although largely avoided by \cite{Hattori2014}, potentially posing a solution in that the nature of the input to the hippocampal module will be different. Furthermore, the dynamics of the neocortical module may be implemented in a fundamentally different way from other proposed models such as those of \cite{Ans1997, Ans2000, French2001, Hattori2010, Hattori2014}. 
It would be interesting to investigate topological implications in such a model, experimenting with how and what parts are connected.
Such an intertwining could have the potential of capturing temporal sequences in a way that previous dual-network memory architectures have failed to. \cite{Hattori2014} proposes a similar idea, namely that spiking neurons could potentially capture the underlying principles for episodic memory. Concluding, it would be very interesting to embed spiking neurons in the dual-network memory architecture, building upon the work of among others \cite{Yamashita2008, McClelland1995, Hattori2014}.

Note that the foundation for future work is studied at a fairly abstract level in this thesis, and the coupling of differential equations has not been studied extensively. Furthermore, how the combination of principles will in fact affect the emergent behaviour of the (complex) artificial neural networks, remains to be estimated by empirical data. Because microscopic network states may be indistinguishable, what needs to be done is a comparative analysis of the macro-scale behaviour of proposed network variations. That being said, high-level couplings of networks may provide clues as to how the intertwining of the networks might behave, as well as as insights about possible issues. There may for instance be intricate dependencies that are only present in certain topological settings, rendering novel combinations useless if they fail to include a foundational aspect of emergent phenomena. Having distinct networks hypothetically glued together in a synthesis may also pose issues in regards to synchronicity and unforeseen low-level detail.
Therefore, as time is of the essence, a high-level language which supports low-level optimization and parallelization will be used as the technological framework in which the model and experiments will be implemented (see chapter \ref{chpt:tech} for details).
Furthermore, experiments need to be designed in a step-wise manner, reproducing the results previously attained by \cite{Hattori2014} before proceeding to test the novel model. Because an empirical study resulting in a comparative analysis is planned to be a part of my future work, I aim to design a test suite which may be run on the different models and configurations. Such a test suite should have enough diversity to capture all aspects that I wish to study, and also include tests that ensure a certain generality of attained performance measures.

Addressing high-level cognition, which has remained largely untouched throughout this thesis:
Continuous neural activity, perhaps even entirely deterministic, in the mammalian brain may actually be constituted entirely by the sum of accumulated experience, i.e. sensory flow, including the internal propagation of it. However, there are several crucial aspects responsible for directing activity, such as the synchrony in what generalizability observe, resulting in the conscious. Whether such mechanisms are inferrable, and whether they may be entirely deterministic, remains a philosophical question.
My approach as outlined in \ref{future_work} for my future work will only remain biologically inspired, and most likely biologically implausible. I would like to emphasize that I do not aspire to capture aspects of consciousness in the proposed model, neither is it my aim to do so in this thesis. However, as \cite{Tani2014} argues; consciousness may emerge in the error-minimization of a top-down bottom-up synthesis of continuous sensori-motor flow. Analogously, I speculate that a form of conscious may arise from the interleaving of memories in the dual-network memory architecture - in a hierarchy more like the functional hierarchy proposed in \citep{Yamashita2008, Tani2014}. Implementing GRUs may enable the network symbolising the neocortical module in the dual-network memory architecture to capture such a hierarchy more fully, possibly providing novel insights into how such a process may occur through the dynamics of complex networks and artificial neural networks.

% ============================ section ===============================
\section{Future work}\label{future_work}

The main goal for my future work is to extend the dual-network memory architecture by implementing a novel neocortical network, inspired by state-of-the-art techniques of deep learning in using using GRUs. In doing so, I wish to investigate the quality of the memories and patterns that are learned by the neocortical network, by using pseudopatterns from the hippocampal network as proposed by \cite{Hattori2014}. Furthermore, I wish to investigate whether this may also enable the neocortical module to learn more elaborate patterns and functions, i.e. that capturing temporal long-term dependencies may enhance the networks' internal representations.
In order to perform these experiments, I plan to implement the model of \cite{Hattori2014}, using Theano for an efficient implementational and experimental phase. Furthermore, I wish to study the novel model by investigating possible implications for generalisation and abstraction. This could suggest further directions for the field of AI and neural networks. Particularly, I seek to answer the following questions (as outlined in the introductory chapter):
\begin{itemize}
\item How does pseudopattern generation enable pattern extraction? With possible parallels to abstraction generally in neural networks.
\item What are the limitations of pattern extraction in the hippocampal network? And are there any limitations to the consolidation of patterns to the neocortical network, once a pattern has been extracted in the hippocampal module?
\item Will a modified neocortical network using GRUs improve the performance of learning and recall in the network, and thus in the model itself?
\item Is the proposed model less prone to network segmentation than a more conventional FFBP ANN? Where network segmentation effectively is different parts of the network operating as decoupled segments even when reacting to patterns with similar features.
\end{itemize}

Furthermore, I wish to investigate the possibility of extending the dual-network memory architecture using an MTRNN. I plan to analyze the tangibility of these aspirations through experimental results from the replication of the model of \cite{Hattori2014}. These empirical data, together with a more in-depth formal novel model proposition, may provide insights about the tangibility of a potential model synthesis. A resulting question may be formulated as follows:
\begin{itemize}
\item Would it be possible to improve and/or embed a dual-network memory model with a multiple-timescales recurrent neural network, such that this gives rise to novel emergent behaviour? And will it be possible to capture this with a form of long-term memory?
\end{itemize}

Combining the suggestions as elaborated on above, if an MTRNN can be used to extract a functional hierarchy, how may this then be consolidated to a type of long-term memory? Remaining biologically inspired, I hypothesize that an MTRNN may be implemented in a synthesis with a hippocampal module, which may consolidate successful (i.e. converging or stable) patterns/settings to long-term memory. The long-term memory also being in a synthesis with the MTRNN. It would be interesting to investigate a top-down bottom-up synthesis where functional primitives may be re-instantiated in such a scenario.
\cite{Hattori2014} demonstrates the ability of a hippocampal network to map binary multi-dimensional hetero-associative patterns, but further demonstrates that performance is much better for the case of auto-associative patterns, suggesting that novel mechanisms for hetero-association might be needed. Nevertheless, the hippocampal module provides the foundation for extraction of pattern-association and hetero-associative mapping.


% ============================ section ===============================
\section{Conclusion}

This thesis forms a platform for the future work of my master's thesis. It touches upon the topics of generalizability and plasticity in ANN architectures. By investigating a body of research on the topics, the aforementioned aspects are illuminated, and previously applied methods for successful elimination of catastrophic forgetting and interleaving of separate memories are outlined. Furthermore, methods for related work, such as capturing long-term temporal dependencies in RNNs, and having MTRNNs self-organize into a functional hierarchy of action primitives, are outlined.
All of these methods are summarised and discussed in this chapter.

I find that neuronal dynamics and network topology in the dual-network memory architecture are aspects of particular interest that I wish to further investigate in my future work. 
Furthermore, I wish to use the dual-network memory architecture as a framework for studying potential emergent neural mechanisms and network behaviour.
I believe that a central aspect in continuing to advance the frontier of deep learning is to investigate how memory may emerge in ANNs. 
A crucial aspect of memory is being able to combine different memories, not the least to simply remember what has previously been learned. This forms the primary inspiration for further investigating the dual-network memory architecture, building upon the model of \cite{Hattori2014}. One of the novel aspects that I plan to implement is using GRUs in the neocortical network, potentially enhancing the performance of learning and recall in the network.

Another interesting aspect which could be investigated in future experiments in the framework is whether the model of \cite{Hattori2014} is as prone to network segmentation as more traditional FFBP ANNs, and furthermore whether the novel model will be as prone as the model of \cite{Hattori2014}. This may potentially provide insights for how one may attain greater plasticity and generalizability in ANNs.

In order to implement the proposed future work, I plan to use Theano; a Python library for optimized mathematical computation. Theano appears to be a good candidate for efficiently implementing models and experiments, ensuring state-of-the-art run-time performance when compared with other competitive tools and languages.

\cleardoublepage