%===================================== CHAP 5 =================================

\chapter{Discussion [Macro level]}\label{chpt:discussion}
% ============================ section ===============================

% \section{Summary}

Through various experiments, a novel dual-network memory model is implemented, based upon Hattori's (2014) proposed model. It is attained through a novel model parametrization, using synchronous neuronal activation value propagation and synaptic modification, as well as by using a novel memory consolidation scheme. This scheme is implemented by using the chaotically extracted outputs of the hippocampal model as the training input and output to the neocortical neural network. The hippocampal model is comparatively speaking significantly better in performance relative to the state-of-the-art in terms of perfect recall, particularly for auto-associative patterns. I believe that by using the novel memory consolidation scheme, a more biologically realistic dual-network memory model may have been attained. Furthermore, the demonstration of reduction in catastrophic forgetting by employing the short-term memory constituted by the hippocampal model, \textit{without} using pseudorehearsal in the neocortical module, suggests that the model contains novel emergent qualities. These may contribute to explaining aspects about memory consolidation, and pattern recoding and separation in biological neural networks.
\\


% \section{Research questions}
% Before discussing aspects of the implemented model on the macro level, I would like to specifically address the research questions:

\begin{itemize}
    \item What are the limitations of pattern extraction in the hippocampal model?
    
    The experiments and results of the previous chapter demonstrate that the hippocampal model may learn both auto-associative and hetero-associative training patterns equally well. This suggests that pattern separation is successful even for highly correlated patterns. However, only patterns with an input and output dimensionality of 49 were used to test the model, and as in all neural network models, a given topology and model will necessarily have a maximum capacity before its 'memory' is digested - a set of weights can only store a certain amount of information. Furthermore, an IO dimensionality of 49 greatly constrains the state space, as does having bipolar output values. That being said, the former chapter demonstrates that the model is capable of having desired biologically resemblant qualities emerge, both in a homogeneous and heterogeneous outcome space.
    
    \item How does asynchronous or synchronous CA3-neuronal simulation affect hippocampal model behaviour?
    
    Asynchronicity introduces randomness to the model in its search, both during learning, and recall. This randomness might increase the model's separation abilities, as suggested by the results and figures of the previous chapter. However, this increased separation ability is shown to most likely come at the cost of increasing an exponential explosion in the state space, resulting in successful convergence and a 100 \% extraction rate only for the smallest of set sizes, i.e. 2x5. A more desirable trade-off, which works more locally by recoding the synaptic connections of the incoming and outgoing connections for a subset of the neurons in the DG, is demonstrated as being part of the most successful model scheme. It becomes evident that solely using the hippocampal network topology as outlined in this thesis does not suffice to successfully separate and recode patterns. The artificially drastically high neuronal turnover rate may reflect that a truly synchronous CA3-updating scheme is biologically implausible. Suggesting that a scheme in which subsets of neurons, constituting layer or local networks, operate according to rhythms, may be what is required in order to have more a more realistic parametrization result in emergent properties and a model performance and pattern quality of equal or better performance.
    
    \item How does neuronal turnover, and a synaptic weight coefficient for the outgoing synapses of the DG impact hippocampal model behaviour?
    
    Addressing the former; neuronal turnover turned out to be crucial in attaining all of the desired model qualities, as well as the discovered mechanism with which patterns may be consolidated to the long-term memory without the use of pseudorehearsal, and yet significantly reducing catastrophic interference.
    Furthermore, the DG-weighting is demonstrated to give rise to a performance nearly twice as high as when being set to $25$, as opposed to being a neutral value of $1$ in the synchronous CA3-neuron updating scheme.
    
    \item What information seems to be inherent in the patterns extracted by chaotic recall in the hippocampal module?
    
    Related to the former research question, the results attained in the previous chapter suggest that the hippocampal model, when being presented with random input, tends to converge towards outputs reflecting the functional mapping of the training patterns. This may possibly suggest a mechanism for compressing auto-associative patterns into a single layer's activity pattern, which may then be relayed to other parts of both artificial or biological neural networks. Furthermore, as catastrophic interference is significantly reduced - in fact, all of the original training patterns remain recognizable, unrelated to the number of neocortical training iterations (given that the number of iterations is above a certain low value such as 15), when only training on the chaotically extracted pattern outputs without using pseudorehearsal. This may further suggest that the extracted patterns are separated in weight space, due to the hippocampal model's recoding and separation qualities, due to the neuronal turnover and topology of the DG-layer.
    
    \item Can the original training patterns be consolidated to the neocortical network by solely using chaotically recalled patterns?
    
    When considering the findings of the experiments and results, they demonstrate a clear and significant capability of memory consolidation and information transfer by solely employing chaotically extracted model outputs for the case of auto-associative training patterns. Thus, the scope remains that of for auto-associative training patterns. It remains slightly obscure whether the model would easily generalise to hetero-associative patterns. Considering that the hippocampal model's extraction rate and performance did not decrease for hetero-associative training patterns, it may be hypothesised that it would generalise to this type of training patterns as well. The mechanism with which the input would have to be considered may be by bi-lateral hippocampal model action potential propagation, or perhaps quite more realistically, by extending the hippocampal model. A more plausible mechanism and model might be attained by the inclusion of a CA1 layer, and a relaying pathway from CA1 back to the EC, which may constitute pathways as well as a mechanism for relaying both auto-associative and hetero-associative patterns to the neocortical network.
    
    \item Can chaotic interference be reduced in the novel dual-network memory model without pseudorehearsal?
    
    As all outputs of the neocortical model could be recognized for the corresponding training inputs, and the average goodness of fit was above the baseline average when training the model on the original training set. Catastrophic forgetting is regarded as significantly reduced by employing the hippocampal model as a short-term memory. Pattern consolidation is performed solely by relaying the outputs extracted by chaotic recall in short-term memory. I am not aware of any previous findings demonstrating a reduction of catastrophic interference solely through the use of emergent neural network model mechanisms and behaviour. It is without the use of pseudorehearsal in the long-term memory module that catastrophic interference and forgetting is reduced. Furthermore, the chaotic recall mechanism employed in the short-term memory resembles those believed to part-take in the biological brain during sleep. Namely random firing activity, which is thought to process the recently formed memories of among other parts; the hippocampus.
    
\end{itemize}

% \section{Additional}

% Kritisk, kunne vært gjort annerledes og sånn... :)


One of the key issues in this thesis is likely the generalizability of the model, and associated results. Due to both the limited training pattern dimensionality, the highly calibrated hippocampal model, as well as the fairly artificial memory consolidation mechanism, are the results generalisable? And furthermore, do they provide a basis for drawing biological parallels?
Presumably, the model is both incomplete, and not biologically sound. This should obviously be taken into account when considering the model results, as well as using the research for further analyses. However, in algorithmically encoding some principles of biological neural network behaviour, the emergent behaviour may be regarded as somewhat generalisable.
An example is employing k-WTA as a crude approximation to lateral inhibition. This is demonstrated within the literature to give rise to feature discovery and self-organization, as well as redundance reduction. Which in turn are aspects thought to be emergent from lateral inhibition. 
Furthermore, while algorithms such as standard FFBP using gradient descent is generally not regarded as biologically plausible, they may perform similar types of orthogonalization. Thus, in only this regard, the former may be regarded as a fair approximation to the latter. Furthermore, FFBP has recently been shown to be able to approximate biologically plausible neural network aspects under given constraints. One example being spike-timing-dependent plasticity, i.e. performing synaptic weight modification according to the relative timing of firings between neurons \citep{Bengio2015}.

Another example addressing the generalizability of the model behaviour is regarding the order of pattern extraction by chaotic recall, where the order is not considered in the model. Nor is the subset origin from which extracted patterns stem from. This poses a potential discrepancy in the extraction scheme, raising the question of whether these details may impact model behaviour, or create room for systematic errors. However, I allowed the order of extraction through chaotic recall to be neglected, because the output is likely to only reflect the current training subset. Furthermore, if reflecting former subsets, chances are that the current subset output will not be extracted properly, and that on average, the model trends will display catastrophic interference.

Because the model is a connectionistic model, leaning more towards computational neuroscience, rather than industry-oriented deep learning, the question of biological plausibility should be addressed. One of the main aspects which may be criticised in this regard is the biological plausibility of using spurious patterns for memory consolidation. In particular, simply relaying the output of the hippocampal model as both input and output to the neocortical module remains a fairly artificial and simplistic approach. However, this may be regarded as an approximation to using actual neural networks to process and relay information. Furthermore, the attained results display a clear capability for reducing catastrophic forgetting. With results being statistically significant, as they are average results of a several trials, usually $20$.

Biological plausibility of \citep{McClelland1995} architecture. Learning direct CA3-output implausible - however, may be regarded as an approximation to the neocortical back-projections through which learning may occur, and also produce fairly stable neocortical patterns \citep{Rolls1998chpt6}. Thus, the model is a very crude approximation to brain function on that level, without episodic memory which would be implemented by associating various CA3-ouputs in a CA1-layer.

While pseudopatterns transferring information as described by Ans and French in their papers may describe a biologically somewhat plausible scenario for underlying mechanisms of memory consolidation in neural circuitry, it is not the aim to replicate it in this thesis. The specific aspect of the cognitive behaviour (i.e. all consolidation performed in the neural networks, and not in other data structures maintaining pseudopatterns). However, pseudopatterns exemplify also a means of transferring information from one network to another, as noted by \cite{Ans1997} (double-check). Therefore, my model contributes to the field by demonstrating information transfer from an actual more biologically realistic network to another. It is limited in terms of explaining intermediary storage before consolidation to the cortex - however, it may be argued that a significant amount of processed information is in fact stored in hippocampal and associated circuitry in the biological brain, which may then be consolidated during sleep, which is hypothesised to trigger a kind of chaotic recall.


% Expansion encoding in DG for decorrelation of overlapping separate memories in CA3, enabling separate storage and recall. Marr (1969), Rosenblatt's original perceptron similar w/ pre-processor. (P. 41)

% The DNMM of \cite{Hattori2014} seems like a competitive network, having sparsification through in-EC and kWTA, and to some extent orthogonalization from the DG-layer through expansion encoding, as well as neuronal turnover. (P. 54)


% Furthermore; implications for neuro. Model mainly concerned with neuro. ? Comp. Sc. Parallels?

% Role of DG-cells: 
% 1. Sparsification, orthogonalization, etc.
% 2. Reduce overlap and separate overlapping inputs using kWTA - i.e. inhibitory interneurons.
% 3. Low contact through MF (DG-CA3); sparsification.
% 4. May be powerful, i.e. force new pattern during learning.
% \citep{Rolls1998chpt6}



\cleardoublepage