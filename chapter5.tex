%===================================== CHAP 5 =================================

\chapter{Discussion}\label{chpt:discussion}
% ============================ section ===============================

\subsection*{Summary}

% Some thoughts from parametrization and debugging of the HPC-net.:
% Some sparsity in IN-EC - may be compared to node sharpening
% Neuronal turnover (DG) not necessary for single set. However, DG only used during learning. How does this affect the dynamics?
% CA3 recurrently connected and seemingly chaotic. Pseudo-chaotic in that it oscillates but forms basins of attraction for learned patterns (because it maps CA3-Output for these patterns). Thus itâ€™s more of a stable state with 

Through various experiments, a novel dual-network memory model is implemented. It is based upon Hattori's (2014) model, and attained through a novel model parametrization, and memory consolidation scheme. Furthermore, the model employs synchronous neuronal activation value propagation and synaptic modification in all layers. The memory consolidation mechanism is implemented by using the chaotically extracted outputs of the hippocampal model as the training input and output to the neocortical neural network. The hippocampal model is comparatively speaking significantly better in performance relative to the state-of-the-art in terms of perfect recall, particularly for auto-associative patterns. I believe that by using the novel memory consolidation scheme, a more biologically realistic dual-network memory model may have been attained. Furthermore, the demonstration of reduction in catastrophic forgetting by employing the short-term memory constituted by the hippocampal model, \textit{without} using pseudorehearsal in the neocortical module, suggests that the model contains novel emergent qualities. These may contribute to explaining aspects about memory consolidation, and pattern recoding and separation in biological neural networks.
While pseudopatterns transferring information as described in the literature review may describe a biologically somewhat plausible scenario for underlying mechanisms of memory consolidation in neural circuitry, it is not the aim in this thesis to replicate it. Contrary, my model contributes to the field by demonstrating information transfer from a more biologically realistic network, namely the hippocampal model, to a more traditional network. By omitting the pseudorehearsal mechanism in the more traditional ANN and long-term memory model, an alternative transfer mechanism is demonstrated. This mechanism may be regarded as more biologically realistic.
Note yet that it may remain limited in terms of explaining intermediary storage before consolidation to the cortex, as the short-term memory is also rather limited. However, it may be argued that a significant amount of processed information is in fact, biologically speaking, stored in hippocampal circuitry. This may then be consolidated during sleep, which in turn is hypothesised to trigger a kind of chaotic recall.


\subsection*{Research questions}

% Before discussing central model decisions and aspects worth criticising, I would like to address the research questions:

\begin{itemize}
    \item What are the limitations of pattern extraction in the hippocampal model?
    
    The experiments and results of the previous chapter demonstrate that the hippocampal model may learn both auto-associative and hetero-associative training patterns equally well. This suggests that pattern separation is successful even for highly correlated patterns. However, only patterns with an input and output dimensionality of 49 were used to test the model, and as in all neural network models, a given topology and model will necessarily have a maximum capacity before its 'memory' is digested - a set of weights can only store a certain amount of information. Furthermore, an IO dimensionality of 49 greatly constrains the state space, as does having bipolar output values. That being said, the former chapter demonstrates that the model is capable of having desired biologically resemblant qualities emerge, both in a homogeneous and heterogeneous outcome space.
    
    \item How does asynchronous or synchronous CA3-neuronal simulation affect hippocampal model behaviour?
    
    Asynchronicity introduces randomness to the model in its search, both during learning, and recall. This randomness might increase the model's separation abilities, as suggested by the results and figures of the previous chapter. However, this increased separation ability is shown to most likely come at the cost of increasing an exponential explosion in the state space, resulting in successful convergence and a 100 \% extraction rate only for the smallest of set sizes, i.e. 2x5. A more desirable trade-off, which works more locally by recoding the synaptic connections of the incoming and outgoing connections for a subset of the neurons in the DG, is demonstrated as being part of the most successful model scheme. It becomes evident that solely using the hippocampal network topology as outlined in this thesis does not suffice to successfully separate and recode patterns. The artificially drastically high neuronal turnover rate may reflect that a truly synchronous CA3-updating scheme is biologically implausible. Suggesting that a scheme in which subsets of neurons, constituting layer or local networks, operate according to rhythms, may be what is required in order to have more a more realistic parametrization result in emergent properties and a model performance and pattern quality of equal or better performance.
    
    \item How does neuronal turnover, and a synaptic weight coefficient for the outgoing synapses of the DG impact hippocampal model behaviour?
    
    Addressing the former; neuronal turnover turned out to be crucial in attaining all of the desired model qualities, as well as the discovered mechanism with which patterns may be consolidated to the long-term memory without the use of pseudorehearsal, and yet significantly reducing catastrophic interference.
    Furthermore, the DG-weighting is demonstrated to give rise to a performance nearly twice as high as when being set to $25$, as opposed to being a neutral value of $1$ in the synchronous CA3-neuron updating scheme.
    
    \item What information seems to be inherent in the patterns extracted by chaotic recall in the hippocampal module?
    
    Related to the former research question, the results attained in the previous chapter suggest that the hippocampal model, when being presented with random input, tends to converge towards outputs reflecting the functional mapping of the training patterns. This may possibly suggest a mechanism for compressing auto-associative patterns into a single layer's activity pattern, which may then be relayed to other parts of both artificial or biological neural networks. Furthermore, as catastrophic interference is significantly reduced - in fact, all of the original training patterns remain recognizable, unrelated to the number of neocortical training iterations (given that the number of iterations is above a certain low value such as 15), when only training on the chaotically extracted pattern outputs without using pseudorehearsal. This may further suggest that the extracted patterns are separated in weight space, due to the hippocampal model's recoding and separation qualities, due to the neuronal turnover and topology of the DG-layer.
    
    \item Can the original training patterns be consolidated to the neocortical network by solely using chaotically recalled patterns?
    
    When considering the findings of the experiments and results, they demonstrate a clear and significant capability of memory consolidation and information transfer by solely employing chaotically extracted model outputs for the case of auto-associative training patterns. Thus, the scope remains that of for auto-associative training patterns. It remains slightly obscure whether the model would easily generalise to hetero-associative patterns. Considering that the hippocampal model's extraction rate and performance did not decrease for hetero-associative training patterns, it may be hypothesised that it would generalise to this type of training patterns as well. The mechanism with which the input would have to be considered may be by bi-lateral hippocampal model action potential propagation, or perhaps quite more realistically, by extending the hippocampal model. A more plausible mechanism and model might be attained by the inclusion of a CA1 layer, and a relaying pathway from CA1 back to the EC, which may constitute pathways as well as a mechanism for relaying both auto-associative and hetero-associative patterns to the neocortical network.
    
    \item Can chaotic interference be reduced in the novel dual-network memory model without pseudorehearsal?
    
    As all outputs of the neocortical model could be recognized for the corresponding training inputs, and the average goodness of fit was above the baseline average when training the model on the original training set. Catastrophic forgetting is regarded as significantly reduced by employing the hippocampal model as a short-term memory. Pattern consolidation is performed solely by relaying the outputs extracted by chaotic recall in short-term memory. I am not aware of any previous findings demonstrating a reduction of catastrophic interference solely through the use of emergent neural network model mechanisms and behaviour. It is without the use of pseudorehearsal in the long-term memory module that catastrophic interference and forgetting is reduced. Furthermore, the chaotic recall mechanism employed in the short-term memory resembles those believed to part-take in the biological brain during sleep. Namely random firing activity, which is thought to process the recently formed memories of among other parts; the hippocampus.
    
\end{itemize}

% \section{Additional}


Limited training pattern dimensionality, a highly calibrated hippocampal model, and the fairly artificial memory consolidation mechanism, raise questions about the model generalizability. Further, they raise the question of whether it may provide a basis for drawing biological parallels.
Presumably, the model is both incomplete and not biologically sound. This should be taken into account when considering the model results, as well as if using the research for further analyses. However, in algorithmically encoding some principles of biological neural network behaviour, the emergent behaviour may be regarded as somewhat generalisable.
An example is employing k-WTA as a crude approximation to lateral inhibition. This is demonstrated within the literature to give rise to feature discovery and self-organization, as well as redundance reduction. Which in turn are aspects thought to be emergent from lateral inhibition. 
Furthermore, while algorithms such as standard FFBP using gradient descent is generally not regarded as biologically plausible, they may perform similar types of orthogonalization. Thus, in this regard, the former may be regarded as a fair approximation to the latter. Furthermore, FFBP has recently been shown to be able to approximate biologically plausible neural network aspects under given constraints. One example being spike-timing-dependent plasticity, i.e. performing synaptic weight modification according to the relative timing of firings between neurons \citep{Bengio2015}.

Another example addressing the generalizability of the model behaviour is regarding the order of pattern extraction by chaotic recall. More specifically, the order is not considered in the model. Nor is the subset which extracted patterns stem from. This poses a potential discrepancy in the extraction scheme, raising the question of whether these details may impact model behaviour, or create room for systematic errors. However, I allowed the order of extraction through chaotic recall to be neglected, because the output is likely to only reflect the current training subset, particularly in the $i$ iterations training scheme. Furthermore, if reflecting former subsets, chances are that the current subset output will not have been extracted properly. This would in turn likely result in average model trends displaying catastrophic interference.

Because the model is a connectionistic model leaning towards computational neuroscience, rather than industry-oriented deep learning, it becomes natural to address the question of biological plausibility. One of the main aspects which may be criticised in this regard, is the biological plausibility of using spurious patterns for memory consolidation. In particular, simply relaying the output of the hippocampal model as both input and output to the neocortical module remains a fairly artificial and simplistic approach. However, this may be regarded as an approximation to using actual neural network processes to relay the information. Furthermore, the attained results display a clear capability for reducing catastrophic forgetting \textit{in the model}. With results being statistically significant, as they are average results of several trials.


% Expansion encoding in DG for decorrelation of overlapping separate memories in CA3, enabling separate storage and recall. Marr (1969), Rosenblatt's original perceptron similar w/ pre-processor. (P. 41)

% The DNMM of \cite{Hattori2014} seems like a competitive network, having sparsification through in-EC and kWTA, and to some extent orthogonalization from the DG-layer through expansion encoding, as well as neuronal turnover. (P. 54)


% Furthermore; implications for neuro. Model mainly concerned with neuro. ? Comp. Sc. Parallels?

% Role of DG-cells: 
% 1. Sparsification, orthogonalization, etc.
% 2. Reduce overlap and separate overlapping inputs using kWTA - i.e. inhibitory interneurons.
% 3. Low contact through MF (DG-CA3); sparsification.
% 4. May be powerful, i.e. force new pattern during learning.
% \citep{Rolls1998chpt6}


\subsection*{Future work}

% Biological plausibility of \citep{McClelland1995} architecture. Learning direct CA3-output implausible - however, may be regarded as an approximation to the neocortical back-projections through which learning may occur, and also produce fairly stable neocortical patterns \citep{Rolls1998chpt6}. Thus, the model is a very crude approximation to brain function on that level, without episodic memory which would be implemented by associating various CA3-ouputs in a CA1-layer.

One of the issues related to the model of this thesis is the limited dimensionality of the input and output patterns which are used for testing the hippocampal model. Regarding this limitation, there is in fact a fairly large body of evidence suggesting that the hippocampus most likely employs a distributed type of encoding. Such an encoding may result in that the capacity of patterns which it may store is exponential to the number of neurons in a layer.
However, this does not imply that an exponential number of pattern associations may be stored, i.e. an exponential number of different stimulus-response patterns. In fact, this has been found to increase only linearly with the number of neurons in empirical studies \citep{Rolls1998chpt6}. These are aspects that may be tested in future work on the model.

Another aspect which is more closely related to computational neuroscience that I would like to address further, is the transfer and learning mechanism which chaotic recall may constitute.
While artificial neural networks do not usually process information using biologically plausible neural network models, the implemented hippocampal model does. More specifically, the short-term memory employs Hebbian learning, and thus model behaviour such as pattern extraction may be regarded as fairly realistic at the network level.
I would like to emphasize that the dual-network model constitutes in itself a mechanism for internal information transfer \textit{from} a fairly biologically realistic network, to a more traditional one. I.e. the exposure of data to the LTM is through the pre-processing of the hippocampal model and STM, which additionally does not require pseudorehearsal in the neocortical model and LTM - at least not for smaller data sets as suggested by the thesis results. Therefore, I would like to further investigate extending the training sets which are used to analyse both pattern extraction in the STM, as well as memory consolidation to the LTM. This may then confirm or disconfirm the generalizability of the entire model.

Orthogonalization is a general machine learning mechanism which may lead to improved classification and/or categorization. By separating patterns from one another such that they are orthogonal in the input and output space, it is easier for auto-associative networks such as the Hopfield network, or similarly the CA3-layer of the hippocampal model, to separate these patterns from one another, and thus perform pattern completion which converges towards the actual undistorted target.
Initial experiments show that a DG-weighting within the hippocampal model of this thesis approximately similar to that in \citep{Wakagi2008}, may improve model performance. More specifically, a weighting resulting in 25 times stronger connections between the DG- and CA3-layer, may yield better recall and quicker convergence in the attained model. This suggests that pattern separation may be performed by the DG-layer, and confirms the hypothesis that it may be more strongly interconnected with the CA3-layer. This pattern separation mechanism is likely due to an orthogonalization, which occurs due to the pattern recoding, likely caused largely by neuronal turnover. It would be interesting to further investigate the relationship between k-WTA, the expansion encoding of the DG-layer, and the neuronal turnover which it may perform, relative to the recoding and separation abilities of the layer. These may be considered relative to activity within the layer itself. Visualizations and/or layer-wise activation activity analyses are aspects that should be further addressed in this regard.

If the mechanism which enables a larger memory space in the hippocampal model is in fact pattern separation, this may be regarded as somewhat distributing the corresponding functional mapping in a diversifying manner. This may suggest that information transfer using chaotically extracted outputs in fact enable training the neocortical model on the consecutive knowledge separately in a way which interferes \textit{less} with previous memories due to the diversification.
It may further be hypothesised that this mechanism participates in internal knowledge transfer and memory consolidation within brain-like structures. However, it remains unclear if this would suffice in explaining the capability of storing more information without disrupting old. Empirical observations of catastrophic interference occurring to some extent in the biological brain, may suggest that a trade-off is inevitable. However, that this does not imply that it \textit{must} occur, only that a type of it may occur in certain situations. Relating this to the artificial model of this thesis; as a network can only hold a certain amount of information, designing experiments exceeding this capacity will necessarily exhaust its memory, potentially resulting in catastrophic interference and forgetting. How and when such interference occurs in comparison with experiments performed on human subjects, should be addressed in future research. Using association training sets as outlined and used in this thesis provides the basis for such comparative analyses, where human subjects are taught pattern-associations. However, the generalizability of such comparisons remains somewhat constrained, as they may be highly context-dependent, and additionally harder to measure in human subjects.
Nevertheless, whether a more explicit type of interleaving occurs in order to preserve former memories, remains slightly obscure, and also an aspect which I would like to address in future work.

Further research is needed on several of the hippocampal model qualities. Such possible research spaces are among others the synthesis of the network topologies and learning mechanisms. The aim of further investigating the aforementioned might be to discover, or illuminate potential information transfer mechanisms in neural networks. These may potentially be used to enhance existing algorithms, develop more sophisticated or intertwined neural network algorithms and models, or to draw further neuroscientific parallels.
\\

Episodic memory.
\\

A major part of the work contained within this thesis is related to the hippocampus. One of the cognitive aspects constituted largely by this part of the brain is thought to be episodic memory. I would like to address some hypotheses that have emerged while working with material on the hippocampus, and relate these to the implemented model.
Firstly, the attained model demonstrates mechanisms that may suggest biological schemes for memory consolidation, which provides the basis for drawing parallels to episodic memory. Episodic memory consists of several consecutive memories that are linked, and presumably recalled, through sequential evocation of the following. I.e. these patterns include temporal information in that they contain information which results in the recall of consecutive patterns, and thus hypothetically events.
Cognition may be said to operate on a stream of input and output, relating a great deal of information temporally. 
In a simplified manner, the patterns of the model in this thesis may be regarded as integrating a temporal aspect through a single pattern-completion mechanism. This includes very little temporal information, but does provide the foundation for linking patterns in time. 
If extending the model such as proposed with respect to relaying hetero-associative patterns to the neocortical network model, this may in fact also provide the basis for a type of episodic memory.
More specifically, relaying patterns back through the entire model would potentially provide the entire model with another layer of pattern-association, only on the level between patterns. Note that this may be performed in synthesis with the auto-association of the CA3-layer, and that input which is relayed back to the EC may then possibly change once the output has been recalled for the current specific and fairly precisely recalled output and former pattern. This may potentially result in a consecutive series of temporally linked patterns being recalled, or episodic memory recall.

A further extension of this model could be to include spiking networks, in addition to the more sophisticated CA1-EC mapping, both which may be required in order to encompass episodic memory. 
It would indeed be interesting to analyse to what extent functional mappings including temporal correlations in data could be captured within the type of information transfer where the activity of the EC and CA1-layers are relayed to a neocortical network model. 
One clear issue related to this is the fact that an FFBP ANN using gradient descent simply learns single pattern associations without relating them temporally.
It could be hypothesized that temporally related patterns are yet another type of compressed single pattern correlation. However, it seems more likely that both the STM and LTM models may require more complexity in order to capture the hypothesized temporal mappings. One suggestion is simply the incusion of GRUs in the neocortical network. Another is modifying the neocortical network such that it implements a type of topological recurrence.

On a side-note: Slow learning may not enhance learning quality or memory capacity in a single FFBP, as noted by \cite{Ans1997} (cross-check reference). However, when using two reverberating networks, as noted by Vik (2006), it may enhance learning performance. This may also be a mechanism which participates in the reduction of catastrophic interference. It is therefore an additional aspect which may be included in future extensions of the dual-network memory model, or in the architecture in general.

Related to memory capacity and dreaming: We do not dream after having learnt every new thing, but it may be argued that even though a skill is temporarily acquired it is not necessarily consolidated without a certain number of repetitions. More importantly, one has to try for a substantial amount of time when attempting to acquire skills in a fairly novel domain (ref. own experience ? ). Thus, it may be argued that it is more biologically realistic to extract chaotically recalled patterns during learning, and not between sets. However, by re-formulating the criterion for stable IO, I feel that while a certain stability is maintained, and a trend is definitely observable, several spurious patterns are also extracted, which is to be expected as a type of interpolation/prediction mechanism, - so while a stability is maintained, there is a more constant flow of chaotically recalled patterns which is generated and consolidated to the neocortical network - this is arguably more biologically realistic than in the former very artificially separated scenario.
Furthermore, in drawing parallels between dreaming and memory consolidation by chaotic recall, it should be noted that certain skills are observed to be enhanced and better learned after sleeping. Another hypothesis which challenges this entire hypothesis is the re-organization process which may be said to constantly occur in the brain as we learn, due to its plasticity.

% Final punchline?
As neural activity is relayed from large parts of the brain to the hippocampus, this may enable the hippocampus to form memories which include information from all of these systems, i.e. to integrate across memories. Such memory integration is believed to be one of the fundamental functions performed by the hippocampus \citep{Rolls1998chpt1}.
In fact, I hypothesise that the hippocampus essentially solely performs mapping. Whether spatial, temporal, or any combination of those. For external, more sensory related data, for abstract patterns, as well as for more internal very abstract patterns. 
Under the assumption that the hippocampus solely performs mapping, pattern association is a central mechanism. Furthermore, being able to compress a pattern association into an activation pattern of excited neurons in a local network or layer, such as for the extracted chaotic outputs of the implemented model, may be thought of as a representation of this mapping. 
Extending this model and the model's mechanisms for pattern compression and association, more explicitly investigating its relation to mapping, may therefore provide valuable insights into high-level cognitive behaviour. Furthermore, it may potentially provide the basis for a framework in which more sophisticated intelligence may emerge. This is one of the main goals which I wish to pursue in my future work.

% NOTES

% Principal cells biologically mostly excitatory; mutual inhibition implemented by inhibitory interneurons. (GABA)


% kWTA - lateral inhibition. Tends to increase the probability of similar patterns activating the same set of neurons later on. The result is categorization of patterns into similar types. In other words; it may be regarded as a type of preprocessor. \citep{Rolls1998chpt1}.

% Could connect place fields/cells or grid cells to memory formation and a contour plot, arguing that both perform only the same operation! See gedit text.

\cleardoublepage