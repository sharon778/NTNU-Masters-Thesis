\section{Equations}

To write an equation

\begin{verbatim}
\begin{eqnarray}\label{eq1}
F = m \times a
\end{eqnarray}
\end{verbatim}

\noindent This will produce

\begin{eqnarray}\label{eq1}
F = m \times a
\end{eqnarray}

\noindent To refer to the equation

\begin{verbatim}
\eqref{eq1}
\end{verbatim}

\noindent This will produce \eqref{eq1}.


\section{Figures}
To create a figure

\begin{verbatim}
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{fig/pikachu}
  \caption{Pikachu.}
\label{fig1}
\end{figure}
\end{verbatim}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{fig/pikachu}
 \caption{Pikachu.}
\label{fig1}
\end{figure}

\noindent To refer to the figure

\begin{verbatim}
\textbf{Fig. \ref{fig1}}
\end{verbatim}

\noindent This will produce \textbf{Fig. \ref{fig1}}

\section{References}

To cite references

\begin{verbatim}
\cite{1,2,3}
\end{verbatim}
or
\begin{verbatim}
\citep{1,2,3}
\end{verbatim}

%\noindent This will produce: \cite{1,2,3} or \citep{1,2,3}, respectively.

\section{Tables}

To create a table

\begin{verbatim}
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    \textbf{No.} & \textbf{Data 1} & \textbf{Data 2} \\ \hline
     1 & a1 & b1 \\ \hline
     2 & a2 & b2 \\ \hline
    \end{tabular}
\end{center}
\caption{Table 1.}
\label{Tab1}
\end{table}
\end{verbatim}

\noindent This will produce

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    \textbf{No.} & \textbf{Data 1} & \textbf{Data 2} \\ \hline
     1 & a1 & b1 \\ \hline
     2 & a2 & b2 \\ \hline
    \end{tabular}
\end{center}
\caption{Table 1.}
\label{Tab1}
\end{table}

\noindent To refer to the table

\begin{verbatim}
\textbf{Table. \ref{Tab1}}
\end{verbatim}

\noindent This will produce \textbf{Table. \ref{Tab1}}.

% ============================ section ===============================
\section{Notes}

Specifically; building upon the work of \cite{Hattori2014} I wish to investigate using gated recurrent units \citep{Cho2014} in the long-term memory module, and to further investigate the potential for extending the model using MTRNNs. My motivation for this stems from seeking to attain an artificial neural network model capable of attaining more high-level cognitive behaviour as well as integration across different abstract memories.

The segmentation issue due to lack of plasticity as previously mentioned is most likely still present in today's deep neural networks. -> Should be addressed.

Idea, thoughts: GRUs and LSTMs work because they implement a type of Hebbian learning. 
Propose that an alternative mechanism for Hebbian learning in deep networks could outperform the use of GRUs \& LSTMs.