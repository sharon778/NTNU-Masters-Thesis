%===================================== CHAP 1 =================================

\chapter{Introduction}\label{chpt:intro}

In recent years the field of artificial intelligence (AI) has seen a great increase in attention drawn towards it. This is largely due to the increasing success of applications during the last decade. We have reached a technological point where processing power enables traditional approaches such as fairly simple neural networks using standard feed-forward back-propagation to achieve good results on tasks like classification of data, largely due to the networks consisting of many layers (deep neural networks). Furthermore, more advanced and computationally demanding approaches have also become tractable, resulting in algorithms that have been able to learn to perform a great deal of complex tasks. Some examples include Google's DeepMind playing Atari 2600 games using deep reinforcement learning \citep{Mnih2015}, and the self-organization of a functional hierarchy of motor primitives enabling a robot to move and perform speech-recognition using a multiple-timescales recurrent neural network \citep{Tani2014}. State-of-the-art algorithms have been improved tremendously within the branches of speech-recognition, visual recognition and image classification, object recognition, and biological computing \citep{LeCun2015} due to the application of deep learning algorithms. Namely when the network-based algorithm employs multiple levels/layers of processing. This thesis focuses on deep neural networks, which have seen a vast amount of new applications during last decade within all of the aforementioned fields of AI.
\\
%\section{Artificial Neural Networks}

An artificial neural network (ANN) is a biologically inspired approach, in which some key principles believed to constitute aspects of brain functionality are implemented. It is a biologically inspired method in that it is very loosely coupled to biological neurons' actual functioning, with most of these processes being omitted algorithmic implementations, due to efficacy and applicability. If one were to implement most of the knowledge gathered from within neuroscience and biology about such networks, the result would be intractable in terms of applicability to computer scientific problems. Therefore, mathematical approximations are used, which usually aims for having some desired functionality emerge at the macro-level. \citet{McCulloch1943} proposes a formal theory for logically defining artificial neurons and neural networks which may be trained by gradient-based approaches. This is one of the core foundations for the field of artificial neural networks, having been successfully applied in different forms in ANN implementations for more than half a century.
Furthermore, the main principle constituting actual neural network functioning is recognized to be Hebbian Learning \citep{Hebb1949}, which can be summarised as "fire together - wire together". In other words, if two neurons are active at the same time, the connection between the two should be strengthened, and conversely; if they do not, the connection should be weakened.
I will address both of these approaches in the thesis.
\\
%\section{Recurrent Neural Networks}

Recurrence in neural networks has been thoroughly investigated, a central aspect being auto-associative memory, first addressed in the context of ANNs by Hopfield \citep{Hopfield1982}, after whom Hopfield networks are named. \citet{Hopfield1982} shows that in connecting McCulloch-Pitts neurons \citep{McCulloch1943} in a recursive manner, a type of auto-associative memory emerges. Presenting only part of a pattern to this network may lead to automatic pattern completion, depending on how much of a pattern that is presented, and how many patterns the network has learned. \citet{Hopfield1982} demonstrated the successful retrieval of memories, i.e. partial pattern completion when learning a number of patterns up to approximately 15 \% of $N$, being the number of neurons in the network. Note that auto-associative memory in Hopfield networks may be regarded as a type of short-term memory, due to convergence towards steady states of previously learned patterns.

I should emphasise that recurrence enables a network to remember backwards in in time. That is, to extract temporally extended patterns in data. This works similarly to how auto-associative memory does, with a connection being recurrent either implicitly in a network cycle, or slightly more explicitly in that a node is simply connected to itself. In other words, recurrent neural networks (RNNs) have the capability of learning temporal dependencies. However, traditional RNN architectures struggled with learning temporal dependencies spanning longer time windows until recently. The remedy contained within state-of-the-art deep learning algorithms today being the use of long short-term memory (LSTM) units \citet{Hochreiter1997}, or general recurrent units (GRUs) \citep{Cho2014}. Both essentially implement the same features for capturing long-term temporal dependencies, and both approaches may be trained with standard gradient-based methods. The latter point being of central importance with the applicability of gradient-based methods.

Lastly, it is worth mentioning that the mammalian brain appears to be of a recurrently connected nature. Indeed, recurrence captures aspects of long- and short-term memory. Insights from neuroscience may therefore provide clues as to how emergent phenomena may be topologically captured in ANNs.
For instance, when regarding the hippocampus' topological map, the CA3 appears to be recurrently connected to itself. This may enable the currently observed features of different parts of the input patterns to affect other parts of the preceding patterns. Potentially affecting the convergence of a population of neurons towards other features through modulatory feedback.
\\
%\section{Catastrophic Interference}

Catastrophic forgetting \citep{McCloskey1989, Ratcliff1990} is a term which describes the phenomena of when an ANN model forgets large parts, or everything, that it has previously learned. This may occur to such an extent that the performance is equal to that of randomly assigning its weights. Catastrophic interference is a term capturing both catastrophic forgetting, as well as other types of interference, such as when a network model fails to attain new knowledge. The latter might occur if a model has a certain storage capacity, and it is presented with more training patterns after its capacity has been exhausted. This may result in the network not only failing to learn new patterns, but also disrupting old patterns in attempting to do so. This is the case in Hopfield networks, where the stable states may be considered as basins of attractions in a three-dimensional space. If there are too many basins of attraction, the behaviour will be unstable, and the state of the network will oscillate (at least seemingly) chaotically throughout the entire state space.
When it comes to traditional FFBP ANNs; training the network in a novel problem domain using gradient-descent will adjust the weights according to the new domain only, neglecting all knowledge that may have previously been attained \citep{McCloskey1989, French1999, French2001}.
It can also be seen that catastrophic interference may occur if the data set that we are training an ANN model on is sufficiently complex. In this case the network may be exhausted and fail to generalise from the data, i.e. the network complexity is not sufficient to extract the desired distributions from a given data set. 
The fact that catastrophic forgetting occurs in an FFBP ANN when it is trained in a novel problem reflects the fact that the network is only a local stochastic extraction of correlations from a probability distribution constituted by a data set. Note that recurrency in networks may enable the network to capture more complex dependencies, e.g. temporal dependencies, but it may also introduce a larger state-space in doing so.
\\
%\section{The Dual-Network Memory Architecture}

\citet{McClelland1995} propose that the brain solves the problem of catastrophic forgetting by a dual-network memory architecture, implemented by the hippocampus and neocortex biologically speaking. However, the body of research within AI on the architecture is to the best of my knowledge fairly small. Further, proposed implementations suffer from issues related to simplification or obscurity \citep{French1997, French2001, Hattori2010, Hattori2014}. Note also that it is only recently recurrence in such a model has been studied. \citet{Hattori2014} investigates how trying to capture the chaotic macro-scale behaviour of the CA3 region of the hippocampus affects such a model, and concludes with the model being significantly improved. Furthermore, he states that it would be very interesting to investigate possible implications of introducing spiking neurons in such a model.
\\

%\section{Deep Learning}

Traditionally, shallow, but wide networks were used to try and solve problems of increasing complexity.
As computing power increased, deep networks started to gain attention, not the least becoming successful in applicability, solving problems of increased complexity. However, approaches still suffer from a lack of plasticity, potentially resulting in different parts of deep networks being very segmented, representing different features. In other words; a network itself may become segmented, in order to segment the patterns of a data set. I would like to emphasise that this is problematic in terms of abstraction and combining different features. For instance, introducing patterns that share many common properties may result in different parts of a network disrupting the convergence of others during training, or in the worst case catastrophic interference.
That being said, deep networks does have an advantage of an increased capacity (in that they are deep), alleviating the aforementioned issue. That being said, segmentation into sub-networks may produce distinct networks, and the model may fail to form certain abstract patterns. Therefore, researching structures capable of more complex abstraction and generalisation is a problem of central importance in deep learning today.
\\
%\section{High-level Cognition and the Symbol Grounding Problem}

High-level cognition may be regarded as sophisticated behaviour observed within the animal kingdom, tightly coupled to and heavily associated with intelligence. Language is perhaps one of the prime examples of high-level cognition. Other mammalian include; learning of motor control from sensory input, different forms of communication, internal mapping of the environment, and emotion.
How different aspects of high-level cognition may emerge in artificial neural networks remains a partly philosophical and very puzzling problem. Several applications of ANNs have demonstrated the ability of models to perform tasks previously only associated with high-level cognition, such as the learning and inference of the dynamically changing Wisconsin card sorting test \citep{Maniadakis2012}, or the successful learning of motor control in robots \citep{Sugita2005, Yamashita2008, Tani2014}. 
This does not, however, address the aspect of how cognition is coupled to sensory input in neural networks, which is again related to consciousness. An analogy is as outlined by \cite{Freeman2003}; how a gas condenses to a liquid, and how the environment enables this process of phase transitioning.
It is not my aim to answer these questions in this thesis. However, I am inspired by these questions in terms of investigating potential mechanisms for plasticity and memory in neural network models.
\\
%\section{Summary and thesis outline}

Deep learning has led to a tremendous advance in the capabilities of AI within recent years. One particular aspect leading to the advances is the inclusion of temporal information for larger time spans in deep neural networks, which is attained by using LSTM units or GRUs. However, these units are usually very spatially constrained topologically speaking, raising the question of whether such solutions are prone to segmentation issues where certain features may only be processed in a local neighbourhood. This would suggest that the plasticity needed to combine memories or patterns in a more general and abstract way may not be not yet be present in today's state-of-the-art deep learning algorithms. There may for instance be slightly more intricate network structures which enable abstraction. This is exactly the basis which is formed by the dual-network memory architecture, and it is my aim in this thesis to build upon this architecture. 
The main research topic is to study memory abstraction in artificial dual-network memory architectures, investigating possible implications for general deep learning algorithms. Where memory is an abstract term used to refer to functions, patterns, or data-set correlations that a network extracts and hopefully learns. More specifically, building upon the model which \cite{Hattori2014} proposes, my aim is to answer the following questions:
\begin{itemize}
\item What are the mechanisms that enable abstraction in the model?
\item What type of functionality does the model appear to be able to extract and consolidate to the neocortical network?
\item Will a modified neocortical network using GRUs improve the model? How/why?
\end{itemize}

Furthermore, self-organization of reusable functional hierarchies such as in \citep{Tani2014} seems to be something that is not yet captured by general deep learning algorithms. Now, in order to make use of such representations, a top down bottom up synthesis of working memory and long term memory may be needed. Therefore I also wish to investigate the possibility of embedding the technique as outlined by \cite{Tani2014} in the dual-network memory model. More specifically:
\begin{itemize}
\item Would it be possible to improve and/or embed a dual-network memory model with a multiple-timescales recurrent neural network?
\end{itemize}

I believe that a central aspect for advancing the frontier of deep learning is to investigate how high-level level cognitive behaviour and functionality may emerge in ANNs. Investigating the mechanism of reasoning over different memories may potentially provide insights for attaining greater plasticity and generalisation in ANN models. This may be seen by considering that a crucial aspect of being able to combine different memories is simply remembering what has previously been learned. Therefore the foremost goal in this thesis is to investigate the dual-network memory architecture which \cite{Hattori2014} proposes in this context.

%\section{Thesis structure}
The structure of this thesis will be as follows. After this introductory chapter, a theoretical foundation is presented, providing the basis for understanding ANNs. This is followed by a chapter on related work such as the dual-network memory architecture, and recurrent neural networks. While I am aware that the research body of the literary review is limited, I believe that it suffices as a foundation for suggesting directions for future work. It is my aim to contribute to the field by providing some novel perspectives on neural network behaviour related to memory and abstraction by using the dual-network memory architecture as a framework. This may have result in suggesting possible implications for deep learning in general. In chapter \ref{chpt:tech} the technological platform is outlined, consisting of a presentation of the framework (Theano) and the reason for why this particular framework was selected, including a demonstration of an intermediary implementation. Lastly, the thesis findings are summarised and discussed in chapter \ref{chpt:discussion_future_work}, before I outline future work which I aspire to address in my master's thesis.


\cleardoublepage