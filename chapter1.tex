%===================================== CHAP 1 =================================

\chapter{Introduction}\label{chpt:intro}

% Recently, achievements AI, mentions DL ...
In recent years, the possible applications of artificial intelligence (AI) have increased drastically. From autonomous self-driving cars \citep{Urmson2009}, to facial recognition systems with super-human performance \citep{Sun2014}, to IBM's Watson performing medical diagnoses \citep{Wagle2013}, to Google's DeepMind playing Atari 2600 games \citep{Mnih2015}. Yet there is a vast amount of potential which has yet to be explored within the field; as IT is becoming increasingly ubiquitous, so does the potential applicability of AI. One of the reasons is that the presence of IT enables the harvesting and analyzing of data. Two of the common factors for recent advances within AI are increased computational power, and algorithmic improvements. Not only have algorithms performing facial recognition using deep learning become tractable on an average high-end desktop-computer, there has also been an explosion in data generation as well as availability within recent decades. In the future, we are likely to see a similar expansion in available data within the public domain, with a vast array of applications. Also quite noteworthy, this is occurring in synthesis with the Internet of things, i.e. connecting ordinary objects to the Internet. In other words, visual recognition will be far from the only tangible deep learning domain in the future.
% Could remove this last bit.
Note also that a synthesis of data from different domains may enable a cross-domain data synthesis, which although increasing the complexity of the data set may also increase the quality of the predictions performed by deep learning algorithms.
Cross-domain syntheses have already shown promising results in tasks such as feature extraction, and image classification, and in recommendation systems \citep{Huang2013, Shapira2013}.
\\

% What is DL?
At the core of several of the aforementioned algorithms lies the sub-field of AI known as deep learning, as previously mentioned. Deep learning is a class of algorithms where multiple layers of processing are employed, enabling the extraction of intricate correlations and patterns in data sets without any prior domain-knowledge nor supervision during learning. Employing a priori knowledge through convolutional filters, has further enabled deep learning algorithms to attain excellent performance in domains such as speech recognition, image classification, and genomics \citep{LeCun2015}, displaying the wide applicability of deep learning, as well as the potential benefits it may bring to society. Furthermore, the convolutional filters, elements of pre-processing that essentially perform feature detection in data sets, may be constituted solely by deep neural networks, i.e. by employing the same structure and algorithm as in the rest of the network \citep{LeCun2015}. This may enable a network to learn convolutional operations, i.e. desired filters, as data sets is presented to the model. Resulting in a more dynamic and generally applicable unsupervised learning algorithm, although possibly requiring larger data sets for training.
Summarising, the same technique for extracting intricate correlations in a data set may in fact be used to find the filters that reduce raw input data to feature vectors, which the remaining deep network may then process. This is the core algorithm employed in deep learning; the artificial neural network (ANN). Two of its variants, the traditional feed-forward back-propagation (FFBP) network, using gradient-descent, and a slightly more complex and biologically realistic network using $k$-winners-take-all and Hebbian learning, are employed in this thesis. I will get back to the research goal as well as clear research questions later in this introductory chapter.

% ANNs
The neural network is a biologically inspired class of algorithms which borrows its vocabulary from neuroscience. Going back to 1943, \cite{McCulloch1943} proposed to formalize neural functioning within logical propositions.
However, they could only create a logical calculus based upon the abstract assumptions of the neurological and psychological basis of the time. Formalizing neural networks was nevertheless a seminal contribution, in which the important assumption that neurons could be considered as binary processing units was made, due to the observation that they seemed to fire in an "all-or-none" fashion. This created a framework with which psychological phenomena could be regarded in a reductionist way; through the lens of two-valued logic. The way in which neural networks were defined, and thus changed during termination, however, remained obscure. \cite{McCulloch1943} noted that: "With determination of the net, the unknowable object of knowledge, the "thing in itself", ceases to be unknowable"
\footnote{McCulloch, W. S., \& Pitts, W. 1943. 'A Logical Calculus of the Idea Immanent in Nervous Activity', \textit{Bulletin of Mathematical Biophysics}, \textbf{5}: 131.}
Thus describing the long-standing symbol-grounding problem, which has yet to be elucidated.

% History for NNs
In his 1895 manuscript for 'Project for a Scientific Psychology', Sigmund Freud had in fact already suggested that synaptic transmission could promote post-synaptic neural excitation \citep{Kiernan2011}. Despite predicting a variant of Hebbian learning more than half a century before Hebb himself, his manuscript was not published until 1950. Therefore the phenomena of synaptic modification resulting from, and occurring relative to the correlation in neural activity among neurons, is accredited to Donald O. Hebb \citep{Kiernan2011}.
As \cite{Hebb1949} eloquently put it: "One cannot logically be a determinist in physics and chemistry and biology, and a mystic in psychology"
\footnote{Hebb, D. O. 1949. \textit{The Organization of Behavior: A Neuropsychological Theory}. New York: JOHN WILEY \& SONS, Inc. Pp. xiii}.
He further outlined the paradigm in which neural functioning is deterministic, and proposed the seminal concept of Hebbian learning, in his 1949 book 'The Organization of Behavior: A Neuropsychological Theory'. Hebbian learning is widely recognized as the fundamental mechanism underlying synaptic modification and learning in biological neural networks. Simply put, Hebbian learning may be summarised as; "fire together; wire together".

% Summary and continuation, introduction of connectionism and comp. neruo.; argumentation for synthesis with AI & DL
In less than a decade after the proposal of \cite{McCulloch1943}, a formal calculus for neural network computation, as well as a neuropsychological theory containing principles for how neurons may perform synaptic modification was proposed. The latter suggesting a mechanism for neural network computation, and thus possible mechanisms for memory and learning. From this historical point in time and forward, there has been a continuous synthesis between psychology, neuroscience and computer science with regard to neural functioning: Computer science lending itself to construct models within computational neuroscience, which seek to explain certain neurological aspects of brain functioning. These, in turn, may explain or be related to aspects of cognition. Conversely, insights from neuroscience and psychology may lend themselves to, and inspire researchers within more computationally related disciplines in creating more powerful neural network algorithms, performing tasks such as pattern extraction, clustering, classification or segmentation. 
Naturally, this synthesis has spawned different scientific fields. Two of which are known as connectionism and computational neuroscience. While computational neuroscience attempts to tackle problems within neuroscience, as briefly touched upon above, connectionism may be described as aiming solely towards attaining neural network behaviour, potentially disregarding the biological plausibility. This does not necessarily mean that researchers seek to use the model only for computer scientific purposes, as one may argue that certain psychological or neurological aspects may be studied without all model aspects being biologically plausible. In fact, until we fully fathom brain functioning neurologically speaking, we cannot hope to create a model which will encompass all of its biological aspects. Note, however that this may not be a necessity in order to have certain phenomena emerge from a neural network model. In fact, it may be hypothesized that certain principles underlie a certain neural functioning, and thus if simulated algorithmically, the same behaviour may emerge in a model, despite possibly being implemented quite differently. This is what largely forms the basis for connectionism, where researchers aim to study cognitive phenomena through usually simplistic computational models. The main point being that the constraint of biological plausibility is relaxed.
Quite often, a synthesis between the two aforementioned fields emerges in engineered models and solutions. This might enable researchers to study both how the brain may implement certain functionality, as well as to study possible algorithmic implications within the more purely applied disciplines of AI such as deep learning.
\\

\textit{This is the primary motivation for the thesis topic.}
\\

%\section{Catastrophic Interference}
Before introducing the model which will be used in this thesis, I would like to introduce the concept of catastrophic forgetting.
\cite{Hebb1949} described the general case of learning new information which may disrupt old, as the sensitivity-stability dilemma, in which one has to decide on how sensitive a network should be to new information through its parametrization. Making it more sensitive to recent information would of course most likely, depending on the network model, largely disrupt the old information contained within the network, thus making it very unstable, and conversely; maintaining the current configuration too heavily may result in failing to acquire new knowledge or to extract new information. This problem is also known as the stability-plasticity \citep{Carpenter1987} problem, placing network models on a scale from being very plastic to very stable.
Catastrophic forgetting \citep{McCloskey1989, Ratcliff1990} is a term which describes the phenomena of when an ANN model "forgets" large parts, or everything, that it has previously learnt. I.e. the weight tuning which corresponds to correlation extraction in a data set is erased. This may occur to such an extent that the model performance is equal to that of randomly assigning its network weights. Catastrophic interference is a term capturing both catastrophic forgetting, as well as other types of interference, such as when a network model fails to attain new knowledge, i.e. the network being unable to capture further correlations. For instance, in a recurrent network using Hebbian learning, catastrophic interference occurs when increasing the number of training patterns beyond a certain extent. This results in the network not only failing to learn new patterns, but it also disrupts old, previously learnt patterns, possibly making the model useless. This is the case in Hopfield networks, which I will get back to in chapter \ref{chpt:background}, where stable states may be considered as basins of attractions in a three-dimensional space. If there are too many basins of attraction, the behaviour will be unstable, and the state of the network will oscillate in a chaotic manner.
When it comes to traditional feed-forward back-propagation ANNs; training a network in a novel problem domain using gradient-descent will adjust the weights according to the new domain only, neglecting all knowledge that may have been previously attained \citep{McCloskey1989, French1999, French2001}. This necessarily results in catastrophic forgetting if training solely on new patterns, which then disregards old. If all patterns are included, the old and new alike, the algorithm of FFBP using gradient-descent will seek to minimize the error signal over all patterns, thus maintaining old knowledge equally well as new, as long as it is represented in the current training data. Note, however, that rehearsing on all previously learnt information may be regarded as biologically unrealistic. This provides the basis for investigating complementary or optional, and potentially more biologically realistic mechanisms for reducing catastrophic forgetting.
% Move this bit to the DNMA to chpt. 2?
Note also that catastrophic interference may occur if a model is given a training set which exceeds the complexity that the network may capture. If the network complexity is insufficient to extract the desired distributions from the data set, gradient-descent may also fail to converge towards a solution, or only extract the most principal components.
The fact that catastrophic \textit{forgetting} occurs in an FFBP ANN model when it is trained in a novel problem domain, reflects that the network is only a local stochastic extraction of correlations from a probability distribution constituted by data sets from that particular domain. Interestingly, recurrence in networks may enable a network to capture more complex dependencies, such as temporal ones, but it may also introduce a larger state-space in doing so.
\\

%\section{The Dual-Network Memory Architecture}
\citet{McClelland1995} propose that the brain solves the problem of catastrophic forgetting by a dual-network memory architecture, implemented by the hippocampus and neocortex. However, the body of research within AI on the architecture, is to the best of my knowledge fairly limited. Furthermore, proposed implementations suffer from issues related to simplification or obscurity \citep{French1997, French2001, Hattori2010, Hattori2014}. Note also that it is only recently that a more biologically plausible hippocampal network in such a model has been studied. \citet{Hattori2014} investigates how seeking to capture the chaotic macro-scale behaviour of the CA3 region of the hippocampus affects the model's behaviour, and concludes that the model is significantly improved when compared to his former work. His former work does in turn significantly outperform previous implementations of the dual-network memory architecture. A short abstract definition of the model is that it consists of two networks, where one represents working or short-term memory, and the other long-term memory. As patterns are learnt by the first network, they are consolidated to the second through the use of pseudopatterns. A pseudopattern remains roughly the same throughout different papers investigating the dual-network memory model, such as \citep{French1997, Ans2000, French2001, Hattori2010, Hattori2014}. Fundamentally, a pseudopattern is a pattern that reflects the configuration of the long-term memory network, which may be used to represent former training patterns such that they may be re-learnt along with new, minimizing the loss of old knowledge. Pseudopatterns are introduced formally in the following chapter; chapter \ref{chpt:background}.
% \cite{Hattori2014} also states that it would be very interesting to investigate possible implications of introducing spiking neurons in his architecture.
\\
%\section{Summary and thesis outline}

Deep learning has led to a tremendous advance in the capabilities of AI within recent years. However, the plasticity needed to combine memories and patterns in a more general and abstract way may not be not yet be present in today's state-of-the-art deep learning algorithms. One observation supporting this statement is the fact that catastrophic forgetting occurs in FFBP ANNs. Because modern state-of-the-art algorithms still largely employ variants of gradient descent, it is likely that they are prone to the same type of interference. Possibly ameliorated by the recently drastically increased storage capacity of today's ANNs and deep learning algorithms.
It is worth mentioning that in addition to seeking to reduce catastrophic forgetting through a more complex short-term memory model, slightly more intricate network structures may also enable more sophisticated abstraction, as exemplified by authors such as \cite{Tani2014}. This basis may be formed by the dual-network memory architecture, which enables abstraction of short- and long-term pattern-associations.
In this thesis the main research topic is to study memory abstraction in the dual-network memory architecture, investigating possible implications for general deep learning algorithms, as well within computational neuroscience. Note that memory is an abstract term used to refer to functions, patterns, or data set correlations that a network extracts and maintains for a variable time span.
\\

In this thesis, 
the primary research goal is: \textit{"To study how the brain might implement short- and long-term memory using the dual-network memory architecture, and to implement a novel dual-network memory model"}.
\\

I aspire to do this by further investigating the dual-network memory model of \citep{Hattori2014}, and related work.
More specifically, building upon \citeauthor{Hattori2014}'s (\citeyear{Hattori2014}) model, my aim is to answer the following questions:
% \begin{itemize}
% \item How does pseudopattern generation enable pattern extraction? With possible parallels to abstraction in neural networks.
% \item What are the limitations of pattern extraction in the hippocampal network? And are there any limitations to consolidation to the neocortical network once a pattern has been extracted in the hippocampal module?
% %\item Will a modified neocortical network using GRUs improve the performance of learning and recall in the network, and thus in the model itself?
% % \item Is the proposed model less prone to network segmentation than a more conventional FFBP ANN? Where network segmentation effectively is different parts of the network operating as decoupled segments. 
% %[Needs refactoring]: First; check whether this occurs. It needs to be demonstrated.
% %Furthermore; it will vary depending on the type of representation used in the network. Distributed rep. may be prone to segmentation. Local representations will be segmented by definition? What happens during overlaps in that case? In the DNMM, there is no guarantee for a given representation, although it is a mix between a distributed rep. using local differences? I.e. more of a fully distributed rep. - Which would not be as prone to segmentation. Parallels to FFBP ANNs?
% \item How does sparsity within the DG-layer affect and improve the hippocampal module?
% \item How does the chaotic nature of the CA3-layer impact the model? Specifically, how does it enable a type of working memory and recall of different patterns with unchanging input?
% \end{itemize}

\begin{itemize}
    \item What are the limitations of pattern extraction in the hippocampal model?
    \item How does asyncronous or synchronous CA3-neuronal simulation affect hippocampal model behaviour?
    \item How does neuronal turnover, and a synaptic weight coefficient for the outgoing synapses of the DG impact hippocampal model behaviour?
    \item What information seems to be inherent in the patterns extracted by chaotic recall in the hippocampal module?
    \item Can the original training patterns be consolidated to the neocortical network by solely using chaotically recalled patterns?
    \item Can chaotic interference be reduced in the novel dual-network memory model without pseudorehearsal?
\end{itemize}

I believe that a central aspect for advancing the frontier of deep learning is to investigate how high-level level cognitive behaviour and functionality may emerge in ANNs. Investigating mechanisms associated with reasoning over different memories may potentially provide insights for attaining greater plasticity and generalization in ANN models. This may be seen by considering that a crucial aspect of being able to combine different memories is simply remembering what has previously been learnt. 
% [Elaborate using theories about the hippocampus and memory from Rolls and Treves] 
Therefore the foremost goal in this thesis is to investigate the dual-network memory architecture in this context.

%\section{Thesis structure}
The structure of the thesis will be as follows: After this introductory chapter, a background chapter follows. This chapter contains two fairly short reviews of the fields of connectionism and computational neuroscience. Furthermore, it formally introduces catastrophic interference and forgetting, and lays out a theoretical foundation for neural networks and the dual-network memory model. 
In chapter \ref{chpt:methods}, the methods and implementation of the model is outlined. This includes describing the elected programming environment, a formal definition of the dual-network memory model which is implemented, along with model decisions and system design.
Preceding the chapter on the methods and implementation is the chapter containing the experiments and results, namely chapter \ref{chpt:experiments}.
This is the main chapter of this thesis, and the initial as well as designed experiments are outlined, along with the attained results. Furthermore, the specific sections, containing the individual experiments, also contain a fairly detailed analysis of the experiment results, as these provide the basis for designing further experiments that are also implemented.
Lastly, in the final chapter the main thesis findings are summarized, particularly from chapter \ref{chpt:experiments}. Furthermore, the research questions are addressed, after which the methods of the thesis are discussed. In the final section aspects and areas of research that I would like to focus on and address in future work are introduced, along with biological and slightly more philosophical parallels and hypotheses.

% \\\\\\\\


% \textbf{Notes}
% \\

% point out weaknesses - when introducing the model I am to investigate

% Place model in context.
% Dual-network memory architecture, sub-field.

% Could be that dual-network models may function in an intertwined that enables emergent phenomena which is previously unattained by single-network approaches. In addition to hypotheses within neuro., etc.

% Establish clear topical question(s).
% Short thesis outline.


% NEXT CHPT.:
% one detailed example from comp. neuro. [CONTEXT] - [next chpt.]

\cleardoublepage