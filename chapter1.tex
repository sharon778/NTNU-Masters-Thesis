%===================================== CHAP 1 =================================

\chapter{Introduction - [Macro level]}\label{chpt:intro}


In recent years, the possible applications of artificial intelligence (AI) have increased tremendously. From autonomous self-driving cars \citep{Urmson2009}, to facial recognition systems with super-human performance \citep{Sun2014}, to IBM's Watson performing medical diagnoses \citep{Wagle2013}, to Google's DeepMind playing Atari 2600 games \citep{Mnih2015}. Yet there is a vast amount of potential which has yet to be explored within the field; as IT is becoming increasingly ubiquitous, so will the potential applicability of AI. Two of the common factors for recent advances within AI are increased computational power, and algorithmic improvements within deep learning. Not only have algorithms such as performing facial recognition using simple deep neural networks become tractable on an average desktop-computer, there has also been an explosion in generated and available data (at least to companies such as Google and Facebook). As deep learning algorithms demand high amounts of data in order to converge towards good accuracy and generalisation, the synergy of improved hardware as well as deep learning algorithms employing long-short term memory units (LSTMs) or gated recurrent units (GRUs) \citep{Hochreiter1997, Cho2014} have resulted in the sudden explosion in potential applicability. Note that deep learning is primarily an engineering discipline. Meaning that it is more concerned with how to create applicable systems and solutions. Rather than with explaining the biological system from which the algorithms stems from. I would like to emphasize that it is the synthesis of neuroscience, psychology, and computer science that has given rise to the field of artificial neural networks, and continues to advance it. Exemplified recently by deep learning algorithms where the biologically inspired LSTM unit, and even more recently proposed and simple GRU, have enabled deep networks to capture temporal dependencies in data-sets. While this does not necessarily demonstrate the workings of the brain, it does demonstrate that looking at aspects which the biological brain captures, and translating that into what an algorithm needs to principally capture may result in significantly improved engineering solutions.

When it comes to neural networks, the most purely engineering-oriented side is possibly the field of connectionism. At least this is the least biologically plausible side within NNs.

Where the other end is computational neuroscience; building computational models which seek to explain or have biological phenomena emerge.



\textbf{Notes}
This is due to both algorithmic as well as computational advances. SLR of the field in the following(?) chapter.

These algorithms, and the field of AI, is largely connectionism. DEFINE: NNs solely based on matrices of weights and values, representing neurons' activation values and synaptic wirings and wiring strengths. This works well for some problems, and has been shown to be able to extract the principal components of a data-set, i.e. doing principal components analysis, by solely performing gradient-descent in weight space, minimizing the loss function of an error criterion (such as the norm of the difference between a target vector output and an acquired vector output).

Recent advances: Deep reinforcement learning. Deep neural networks. Hardware advances (GPUs). Algorithmic improvements using LSTMs or GRUs.

Convolutional nets: Learning feature extraction. More dynamic pre-processing layer.

Connectionism: Purely computer scientific?

Comp. Neuro.: Try and simulate aspects of brain functionality.

Examples of comp. neuro., example from connectionism? (FFBP).

Place model in context.



(Deep learning/connectionism, why it’s interesting and recent advances)
point out weaknesses
nevne convolutional nets
Nevne connectionism og comp. neuro.
one detailed example from comp. neuro. [CONTEXT]
Dual-network memory architecture, sub-field.

Coupling between the above. Justification for computer-scientific value? 
Could be that dual-network models may function in an intertwined that enables emergent phenomena which is previously unattained by single-network approaches. In addition to hypotheses within neuro., etc.

Establish clear topical question(s).
Short thesis outline.

Primary research question: “To study how the brain might implement working and long-term memory using the dual-network memory architecture, and to implement a novel dual-network memory model”


\cleardoublepage