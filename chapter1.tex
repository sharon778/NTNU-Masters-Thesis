%===================================== CHAP 1 =================================

\chapter{Introduction [Macro level]}\label{chpt:intro}

% Recently, achievements AI, mentions DL ...
In recent years, the possible applications of artificial intelligence (AI) have increased drastically. From autonomous self-driving cars \citep{Urmson2009}, to facial recognition systems with super-human performance \citep{Sun2014}, to IBM's Watson performing medical diagnoses \citep{Wagle2013}, to Google's DeepMind playing Atari 2600 games \citep{Mnih2015}. Yet there is a vast amount of potential which has yet to be explored within the field; as IT is becoming increasingly ubiquitous, so does the potential applicability of AI. One of the reasons is that the presence of IT enables harvesting and analysing data. Two of the common factors for recent advances within AI are increased computational power, and algorithmic improvements. Not only have algorithms performing facial recognition using deep learning become tractable on an average high-end desktop-computer, there has also been an explosion in generated and available data within recent decades (at least to certain companies such as Google and Facebook). In the future, we are likely to see a similar expansion in available data for the public within a vast array of domains, due to the Internet of Things. In other words, visual recognition will be far from the only tangible high-performance deep learning algorithm. Note also that a synthesis of data from different domains will most likely become more relevant too in this event, as it has already shown to be key to the performance of systems such as Facebook's excellent visual recognition system \citep{}.
\\

% What is DL?
At the core of several of the aforementioned algorithms lies the sub-field of AI known as deep learning. Deep learning is a class of algorithms where multiple layers of processing are employed, enabling the extraction of intricate correlations and patterns in data sets without any prior domain-knowledge nor supervision during training. Employing what is known as convolutional filters, functioning as elements of pre-processing, has enabled deep learning algorithms to attain excellent performance in domains such as speech recognition, image classification, and genomics \citep{LeCun2015}, the latter displaying the wide applicability of deep learning, as well as the potential benefits it may bring to society. Furthermore, the convolutional filters, essentially performing feature detection in a data set, may be trained using deep neural networks themselves \citep{LeCun2015}. This enables the nets to learn the filters as data sets are investigated, providing a much more dynamic and generally applicable unsupervised learning algorithm, although it may require larger amounts of data. In other words; the same technique for extracting intricate correlations in a data set may in fact be used to find the filters that reduce raw input data to feature vectors, which the remaining deep network may then process. This leads us to one of the core classes of general learning algorithms employed in deep learning; namely the artificial neural network (ANN), which is the overarching focal point of this thesis. I will get back to and pose the main thesis topic, as well as clear topical questions later in the current chapter.
% ANNs
Neural networks is a biologically inspired class of algorithms, borrowing its vocabulary from neuroscience. Going back to 1943, \cite{McCulloch1943} proposed to formalize neural functioning within logical propositions.
However, they could only create a logical calculus based on the abstract assumptions of the neurological and psychological basis of the time. Hypothesizing that a neural network could be calculated formally was nevertheless a seminal proposal, making the important assumption that neurons could be considered as binary processing units, due to the observation that they seemed to fire in an "all-or-none" fashion. This created a framework with which psychological phenomena could be regarded in a reductionist way; through the lens of two-valued logic. The way in which neural networks were defined, and thus changed during termination, however, remained obscure. \cite{McCulloch1943} noted that: "With determination of the net, the unknowable object of knowledge, the "thing in itself", ceases to be unknowable"
\footnote{McCulloch, W. S., \& Pitts, W. 1943. 'A Logical Calculus of the Idea Immanent in Nervous Activity', \textit{Bulletin of Mathematical Biophysics}, \textbf{5}: 131.}
Thus describing the long-standing symbol-grounding problem, which has yet to be elucidated.

% History for NNs
In his 1895 manuscript for 'Project for a Scientific Psychology', Sigmund Freud had in fact already suggested that synaptic transmission could promote post-synaptic neural excitation \citep{Kiernan2011}. Despite predicting a variant of Hebbian learning more than half a century before Hebb himself, his manuscript was not published until 1950. Therefore the phenomena of synaptic modification resulting from, and occurring relative to the correlation in neural activity among neurons, is accredited to Donald O. Hebb \citep{Kiernan2011}.
As \cite{Hebb1949} eloquently put it: "One cannot logically be a determinist in physics and chemistry and biology, and a mystic in psychology"
\footnote{Hebb, D. O. 1949. \textit{The Organization of Behavior: A Neuropsychological Theory}. New York: JOHN WILEY \& SONS, Inc. Pp. xiii}.
He further outlined the paradigm in which neural functioning is deterministic, and proposed the seminal concept of Hebbian learning, in his 1949 book 'The Organization of Behavior: A Neuropsychological Theory'. Hebbian learning is widely recognized as the fundamental mechanism underlying synaptic modification and learning in biological neural networks. Simply put, Hebbian learning may be summarised as; "fire together; wire together".

% Summary and continuation, introduction of connectionism and comp. neruo.; argumentation for synthesis with AI & DL
In less than a decade, a formal calculus for neural network computation, as well as a neuropsychological theory containing principles for how neurons may perform synaptic modification was proposed. The latter suggesting a mechanism for neural network computation, and thus possible mechanisms for memory and learning. From this historical point in time and forward, there has been a continuous synthesis between psychology, neuroscience and computer science with regards to neural computation. Computer science lending itself to construct models within computational neuroscience that seek to explain certain neurological aspects of brain functioning, which in turn may explain or be related to aspects of cognition. Conversely, insights from these fields may lend themselves to, and/or inspire computer scientists in creating more powerful neural network algorithms performing tasks such as pattern extraction, clustering, classification or segmentation. 
Naturally, this synthesis has spawned different scientific fields. Two of which are known as connectionism and computational neuroscience. While computational neuroscience attempts to tackle problems within neuroscience as touched upon above, connectionism can be described as a diversion from the biological plausibility in neural network models. This does not necessarily mean that researchers seek to use the model only for computer scientific purposes, as one may argue that certain psychological or neurological aspects may be studied without all model aspects being biologically plausible. In fact, until we fully fathom brain functioning neurologically speaking, we cannot hope to create a model which will encompass all of its biological aspects. Note, however that this may not be a necessity in order to have certain phenomena emerge from a neural network model. It may be hypothesized that certain principles underlie a certain neural functioning, and thus if simulated algorithmically, the same behaviour might emerge in the model, despite possibly being implemented quite differently. This is what largely forms the basis for connectionism, where researchers aim to study cognitive phenomena through usually simplistic computational models. The main point being that the constraint of biological plausibility is relaxed.
Quite often, a synthesis between the two aforementioned fields emerges in engineering solutions. This may possibly enable researchers to study both how the brain might implement certain functionality, as well as to study possible algorithmic implications within the more purely applied disciplines of AI such as deep learning.
\\

\textit{This is the primary motivation for the topic which is studied in this thesis.}
\\

%\section{Catastrophic Interference}
Before introducing the model which will be used in this thesis, I would like to introduce the concept of catastrophic forgetting.
\cite{Hebb1949} described the general case of learning new information which may disrupt old as the sensitivity-stability dilemma, in which one has to decide on how sensitive a network should be to new information through its parametrization. Making it more sensitive to recent information would of course most likely, depending on the network model, largely disrupt the old information contained within the network, thus making it very unstable. This problem is also known as the stability-plasticity \citep{Carpenter1987} problem, placing network models on a scale from being very plastic to very stable.
Catastrophic forgetting \citep{McCloskey1989, Ratcliff1990} is a term which describes the phenomena of when an ANN model "forgets" large parts, or everything, that it has previously learned. I.e. the weight tuning which corresponds to correlation extraction in a data set is erased. This may occur to such an extent that the model performance is equal to that of randomly assigning its network weights. Catastrophic interference is a term capturing both catastrophic forgetting, as well as other types of interference, such as when a network model fails to attain new knowledge, i.e. the network being unable to capture more correlations. In a recurrent network using Hebbian learning, catastrophic interference may occur when increasing the number of training patterns. Resulting in the network not only failing to learn new patterns, but also disrupting old patterns in attempting to learn new. This is the case in Hopfield networks, which I will get back to in chapter \ref{chpt:background}, where stable states may be considered as basins of attractions in a three-dimensional space. If there are too many basins of attraction, the behaviour will be unstable, and the state of the network will oscillate (at least seemingly) chaotically throughout the entire state space.
When it comes to traditional feed-forward back-propagation ANNs; training a network in a novel problem domain using gradient-descent will adjust the weights according to the new domain only, neglecting all knowledge that may have been previously attained \citep{McCloskey1989, French1999, French2001}.
% Move this bit to the DNMA to chpt. 2?
It can also be seen that catastrophic interference may occur if a model is given a training set which exceeds the complexity which the network may possibly capture given its topology. If the network complexity is insufficient to extract the desired distributions from the data set, gradient-descent may also fail to converge towards a solution, or only extract the most principal components.

The fact that catastrophic \textit{forgetting} occurs in an FFBP ANN model when it is trained in a novel problem domain, reflects that the network is only a local stochastic extraction of correlations from a probability distribution constituted by data sets from that particular domain. Note that recurrence in networks may enable a network to capture more complex dependencies, such as temporal dependencies, but it may also introduce a larger state-space for training algorithms in doing so.
\\

%\section{The Dual-Network Memory Architecture}

\citet{McClelland1995} propose that the brain solves the problem of catastrophic forgetting by a dual-network memory architecture, implemented by the hippocampus and neocortex. However, the body of research within AI on the architecture, is to the best of my knowledge fairly limited. Furthermore, proposed implementations suffer from issues related to simplification or obscurity \citep{French1997, French2001, Hattori2010, Hattori2014}. Note also that it is only recently that a more biologically plausible hippocampal network in such a model has been studied. \citet{Hattori2014} investigates how trying to capture the chaotic macro-scale behaviour of the CA3 region of the hippocampus affects the model's behaviour, and concludes that the model is significantly improved when compared to his former work, which in turn significantly outperforms previous implementations of the dual-network memory model. A short abstract definition of the model is that it consists of two networks, where one represents working memory, and the other long-term memory. As patterns are learned by the first network, they are consolidated to the second through the use of pseudopatterns. A pseudopattern remains roughly the same throughout different papers investigating the dual-network memory model, such as \citep{French1997, Ans2000, French2001, Hattori2010, Hattori2014}. Fundamentally, a pseudopattern is a pattern that reflects both the former configuration of the long-term memory network and new patterns that are to be learnt. Pseudopatterns are introduced formally in the following chapter; chapter \ref{chpt:background}.
\cite{Hattori2014} also states that it would be very interesting to investigate possible implications of introducing spiking neurons in his architecture.
\\

In this thesis, 
the primary research question is: \textit{"To study how the brain might implement working and long-term memory using the dual-network memory architecture, and to implement a novel dual-network memory model"}.

I aspire to do this by further investigating the dual-network memory model of \citep{Hattori2014}, and to build upon among others Hattori's work.
\\
%\section{Summary and thesis outline}

Deep learning has led to a tremendous advance in the capabilities of AI within recent years. However, the plasticity needed to combine memories or patterns in a more general and abstract way may not be not yet be present in today's state-of-the-art deep learning algorithms. One observation supporting this statement is the fact that catastrophic forgetting occurs in FFBP ANNs - and as modern state-of-the-art algorithms are still trained using forms of gradient descent, it is likely that they are prone to the same type of interference, although the storage capacity of today's ANNs within deep learning has drastically increased. 
Note also that slightly more intricate network structures may enable more sophisticated abstraction, as exemplified by authors such as \cite{Tani2014}. This is exactly the basis which is formed by the dual-network memory architecture; a recently proposed architecture which enables abstraction of short- and long-term pattern-associations.
In this thesis the main research topic is to study memory abstraction in the dual-network memory architecture, investigating possible implications for general deep learning algorithms, as well as computational neuroscience. Note that memory is an abstract term used to refer to functions, patterns, or data set-correlations that a network extracts and preferably learns. More specifically, building upon the model which \cite{Hattori2014} proposes, my aim is to answer the following questions:
\begin{itemize}
\item How does pseudopattern generation enable pattern extraction? With possible parallels to abstraction in neural networks.
\item What are the limitations of pattern extraction in the hippocampal network? And are there any limitations to consolidation to the neocortical network once a pattern has been extracted in the hippocampal module?
%\item Will a modified neocortical network using GRUs improve the performance of learning and recall in the network, and thus in the model itself?
% \item Is the proposed model less prone to network segmentation than a more conventional FFBP ANN? Where network segmentation effectively is different parts of the network operating as decoupled segments. 
%[Needs refactoring]: First; check whether this occurs. It needs to be demonstrated.
%Furthermore; it will vary depending on the type of representation used in the network. Distributed rep. may be prone to segmentation. Local representations will be segmented by definition? What happens during overlaps in that case? In the DNMM, there is no guarantee for a given representation, although it is a mix between a distributed rep. using local differences? I.e. more of a fully distributed rep. - Which would not be as prone to segmentation. Parallels to FFBP ANNs?
\item How does sparsity within the DG-layer affect and improve the hippocampal module?
\item How does the chaotic nature of the CA3-layer impact the model? Specifically, how does it enable a type of working memory and recall of different patterns with unchanging input?
\end{itemize}

I believe that a central aspect for advancing the frontier of deep learning is to investigate how high-level level cognitive behaviour and functionality may emerge in ANNs. Investigating mechanisms associated with reasoning over different memories may potentially provide insights for attaining greater plasticity and generalization in ANN models. This may be seen by considering that a crucial aspect of being able to combine different memories is simply remembering what has previously been learned. [Elaborate using theories about the hippocampus and memory from Rolls and Treves] Therefore the foremost goal in this thesis is to investigate the dual-network memory architecture in this context.

%\section{Thesis structure}
The structure of the thesis will be as follows: After this introductory chapter a background chapter follows, which contains two fairly short reviews of the fields of connectionism, and computational neuroscience. Furthermore, the chapter lays out a theoretical foundation for neural networks, as well as a more detailed presentation of catastrophic forgetting. Furthermore, related work such as the dual-network memory architecture is presented. In chapter \ref{chpt:methods}, the methods and implementation of the model is outlined. After which the experiments and results are presented in chapter \ref{chpt:experiments}, naturally followed by a discussion of the thesis findings at the macro level, in chapter \ref{chpt:discussion}. Finally, chapter \ref{chpt:conclusion} concludes the thesis, and outlines what I aspire to pursue in my future work.
\\\\\\\\


\textbf{Notes}
\\

point out weaknesses - when introducing the model I am to investigate

Place model in context.
Dual-network memory architecture, sub-field.

Could be that dual-network models may function in an intertwined that enables emergent phenomena which is previously unattained by single-network approaches. In addition to hypotheses within neuro., etc.

Establish clear topical question(s).
Short thesis outline.


NEXT CHPT.:
one detailed example from comp. neuro. [CONTEXT] - [next chpt.]

\cleardoublepage