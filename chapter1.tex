%===================================== CHAP 1 =================================

\chapter{Introduction}

AI, what is it?
\\
Neural networks.
\\
Deep learning.
\\
Deep learning, advances, powerful, more general, \textbf{bio-inspired}.
\\
I'd like to advance the capabilities. Move towards strong AI, general AI.
I believe a central aspect is investigating how high-level level cognitive behaviour and functionality emerges in ANNs.
One of these include being able to reason over different memories. Combining them. I believe a lot of complexity may emerge from this.
Remembering what has been learned is also crucial to being able to reason over distinct and differing memories.
Thus it is my goal to first and foremost address catastrophic forgetting in modern architectures.
Secondly, I wish to look upon how topological structures can support generalisation and avoiding catastrophic forgetting.
This leads one towards modern models and topologies from which complex high-level cognitive behaviour has emerged (\cite{Tani2014}).
Another interesting aspect when looking at these models and topical questions is how cognitive behaviour might emerge. How it is constituted and why it emerges. Particularly how the recurrency supports that kind of functionality and emergence from the network. In other words: How recurrency is coupled to both avoiding catastrophic forgetting, and also if any deductions upon how it adds dimensions to the levels of information processing in the models.
\\
Catastrophic forgetting, a problem in classical FFBP networks.
\\
Addressing catastrophic interference in modern networks.
\\
Aspects of high-level cognition. Arguing why this is incredibly interesting in its own right, as well as a vast amount of potential.
\\\\
The review of the research body is not complete. However, we believe the foundation will suffice to be able to contribute to the field by providing some novel perspectives on the aforementioned aspects... Investigating the created research space...
\\\\
It should be noted that as the paper shows that catastrophic remembering does occur in special case events where incredibly overlapping inputs are presented, this does not necessarily demonstrate a direct biological implausibility as implied by the authors. A biological input would probably be more multidimensional and noisy, which might play a key role in neural network activity in the event of overlapping inputs. This raises the question about whether this is something that removes the problem of catastrophic remembering. It also raises the question about whether the pseudo-patterns should be generated explicitly, or whether they are generated implicitly by nature, as noise can be said to be inherent in nature. However, from an information theoretic perspective, it is interesting to look at the implications of very dense, low-noise and overlapping inputs using pseudo-patterns, as this is what is dealt with in many of the field's applications. Again when investigating the biological brain, it does appear as if that boggles up certain memories, too. This can be seen by simply thinking of events around the same time or place. If doing so, you might experience that these memories feel fuzzy, and possibly flow over in one another - in other words; overlap. Thus it would be reasonable to state that some boggling up due to overlap is in fact biologically plausible, and possibly part of an efficient computational algorithm for information processing in neural networks.

\cleardoublepage